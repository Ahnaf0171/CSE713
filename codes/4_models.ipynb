{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Dq2BLAkXnsLC",
        "outputId": "91994936-2035-4583-e9d6-ee3561d2da2e"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ],
      "source": [
        "# Mount Google Drive to access dataset folders\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "import os\n",
        "from PIL import Image, ImageOps\n",
        "from collections import defaultdict\n",
        "\n",
        "DS1 = \"/content/drive/MyDrive/Bacteria Paper/datasets/Micro_Organism\"\n",
        "IMG_EXTS = {'.jpg','.jpeg','.png','.bmp','.tif','.tiff','.webp'}\n",
        "\n",
        "def is_image(fname):\n",
        "    return os.path.splitext(fname)[1].lower() in IMG_EXTS\n",
        "\n",
        "def convert_all_to_jpg(root, quality=95):\n",
        "    \"\"\"\n",
        "    Recursively à¦¸à¦¬ image à¦«à¦¾à¦‡à¦²à¦•à§‡ .jpg à¦«à¦°à¦®à§à¦¯à¦¾à¦Ÿà§‡ à¦•à¦¨à¦­à¦¾à¦°à§à¦Ÿ à¦•à¦°à§‡ à¦¦à§‡à§Ÿà¥¤\n",
        "    à¦…à¦°à¦¿à¦œà¦¿à¦¨à¦¾à¦² à¦«à¦¾à¦‡à¦² à¦¡à¦¿à¦²à¦¿à¦Ÿ à¦•à¦°à§‡ à¦¨à¦¤à§à¦¨ à¦«à¦¾à¦‡à¦² à¦°à¦¾à¦–à§‡à¥¤\n",
        "    \"\"\"\n",
        "    for r, _, files in os.walk(root):\n",
        "        for f in files:\n",
        "            if not is_image(f):\n",
        "                continue\n",
        "            src_path = os.path.join(r, f)\n",
        "            ext = os.path.splitext(f)[1].lower()\n",
        "            if ext == \".jpg\":\n",
        "                continue  # à¦†à¦—à§‡ à¦¥à§‡à¦•à§‡à¦‡ JPG\n",
        "\n",
        "            try:\n",
        "                with Image.open(src_path) as im:\n",
        "                    im = ImageOps.exif_transpose(im)\n",
        "                    if im.mode in (\"RGBA\",\"LA\"):\n",
        "                        bg = Image.new(\"RGB\", im.size, (255,255,255))\n",
        "                        if im.mode == \"LA\": im = im.convert(\"RGBA\")\n",
        "                        bg.paste(im, mask=im.getchannel(\"A\"))\n",
        "                        im = bg\n",
        "                    elif im.mode != \"RGB\":\n",
        "                        im = im.convert(\"RGB\")\n",
        "\n",
        "                    dst_path = os.path.splitext(src_path)[0] + \".jpg\"\n",
        "                    im.save(dst_path, format=\"JPEG\", quality=quality, optimize=True)\n",
        "\n",
        "                # à¦ªà§à¦°à¦¾à¦¨à§‹ à¦«à¦¾à¦‡à¦² à¦®à§à¦›à§‡ à¦«à§‡à¦²à§à¦¨\n",
        "                os.remove(src_path)\n",
        "                print(f\"âœ… Converted: {src_path} â†’ {dst_path}\")\n",
        "\n",
        "            except Exception as e:\n",
        "                print(f\"âš ï¸ Failed: {src_path} ({e})\")\n",
        "\n",
        "def count_formats(root):\n",
        "    counts = defaultdict(int)\n",
        "    for r, _, files in os.walk(root):\n",
        "        for f in files:\n",
        "            ext = os.path.splitext(f)[1].lower()\n",
        "            counts[ext] += 1\n",
        "    return dict(counts)\n",
        "\n",
        "# --- Run ---\n",
        "print(\"ðŸ”„ Converting all images to .jpg ...\")\n",
        "convert_all_to_jpg(DS1)\n",
        "\n",
        "print(\"\\nðŸ“Š Final format summary:\")\n",
        "summary = count_formats(DS1)\n",
        "for ext, n in summary.items():\n",
        "    print(f\"  {ext}: {n}\")\n",
        "print(\"Total images:\", sum(summary.values()))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xkjWv7Skn3fo",
        "outputId": "595ef0c5-79a0-440e-c8ac-0d18f7b59563"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "ðŸ”„ Converting all images to .jpg ...\n",
            "âœ… Converted: /content/drive/MyDrive/Bacteria Paper/datasets/Micro_Organism/Yeast/Image_17.png â†’ /content/drive/MyDrive/Bacteria Paper/datasets/Micro_Organism/Yeast/Image_17.jpg\n",
            "âœ… Converted: /content/drive/MyDrive/Bacteria Paper/datasets/Micro_Organism/Yeast/Image_20.jpeg â†’ /content/drive/MyDrive/Bacteria Paper/datasets/Micro_Organism/Yeast/Image_20.jpg\n",
            "âœ… Converted: /content/drive/MyDrive/Bacteria Paper/datasets/Micro_Organism/Spiral_bacteria/Image_39.jpeg â†’ /content/drive/MyDrive/Bacteria Paper/datasets/Micro_Organism/Spiral_bacteria/Image_39.jpg\n",
            "âœ… Converted: /content/drive/MyDrive/Bacteria Paper/datasets/Micro_Organism/Spiral_bacteria/image_81.png â†’ /content/drive/MyDrive/Bacteria Paper/datasets/Micro_Organism/Spiral_bacteria/image_81.jpg\n",
            "âœ… Converted: /content/drive/MyDrive/Bacteria Paper/datasets/Micro_Organism/Spiral_bacteria/Image_31.jpeg â†’ /content/drive/MyDrive/Bacteria Paper/datasets/Micro_Organism/Spiral_bacteria/Image_31.jpg\n",
            "âœ… Converted: /content/drive/MyDrive/Bacteria Paper/datasets/Micro_Organism/Spiral_bacteria/Image_13.png â†’ /content/drive/MyDrive/Bacteria Paper/datasets/Micro_Organism/Spiral_bacteria/Image_13.jpg\n",
            "âœ… Converted: /content/drive/MyDrive/Bacteria Paper/datasets/Micro_Organism/Spherical_bacteria/Image_20.png â†’ /content/drive/MyDrive/Bacteria Paper/datasets/Micro_Organism/Spherical_bacteria/Image_20.jpg\n",
            "âœ… Converted: /content/drive/MyDrive/Bacteria Paper/datasets/Micro_Organism/Spherical_bacteria/Image_175.png â†’ /content/drive/MyDrive/Bacteria Paper/datasets/Micro_Organism/Spherical_bacteria/Image_175.jpg\n",
            "âœ… Converted: /content/drive/MyDrive/Bacteria Paper/datasets/Micro_Organism/Spherical_bacteria/Image_246.png â†’ /content/drive/MyDrive/Bacteria Paper/datasets/Micro_Organism/Spherical_bacteria/Image_246.jpg\n",
            "âœ… Converted: /content/drive/MyDrive/Bacteria Paper/datasets/Micro_Organism/Spherical_bacteria/Image_236.png â†’ /content/drive/MyDrive/Bacteria Paper/datasets/Micro_Organism/Spherical_bacteria/Image_236.jpg\n",
            "âœ… Converted: /content/drive/MyDrive/Bacteria Paper/datasets/Micro_Organism/Spherical_bacteria/Image_316.png â†’ /content/drive/MyDrive/Bacteria Paper/datasets/Micro_Organism/Spherical_bacteria/Image_316.jpg\n",
            "âœ… Converted: /content/drive/MyDrive/Bacteria Paper/datasets/Micro_Organism/Rod_bacteria/Image_5.png â†’ /content/drive/MyDrive/Bacteria Paper/datasets/Micro_Organism/Rod_bacteria/Image_5.jpg\n",
            "âœ… Converted: /content/drive/MyDrive/Bacteria Paper/datasets/Micro_Organism/Rod_bacteria/Image_96.JPEG â†’ /content/drive/MyDrive/Bacteria Paper/datasets/Micro_Organism/Rod_bacteria/Image_96.jpg\n",
            "âœ… Converted: /content/drive/MyDrive/Bacteria Paper/datasets/Micro_Organism/Rod_bacteria/Image_51.jpeg â†’ /content/drive/MyDrive/Bacteria Paper/datasets/Micro_Organism/Rod_bacteria/Image_51.jpg\n",
            "âœ… Converted: /content/drive/MyDrive/Bacteria Paper/datasets/Micro_Organism/Rod_bacteria/Image_37.png â†’ /content/drive/MyDrive/Bacteria Paper/datasets/Micro_Organism/Rod_bacteria/Image_37.jpg\n",
            "âœ… Converted: /content/drive/MyDrive/Bacteria Paper/datasets/Micro_Organism/Rod_bacteria/Image_92.jpeg â†’ /content/drive/MyDrive/Bacteria Paper/datasets/Micro_Organism/Rod_bacteria/Image_92.jpg\n",
            "âœ… Converted: /content/drive/MyDrive/Bacteria Paper/datasets/Micro_Organism/Rod_bacteria/Image_29.jpeg â†’ /content/drive/MyDrive/Bacteria Paper/datasets/Micro_Organism/Rod_bacteria/Image_29.jpg\n",
            "âœ… Converted: /content/drive/MyDrive/Bacteria Paper/datasets/Micro_Organism/Rod_bacteria/Image_62.png â†’ /content/drive/MyDrive/Bacteria Paper/datasets/Micro_Organism/Rod_bacteria/Image_62.jpg\n",
            "âœ… Converted: /content/drive/MyDrive/Bacteria Paper/datasets/Micro_Organism/Rod_bacteria/Image_80.jpeg â†’ /content/drive/MyDrive/Bacteria Paper/datasets/Micro_Organism/Rod_bacteria/Image_80.jpg\n",
            "âœ… Converted: /content/drive/MyDrive/Bacteria Paper/datasets/Micro_Organism/Rod_bacteria/Image_73.png â†’ /content/drive/MyDrive/Bacteria Paper/datasets/Micro_Organism/Rod_bacteria/Image_73.jpg\n",
            "âœ… Converted: /content/drive/MyDrive/Bacteria Paper/datasets/Micro_Organism/Rod_bacteria/Image_38.png â†’ /content/drive/MyDrive/Bacteria Paper/datasets/Micro_Organism/Rod_bacteria/Image_38.jpg\n",
            "âœ… Converted: /content/drive/MyDrive/Bacteria Paper/datasets/Micro_Organism/Rod_bacteria/Image_2.png â†’ /content/drive/MyDrive/Bacteria Paper/datasets/Micro_Organism/Rod_bacteria/Image_2.jpg\n",
            "âœ… Converted: /content/drive/MyDrive/Bacteria Paper/datasets/Micro_Organism/Paramecium/Image_177.png â†’ /content/drive/MyDrive/Bacteria Paper/datasets/Micro_Organism/Paramecium/Image_177.jpg\n",
            "âœ… Converted: /content/drive/MyDrive/Bacteria Paper/datasets/Micro_Organism/Paramecium/Image_184.png â†’ /content/drive/MyDrive/Bacteria Paper/datasets/Micro_Organism/Paramecium/Image_184.jpg\n",
            "âœ… Converted: /content/drive/MyDrive/Bacteria Paper/datasets/Micro_Organism/Paramecium/Image_209.png â†’ /content/drive/MyDrive/Bacteria Paper/datasets/Micro_Organism/Paramecium/Image_209.jpg\n",
            "âœ… Converted: /content/drive/MyDrive/Bacteria Paper/datasets/Micro_Organism/Paramecium/Image_88.png â†’ /content/drive/MyDrive/Bacteria Paper/datasets/Micro_Organism/Paramecium/Image_88.jpg\n",
            "âœ… Converted: /content/drive/MyDrive/Bacteria Paper/datasets/Micro_Organism/Paramecium/Image_63.jpeg â†’ /content/drive/MyDrive/Bacteria Paper/datasets/Micro_Organism/Paramecium/Image_63.jpg\n",
            "âœ… Converted: /content/drive/MyDrive/Bacteria Paper/datasets/Micro_Organism/Euglena/Image_123.png â†’ /content/drive/MyDrive/Bacteria Paper/datasets/Micro_Organism/Euglena/Image_123.jpg\n",
            "âœ… Converted: /content/drive/MyDrive/Bacteria Paper/datasets/Micro_Organism/Euglena/Image_134.jpeg â†’ /content/drive/MyDrive/Bacteria Paper/datasets/Micro_Organism/Euglena/Image_134.jpg\n",
            "âœ… Converted: /content/drive/MyDrive/Bacteria Paper/datasets/Micro_Organism/Euglena/Image_117.png â†’ /content/drive/MyDrive/Bacteria Paper/datasets/Micro_Organism/Euglena/Image_117.jpg\n",
            "âœ… Converted: /content/drive/MyDrive/Bacteria Paper/datasets/Micro_Organism/Euglena/Image_81.png â†’ /content/drive/MyDrive/Bacteria Paper/datasets/Micro_Organism/Euglena/Image_81.jpg\n",
            "âœ… Converted: /content/drive/MyDrive/Bacteria Paper/datasets/Micro_Organism/Euglena/Image_39.png â†’ /content/drive/MyDrive/Bacteria Paper/datasets/Micro_Organism/Euglena/Image_39.jpg\n",
            "âœ… Converted: /content/drive/MyDrive/Bacteria Paper/datasets/Micro_Organism/Euglena/Image_99.jpeg â†’ /content/drive/MyDrive/Bacteria Paper/datasets/Micro_Organism/Euglena/Image_99.jpg\n",
            "âœ… Converted: /content/drive/MyDrive/Bacteria Paper/datasets/Micro_Organism/Amoeba/Image_74.png â†’ /content/drive/MyDrive/Bacteria Paper/datasets/Micro_Organism/Amoeba/Image_74.jpg\n",
            "âœ… Converted: /content/drive/MyDrive/Bacteria Paper/datasets/Micro_Organism/Amoeba/Image_20.png â†’ /content/drive/MyDrive/Bacteria Paper/datasets/Micro_Organism/Amoeba/Image_20.jpg\n",
            "âœ… Converted: /content/drive/MyDrive/Bacteria Paper/datasets/Micro_Organism/Amoeba/Image_60.png â†’ /content/drive/MyDrive/Bacteria Paper/datasets/Micro_Organism/Amoeba/Image_60.jpg\n",
            "\n",
            "ðŸ“Š Final format summary:\n",
            "  .jpg: 788\n",
            "Total images: 788\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "from collections import defaultdict\n",
        "\n",
        "# === à¦†à¦ªà¦¨à¦¾à¦° à¦¡à§‡à¦Ÿà¦¾à¦¸à§‡à¦Ÿà§‡à¦° à¦ªà¦¥ ===\n",
        "DS1 = \"/content/drive/MyDrive/Bacteria Paper/datasets/Micro_Organism\"\n",
        "DS2 = \"/content/drive/MyDrive/Bacteria Paper/datasets/EMDS_5_Custom\"\n",
        "\n",
        "IMG_EXTS = {'.jpg', '.jpeg', '.png', '.bmp', '.tif', '.tiff', '.webp'}\n",
        "\n",
        "def is_image(fname):\n",
        "    return os.path.splitext(fname)[1].lower() in IMG_EXTS\n",
        "\n",
        "def count_images_by_class(root):\n",
        "    \"\"\"\n",
        "    ImageFolder-style: root/class_name/*.*\n",
        "    à¦¸à¦¾à¦¬à¦«à§‹à¦²à§à¦¡à¦¾à¦° à¦¥à¦¾à¦•à¦²à§‡ recursive à¦­à¦¾à¦¬à§‡ à¦—à¦£à¦¨à¦¾ à¦•à¦°à¦¬à§‡à¥¤\n",
        "    à¦°à¦¿à¦Ÿà¦¾à¦°à§à¦¨: total_count, dict(class_name -> count), dict(ext -> count)\n",
        "    \"\"\"\n",
        "    per_class = defaultdict(int)\n",
        "    per_ext = defaultdict(int)\n",
        "    total = 0\n",
        "\n",
        "    if not os.path.isdir(root):\n",
        "        raise FileNotFoundError(f\"Path not found: {root}\")\n",
        "\n",
        "    # à¦•à§à¦²à¦¾à¦¸ à¦¨à¦¾à¦® à¦§à¦°à¦¾ à¦¹à¦šà§à¦›à§‡ root-à¦à¦° à¦ à¦¿à¦• à¦¨à¦¿à¦šà§‡à¦° à¦¸à¦¾à¦¬à¦«à§‹à¦²à§à¦¡à¦¾à¦°\n",
        "    class_dirs = [d for d in os.listdir(root) if os.path.isdir(os.path.join(root, d))]\n",
        "    if not class_dirs:\n",
        "        # à¦•à§à¦²à¦¾à¦¸ à¦«à§‹à¦²à§à¦¡à¦¾à¦° à¦¨à¦¾ à¦¥à¦¾à¦•à¦²à§‡ root-à¦•à§‡ \"_root_\" à¦¹à¦¿à¦¸à§‡à¦¬à§‡ à¦—à¦£à§à¦¯\n",
        "        class_dirs = [\"_root_\"]\n",
        "\n",
        "    for cls in class_dirs:\n",
        "        base = root if cls == \"_root_\" else os.path.join(root, cls)\n",
        "        for r, _, files in os.walk(base):\n",
        "            for f in files:\n",
        "                if is_image(f):\n",
        "                    per_class[cls] += 1\n",
        "                    per_ext[os.path.splitext(f)[1].lower()] += 1\n",
        "                    total += 1\n",
        "    return total, dict(sorted(per_class.items())), dict(sorted(per_ext.items()))\n",
        "\n",
        "def pretty_print(name, total, per_class, per_ext):\n",
        "    print(f\"\\n==== {name} ====\")\n",
        "    print(f\"ðŸ”¢ Total images: {total}\")\n",
        "    if len(per_class) == 1 and \"_root_\" in per_class:\n",
        "        print(\"âš ï¸ à¦•à§‹à¦¨à§‹ à¦•à§à¦²à¦¾à¦¸ à¦«à§‹à¦²à§à¦¡à¦¾à¦° à¦ªà¦¾à¦“à§Ÿà¦¾ à¦¯à¦¾à§Ÿà¦¨à¦¿; à¦¸à¦¬ à¦‡à¦®à§‡à¦œ à¦à¦• à¦œà¦¾à§Ÿà¦—à¦¾à§Ÿ à¦†à¦›à§‡ à¦®à¦¨à§‡ à¦¹à¦šà§à¦›à§‡à¥¤\")\n",
        "    else:\n",
        "        print(\"ðŸ“‚ Per-class counts:\")\n",
        "        for cls, n in per_class.items():\n",
        "            if cls != \"_root_\":\n",
        "                print(f\"  - {cls}: {n}\")\n",
        "    if per_ext:\n",
        "        print(\"ðŸ§© By extension:\")\n",
        "        for ext, n in per_ext.items():\n",
        "            print(f\"  {ext}: {n}\")\n",
        "\n",
        "# === Run for both datasets ===\n",
        "tot1, pc1, pe1 = count_images_by_class(DS1)\n",
        "pretty_print(\"Micro_Organism\", tot1, pc1, pe1)\n",
        "\n",
        "tot2, pc2, pe2 = count_images_by_class(DS2)\n",
        "pretty_print(\"EMDS_5_Custom\", tot2, pc2, pe2)\n",
        "\n",
        "print(f\"\\nðŸ“Š Combined total (both datasets): {tot1 + tot2}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "G6mudJshoITc",
        "outputId": "986155a0-2d59-4464-ab97-4764995da193"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "==== Micro_Organism ====\n",
            "ðŸ”¢ Total images: 788\n",
            "ðŸ“‚ Per-class counts:\n",
            "  - Amoeba: 72\n",
            "  - Euglena: 168\n",
            "  - Hydra: 76\n",
            "  - Paramecium: 152\n",
            "  - Rod_bacteria: 85\n",
            "  - Spherical_bacteria: 86\n",
            "  - Spiral_bacteria: 74\n",
            "  - Yeast: 75\n",
            "ðŸ§© By extension:\n",
            "  .jpg: 788\n",
            "\n",
            "==== EMDS_5_Custom ====\n",
            "ðŸ”¢ Total images: 817\n",
            "ðŸ“‚ Per-class counts:\n",
            "  - Actinophrys: 39\n",
            "  - Arcella: 40\n",
            "  - Aspidisca: 40\n",
            "  - Ceratium: 38\n",
            "  - Codosiga: 39\n",
            "  - Colpoda: 39\n",
            "  - Epistylis: 40\n",
            "  - Euglena: 37\n",
            "  - Euglypha: 40\n",
            "  - Gonyaulax: 38\n",
            "  - Gymnodinium: 39\n",
            "  - Keratella_quadrala: 40\n",
            "  - Noctiluca: 38\n",
            "  - Paramecium: 40\n",
            "  - Phacus: 36\n",
            "  - Rotifera: 40\n",
            "  - Siprostomum: 37\n",
            "  - Stentor: 38\n",
            "  - Stylonychia: 40\n",
            "  - Synchaeta: 39\n",
            "  - Vorticella: 40\n",
            "ðŸ§© By extension:\n",
            "  .jpg: 817\n",
            "\n",
            "ðŸ“Š Combined total (both datasets): 1605\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os, io, shutil, hashlib\n",
        "from PIL import Image, ImageOps\n",
        "from collections import defaultdict\n",
        "\n",
        "# ================== Paths ==================\n",
        "DS1 = \"/content/drive/MyDrive/Bacteria Paper/datasets/Micro_Organism\"\n",
        "DS2 = \"/content/drive/MyDrive/Bacteria Paper/datasets/EMDS_5_Custom\"\n",
        "DST = \"/content/drive/MyDrive/Bacteria Paper/datasets/merged_dataset\"  # âœ… final merged dataset\n",
        "\n",
        "# ================== Settings ==================\n",
        "IMG_EXTS = {\".jpg\", \".jpeg\", \".png\", \".bmp\", \".tif\", \".tiff\", \".webp\"}\n",
        "SKIP_DIR_KEYS = ('mask', 'masks', 'gt', 'ground', 'label', 'edge', 'seg')  # skip ground-truth folders\n",
        "JPEG_QUALITY = 95\n",
        "MIN_PAD_WIDTH = 2  # 01, 02 â€¦; auto increases to 3 if 100+\n",
        "\n",
        "# ================== Utils ==================\n",
        "def is_image(fname: str) -> bool:\n",
        "    return os.path.splitext(fname)[1].lower() in IMG_EXTS\n",
        "\n",
        "def should_skip_dir(path: str) -> bool:\n",
        "    base = os.path.basename(path).lower()\n",
        "    return any(k in base for k in SKIP_DIR_KEYS)\n",
        "\n",
        "def load_rgb_canonical(path):\n",
        "    with Image.open(path) as im:\n",
        "        im = ImageOps.exif_transpose(im)\n",
        "        if im.mode in (\"RGBA\", \"LA\"):\n",
        "            bg = Image.new(\"RGB\", im.size, (255, 255, 255))\n",
        "            if im.mode == \"LA\":\n",
        "                im = im.convert(\"RGBA\")\n",
        "            bg.paste(im, mask=im.getchannel(\"A\"))\n",
        "            im = bg\n",
        "        elif im.mode != \"RGB\":\n",
        "            im = im.convert(\"RGB\")\n",
        "        return im\n",
        "\n",
        "def to_canonical_jpeg_bytes(pil_img, quality=JPEG_QUALITY) -> bytes:\n",
        "    buf = io.BytesIO()\n",
        "    pil_img.save(buf, format=\"JPEG\", quality=quality, optimize=True)\n",
        "    return buf.getvalue()\n",
        "\n",
        "def md5(b: bytes) -> str:\n",
        "    h = hashlib.md5(); h.update(b); return h.hexdigest()\n",
        "\n",
        "def list_classes(root):\n",
        "    if not os.path.isdir(root):\n",
        "        return set()\n",
        "    return {d for d in os.listdir(root) if os.path.isdir(os.path.join(root, d))}\n",
        "\n",
        "def iter_image_files(class_root):\n",
        "    for r, dnames, fnames in os.walk(class_root):\n",
        "        if should_skip_dir(r):\n",
        "            continue\n",
        "        for f in fnames:\n",
        "            if is_image(f):\n",
        "                yield os.path.join(r, f)\n",
        "\n",
        "def gather_unique_jpegs_for_class(cls_name, sources):\n",
        "    seen_hashes = set()\n",
        "    unique_bytes = []\n",
        "    for ds_root in sources:\n",
        "        if not ds_root: continue\n",
        "        cls_path = os.path.join(ds_root, cls_name)\n",
        "        if not os.path.isdir(cls_path): continue\n",
        "\n",
        "        for fp in iter_image_files(cls_path):\n",
        "            try:\n",
        "                pil = load_rgb_canonical(fp)\n",
        "                jb = to_canonical_jpeg_bytes(pil, quality=JPEG_QUALITY)\n",
        "                h = md5(jb)\n",
        "                if h in seen_hashes:\n",
        "                    continue\n",
        "                seen_hashes.add(h)\n",
        "                unique_bytes.append(jb)\n",
        "            except Exception as e:\n",
        "                print(f\"âš ï¸ Skipping unreadable: {fp} ({e})\")\n",
        "    return unique_bytes\n",
        "\n",
        "def write_numbered_jpegs(dst_class_dir, jpeg_bytes_list, min_pad=MIN_PAD_WIDTH):\n",
        "    if os.path.exists(dst_class_dir):\n",
        "        shutil.rmtree(dst_class_dir)\n",
        "    os.makedirs(dst_class_dir, exist_ok=True)\n",
        "\n",
        "    n = len(jpeg_bytes_list)\n",
        "    if n == 0: return 0\n",
        "\n",
        "    pad = max(min_pad, len(str(n)))\n",
        "    for i, jb in enumerate(jpeg_bytes_list, start=1):\n",
        "        out = os.path.join(dst_class_dir, f\"{i:0{pad}d}.jpg\")\n",
        "        with open(out, \"wb\") as f:\n",
        "            f.write(jb)\n",
        "    return n\n",
        "\n",
        "def count_jpgs(root):\n",
        "    total = 0\n",
        "    per_class = {}\n",
        "    for cls in sorted(list_classes(root)):\n",
        "        cls_dir = os.path.join(root, cls)\n",
        "        n = sum(1 for f in os.listdir(cls_dir)\n",
        "                if os.path.isfile(os.path.join(cls_dir, f)) and f.lower().endswith(\".jpg\"))\n",
        "        total += n\n",
        "        per_class[cls] = n\n",
        "    return total, per_class\n",
        "\n",
        "# ================== Merge ==================\n",
        "def merge_datasets(ds1, ds2, dst):\n",
        "    # âœ… Create destination folder if not exists\n",
        "    if not os.path.exists(dst):\n",
        "        os.makedirs(dst, exist_ok=True)\n",
        "\n",
        "    classes = sorted(list(list_classes(ds1) | list_classes(ds2)))\n",
        "    if not classes:\n",
        "        raise RuntimeError(\"No class folders found in DS1 or DS2.\")\n",
        "\n",
        "    print(f\"Found {len(classes)} classes.\")\n",
        "    grand_total = 0\n",
        "    report = defaultdict(int)\n",
        "\n",
        "    for cls in classes:\n",
        "        print(f\"\\nâ–¶ Merging class: {cls}\")\n",
        "        jb_list = gather_unique_jpegs_for_class(cls, [ds1, ds2])\n",
        "        dst_class = os.path.join(dst, cls)\n",
        "        kept = write_numbered_jpegs(dst_class, jb_list, min_pad=MIN_PAD_WIDTH)\n",
        "        grand_total += kept\n",
        "        report[cls] = kept\n",
        "        print(f\"   Saved {kept} images â†’ {dst_class}\")\n",
        "\n",
        "    print(\"\\n===== Merge Summary =====\")\n",
        "    for cls in sorted(report):\n",
        "        print(f\"  - {cls}: {report[cls]} JPG\")\n",
        "    print(f\"ðŸ”¢ Total JPG images (merged): {grand_total}\")\n",
        "\n",
        "# ================== Run ==================\n",
        "if __name__ == \"__main__\":\n",
        "    print(f\"Merging:\\n  DS1 = {DS1}\\n  DS2 = {DS2}\\nâ†’ DST = {DST}\")\n",
        "    merge_datasets(DS1, DS2, DST)\n",
        "\n",
        "    tot, per = count_jpgs(DST)\n",
        "    print(\"\\nVerification (final counts):\")\n",
        "    for cls, n in per.items():\n",
        "        print(f\"  - {cls}: {n}\")\n",
        "    print(\"Total:\", tot)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wNcSbD1IoTS0",
        "outputId": "f20b89af-7536-4377-ea4e-d434d0badc50"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Merging:\n",
            "  DS1 = /content/drive/MyDrive/Bacteria Paper/datasets/Micro_Organism\n",
            "  DS2 = /content/drive/MyDrive/Bacteria Paper/datasets/EMDS_5_Custom\n",
            "â†’ DST = /content/drive/MyDrive/Bacteria Paper/datasets/merged_dataset\n",
            "Found 27 classes.\n",
            "\n",
            "â–¶ Merging class: Actinophrys\n",
            "   Saved 39 images â†’ /content/drive/MyDrive/Bacteria Paper/datasets/merged_dataset/Actinophrys\n",
            "\n",
            "â–¶ Merging class: Amoeba\n",
            "   Saved 69 images â†’ /content/drive/MyDrive/Bacteria Paper/datasets/merged_dataset/Amoeba\n",
            "\n",
            "â–¶ Merging class: Arcella\n",
            "   Saved 40 images â†’ /content/drive/MyDrive/Bacteria Paper/datasets/merged_dataset/Arcella\n",
            "\n",
            "â–¶ Merging class: Aspidisca\n",
            "   Saved 40 images â†’ /content/drive/MyDrive/Bacteria Paper/datasets/merged_dataset/Aspidisca\n",
            "\n",
            "â–¶ Merging class: Ceratium\n",
            "   Saved 38 images â†’ /content/drive/MyDrive/Bacteria Paper/datasets/merged_dataset/Ceratium\n",
            "\n",
            "â–¶ Merging class: Codosiga\n",
            "   Saved 39 images â†’ /content/drive/MyDrive/Bacteria Paper/datasets/merged_dataset/Codosiga\n",
            "\n",
            "â–¶ Merging class: Colpoda\n",
            "   Saved 39 images â†’ /content/drive/MyDrive/Bacteria Paper/datasets/merged_dataset/Colpoda\n",
            "\n",
            "â–¶ Merging class: Epistylis\n",
            "   Saved 40 images â†’ /content/drive/MyDrive/Bacteria Paper/datasets/merged_dataset/Epistylis\n",
            "\n",
            "â–¶ Merging class: Euglena\n",
            "   Saved 201 images â†’ /content/drive/MyDrive/Bacteria Paper/datasets/merged_dataset/Euglena\n",
            "\n",
            "â–¶ Merging class: Euglypha\n",
            "   Saved 40 images â†’ /content/drive/MyDrive/Bacteria Paper/datasets/merged_dataset/Euglypha\n",
            "\n",
            "â–¶ Merging class: Gonyaulax\n",
            "   Saved 38 images â†’ /content/drive/MyDrive/Bacteria Paper/datasets/merged_dataset/Gonyaulax\n",
            "\n",
            "â–¶ Merging class: Gymnodinium\n",
            "   Saved 39 images â†’ /content/drive/MyDrive/Bacteria Paper/datasets/merged_dataset/Gymnodinium\n",
            "\n",
            "â–¶ Merging class: Hydra\n",
            "   Saved 76 images â†’ /content/drive/MyDrive/Bacteria Paper/datasets/merged_dataset/Hydra\n",
            "\n",
            "â–¶ Merging class: Keratella_quadrala\n",
            "   Saved 40 images â†’ /content/drive/MyDrive/Bacteria Paper/datasets/merged_dataset/Keratella_quadrala\n",
            "\n",
            "â–¶ Merging class: Noctiluca\n",
            "   Saved 38 images â†’ /content/drive/MyDrive/Bacteria Paper/datasets/merged_dataset/Noctiluca\n",
            "\n",
            "â–¶ Merging class: Paramecium\n",
            "   Saved 188 images â†’ /content/drive/MyDrive/Bacteria Paper/datasets/merged_dataset/Paramecium\n",
            "\n",
            "â–¶ Merging class: Phacus\n",
            "   Saved 36 images â†’ /content/drive/MyDrive/Bacteria Paper/datasets/merged_dataset/Phacus\n",
            "\n",
            "â–¶ Merging class: Rod_bacteria\n",
            "   Saved 84 images â†’ /content/drive/MyDrive/Bacteria Paper/datasets/merged_dataset/Rod_bacteria\n",
            "\n",
            "â–¶ Merging class: Rotifera\n",
            "   Saved 40 images â†’ /content/drive/MyDrive/Bacteria Paper/datasets/merged_dataset/Rotifera\n",
            "\n",
            "â–¶ Merging class: Siprostomum\n",
            "   Saved 37 images â†’ /content/drive/MyDrive/Bacteria Paper/datasets/merged_dataset/Siprostomum\n",
            "\n",
            "â–¶ Merging class: Spherical_bacteria\n",
            "   Saved 86 images â†’ /content/drive/MyDrive/Bacteria Paper/datasets/merged_dataset/Spherical_bacteria\n",
            "\n",
            "â–¶ Merging class: Spiral_bacteria\n",
            "   Saved 72 images â†’ /content/drive/MyDrive/Bacteria Paper/datasets/merged_dataset/Spiral_bacteria\n",
            "\n",
            "â–¶ Merging class: Stentor\n",
            "   Saved 38 images â†’ /content/drive/MyDrive/Bacteria Paper/datasets/merged_dataset/Stentor\n",
            "\n",
            "â–¶ Merging class: Stylonychia\n",
            "   Saved 39 images â†’ /content/drive/MyDrive/Bacteria Paper/datasets/merged_dataset/Stylonychia\n",
            "\n",
            "â–¶ Merging class: Synchaeta\n",
            "   Saved 39 images â†’ /content/drive/MyDrive/Bacteria Paper/datasets/merged_dataset/Synchaeta\n",
            "\n",
            "â–¶ Merging class: Vorticella\n",
            "   Saved 40 images â†’ /content/drive/MyDrive/Bacteria Paper/datasets/merged_dataset/Vorticella\n",
            "\n",
            "â–¶ Merging class: Yeast\n",
            "   Saved 73 images â†’ /content/drive/MyDrive/Bacteria Paper/datasets/merged_dataset/Yeast\n",
            "\n",
            "===== Merge Summary =====\n",
            "  - Actinophrys: 39 JPG\n",
            "  - Amoeba: 69 JPG\n",
            "  - Arcella: 40 JPG\n",
            "  - Aspidisca: 40 JPG\n",
            "  - Ceratium: 38 JPG\n",
            "  - Codosiga: 39 JPG\n",
            "  - Colpoda: 39 JPG\n",
            "  - Epistylis: 40 JPG\n",
            "  - Euglena: 201 JPG\n",
            "  - Euglypha: 40 JPG\n",
            "  - Gonyaulax: 38 JPG\n",
            "  - Gymnodinium: 39 JPG\n",
            "  - Hydra: 76 JPG\n",
            "  - Keratella_quadrala: 40 JPG\n",
            "  - Noctiluca: 38 JPG\n",
            "  - Paramecium: 188 JPG\n",
            "  - Phacus: 36 JPG\n",
            "  - Rod_bacteria: 84 JPG\n",
            "  - Rotifera: 40 JPG\n",
            "  - Siprostomum: 37 JPG\n",
            "  - Spherical_bacteria: 86 JPG\n",
            "  - Spiral_bacteria: 72 JPG\n",
            "  - Stentor: 38 JPG\n",
            "  - Stylonychia: 39 JPG\n",
            "  - Synchaeta: 39 JPG\n",
            "  - Vorticella: 40 JPG\n",
            "  - Yeast: 73 JPG\n",
            "ðŸ”¢ Total JPG images (merged): 1588\n",
            "\n",
            "Verification (final counts):\n",
            "  - Actinophrys: 39\n",
            "  - Amoeba: 69\n",
            "  - Arcella: 40\n",
            "  - Aspidisca: 40\n",
            "  - Ceratium: 38\n",
            "  - Codosiga: 39\n",
            "  - Colpoda: 39\n",
            "  - Epistylis: 40\n",
            "  - Euglena: 201\n",
            "  - Euglypha: 40\n",
            "  - Gonyaulax: 38\n",
            "  - Gymnodinium: 39\n",
            "  - Hydra: 76\n",
            "  - Keratella_quadrala: 40\n",
            "  - Noctiluca: 38\n",
            "  - Paramecium: 188\n",
            "  - Phacus: 36\n",
            "  - Rod_bacteria: 84\n",
            "  - Rotifera: 40\n",
            "  - Siprostomum: 37\n",
            "  - Spherical_bacteria: 86\n",
            "  - Spiral_bacteria: 72\n",
            "  - Stentor: 38\n",
            "  - Stylonychia: 39\n",
            "  - Synchaeta: 39\n",
            "  - Vorticella: 40\n",
            "  - Yeast: 73\n",
            "Total: 1588\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# ==== Class distribution after merge (readable labels + distinct colors) ====\n",
        "import os\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import matplotlib as mpl\n",
        "\n",
        "# --- your merged dataset path (fixed) ---\n",
        "DST = \"/content/drive/MyDrive/Bacteria Paper/datasets/merged_dataset\"\n",
        "merged_dir = DST\n",
        "\n",
        "if not os.path.isdir(merged_dir):\n",
        "    raise FileNotFoundError(f\"Merged dataset folder not found at: {merged_dir}\")\n",
        "\n",
        "print(\"Using merged dataset from:\", merged_dir)\n",
        "\n",
        "def pretty_name(s):\n",
        "    # underscores/dashes -> space, Title Case\n",
        "    return s.replace('_', ' ').replace('-', ' ').strip().title()\n",
        "\n",
        "# ---- collect counts from immediate subfolders (class-wise) ----\n",
        "IMG_EXTS = ('.jpg', '.jpeg', '.png', '.bmp', '.tif', '.tiff', '.webp')\n",
        "class_counts = {}\n",
        "\n",
        "for cls in sorted(os.listdir(merged_dir)):\n",
        "    p = os.path.join(merged_dir, cls)\n",
        "    if not os.path.isdir(p):\n",
        "        continue\n",
        "    n = 0\n",
        "    # count only files in this class folder (non-recursive)\n",
        "    for f in os.listdir(p):\n",
        "        if f.lower().endswith(IMG_EXTS) and os.path.isfile(os.path.join(p, f)):\n",
        "            n += 1\n",
        "    if n > 0:\n",
        "        class_counts[cls] = n\n",
        "\n",
        "# ---- nothing to plot? ----\n",
        "if not class_counts:\n",
        "    print(\"No images found under class folders in:\", merged_dir)\n",
        "else:\n",
        "    # sort by count (desc)\n",
        "    items = sorted(class_counts.items(), key=lambda x: x[1], reverse=True)\n",
        "    raw_names  = [k for k, _ in items]\n",
        "    disp_names = [pretty_name(k) for k, _ in items]\n",
        "    counts     = [v for _, v in items]\n",
        "    n = len(disp_names)\n",
        "\n",
        "    # ---- build a robust color palette with many distinct colors\n",
        "    cmaps = [\n",
        "        mpl.cm.get_cmap('tab20', 20),\n",
        "        mpl.cm.get_cmap('tab20b', 20),\n",
        "        mpl.cm.get_cmap('tab20c', 20),\n",
        "    ]\n",
        "    colors = np.vstack([c(np.arange(c.N))[:, :3] for c in cmaps])  # RGB only\n",
        "    if n > colors.shape[0]:\n",
        "        extra = mpl.cm.get_cmap('hsv', n - colors.shape[0])(np.arange(n - colors.shape[0]))[:, :3]\n",
        "        colors = np.vstack([colors, extra])\n",
        "    colors = colors[:n]\n",
        "\n",
        "    # ---- plot (horizontal bars for readability)\n",
        "    fig_h = max(6, 0.45 * n)  # scale height with number of classes\n",
        "    plt.figure(figsize=(14, fig_h))\n",
        "\n",
        "    ypos = np.arange(n)\n",
        "    bars = plt.barh(ypos, counts, color=colors, edgecolor='none')\n",
        "\n",
        "    # counts at end of bar\n",
        "    max_count = max(counts) if counts else 1\n",
        "    for i, b in enumerate(bars):\n",
        "        w = b.get_width()\n",
        "        plt.text(w + max_count * 0.01, b.get_y() + b.get_height()/2,\n",
        "                 f\"{counts[i]}\", va='center', fontsize=11)\n",
        "\n",
        "    # labels, titles, layout\n",
        "    plt.yticks(ypos, disp_names, fontsize=11)\n",
        "    plt.gca().invert_yaxis()  # largest on top\n",
        "    plt.xlabel(\"Number of Images\", fontsize=12)\n",
        "    plt.title(\"Class Distribution of Merged Dataset\", fontsize=15, weight='bold')\n",
        "\n",
        "    # add some padding on the right so numbers don't get cut off\n",
        "    plt.xlim(0, max_count * 1.12)\n",
        "\n",
        "    plt.tight_layout()\n",
        "    save_path = os.path.join(merged_dir, \"class_distribution_readable.png\")\n",
        "    plt.savefig(save_path, dpi=250, bbox_inches='tight')\n",
        "    plt.show()\n",
        "\n",
        "    # console summary\n",
        "    print(\"Total classes:\", n)\n",
        "    print(\"Total images:\", sum(counts))\n",
        "    print(\"Saved plot to:\", save_path)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "0kg-62mDofVI",
        "outputId": "b416db22-6273-4394-80f1-aab750435b3b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Using merged dataset from: /content/drive/MyDrive/Bacteria Paper/datasets/merged_dataset\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/tmp/ipython-input-1769548452.py:49: MatplotlibDeprecationWarning: The get_cmap function was deprecated in Matplotlib 3.7 and will be removed in 3.11. Use ``matplotlib.colormaps[name]`` or ``matplotlib.colormaps.get_cmap()`` or ``pyplot.get_cmap()`` instead.\n",
            "  mpl.cm.get_cmap('tab20', 20),\n",
            "/tmp/ipython-input-1769548452.py:50: MatplotlibDeprecationWarning: The get_cmap function was deprecated in Matplotlib 3.7 and will be removed in 3.11. Use ``matplotlib.colormaps[name]`` or ``matplotlib.colormaps.get_cmap()`` or ``pyplot.get_cmap()`` instead.\n",
            "  mpl.cm.get_cmap('tab20b', 20),\n",
            "/tmp/ipython-input-1769548452.py:51: MatplotlibDeprecationWarning: The get_cmap function was deprecated in Matplotlib 3.7 and will be removed in 3.11. Use ``matplotlib.colormaps[name]`` or ``matplotlib.colormaps.get_cmap()`` or ``pyplot.get_cmap()`` instead.\n",
            "  mpl.cm.get_cmap('tab20c', 20),\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 1400x1215 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Total classes: 27\n",
            "Total images: 1588\n",
            "Saved plot to: /content/drive/MyDrive/Bacteria Paper/datasets/merged_dataset/class_distribution_readable.png\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Install dependency for perceptual hashing\n",
        "!pip install -q ImageHash\n",
        "\n",
        "import os\n",
        "import shutil\n",
        "import uuid\n",
        "import cv2\n",
        "from PIL import Image\n",
        "import imagehash\n",
        "\n",
        "# ==== Use your merged dataset path (fixed) ====\n",
        "DST = \"/content/drive/MyDrive/Bacteria Paper/datasets/merged_dataset\"\n",
        "merged_dir = DST\n",
        "\n",
        "if not os.path.isdir(merged_dir):\n",
        "    raise FileNotFoundError(f\"Merged dataset folder not found at: {merged_dir}\")\n",
        "\n",
        "print(\"Using merged dataset from:\", merged_dir)\n",
        "\n",
        "# Consider these as image files\n",
        "IMG_EXTS = ('.jpg', '.jpeg', '.png', '.bmp', '.tif', '.tiff', '.webp')\n",
        "\n",
        "# ==== Show total images before cleaning ====\n",
        "def count_total_images(root):\n",
        "    total_images = 0\n",
        "    for cls in sorted(os.listdir(root)):\n",
        "        cls_path = os.path.join(root, cls)\n",
        "        if not os.path.isdir(cls_path):\n",
        "            continue\n",
        "        total_images += len([f for f in os.listdir(cls_path)\n",
        "                             if os.path.isfile(os.path.join(cls_path, f)) and\n",
        "                             f.lower().endswith(IMG_EXTS)])\n",
        "    return total_images\n",
        "\n",
        "total_images_before_cleaning = count_total_images(merged_dir)\n",
        "print(\"Total images before cleaning:\", total_images_before_cleaning)\n",
        "\n",
        "# ==== Remove near-duplicates using pHash Hamming distance threshold ====\n",
        "def remove_near_duplicates(root, thresh=6):\n",
        "    \"\"\"\n",
        "    For each class folder:\n",
        "      - compute perceptual hash (pHash) for each image\n",
        "      - if any previously kept hash differs by <= thresh (Hamming distance), treat as near-duplicate and remove\n",
        "    Returns: number of removed files\n",
        "    \"\"\"\n",
        "    removed = 0\n",
        "    for cls in sorted(os.listdir(root)):\n",
        "        cls_path = os.path.join(root, cls)\n",
        "        if not os.path.isdir(cls_path):\n",
        "            continue\n",
        "\n",
        "        kept_hashes = []  # list[imagehash.ImageHash]\n",
        "        for fn in list(os.listdir(cls_path)):\n",
        "            fp = os.path.join(cls_path, fn)\n",
        "            if not os.path.isfile(fp):\n",
        "                continue\n",
        "            if not fn.lower().endswith(IMG_EXTS):\n",
        "                # non-image (or leftover) -> try remove\n",
        "                try:\n",
        "                    os.remove(fp); removed += 1\n",
        "                except:\n",
        "                    pass\n",
        "                continue\n",
        "            try:\n",
        "                with Image.open(fp) as im:\n",
        "                    im = im.convert(\"RGB\")\n",
        "                    h = imagehash.phash(im)\n",
        "                # near-dup if any existing hash is within threshold\n",
        "                if any((h - kh) <= thresh for kh in kept_hashes):\n",
        "                    os.remove(fp); removed += 1\n",
        "                else:\n",
        "                    kept_hashes.append(h)\n",
        "            except:\n",
        "                # unreadable/corrupted file -> remove\n",
        "                try:\n",
        "                    os.remove(fp); removed += 1\n",
        "                except:\n",
        "                    pass\n",
        "    return removed\n",
        "\n",
        "removed_dups = remove_near_duplicates(merged_dir, thresh=6)\n",
        "print(f\"Removed near-duplicates: {removed_dups}\")\n",
        "\n",
        "# ==== Remove blurry images by variance of Laplacian ====\n",
        "def remove_blurry(root, blurry_threshold=100.0):\n",
        "    \"\"\"\n",
        "    Removes images whose Laplacian variance (focus measure) is below threshold.\n",
        "    Returns: number of removed files\n",
        "    \"\"\"\n",
        "    removed = 0\n",
        "    for cls in sorted(os.listdir(root)):\n",
        "        cls_path = os.path.join(root, cls)\n",
        "        if not os.path.isdir(cls_path):\n",
        "            continue\n",
        "        for fn in list(os.listdir(cls_path)):\n",
        "            fp = os.path.join(cls_path, fn)\n",
        "            if not os.path.isfile(fp):\n",
        "                continue\n",
        "            if not fn.lower().endswith(IMG_EXTS):\n",
        "                continue\n",
        "            try:\n",
        "                img = cv2.imread(fp)\n",
        "                if img is None:\n",
        "                    os.remove(fp); removed += 1; continue\n",
        "                gray = cv2.cvtColor(img, cv2.COLOR_BGR2GRAY)\n",
        "                var = cv2.Laplacian(gray, cv2.CV_64F).var()\n",
        "                if var < blurry_threshold:\n",
        "                    os.remove(fp); removed += 1\n",
        "            except:\n",
        "                try:\n",
        "                    os.remove(fp); removed += 1\n",
        "                except:\n",
        "                    pass\n",
        "    return removed\n",
        "\n",
        "removed_blur = remove_blurry(merged_dir, blurry_threshold=100.0)\n",
        "print(f\"Removed blurry: {removed_blur}\")\n",
        "\n",
        "# ==== Recount after cleaning (per class + total) ====\n",
        "class_counts_clean = {}\n",
        "for cls in sorted(os.listdir(merged_dir)):\n",
        "    p = os.path.join(merged_dir, cls)\n",
        "    if os.path.isdir(p):\n",
        "        n = len([f for f in os.listdir(p)\n",
        "                 if f.lower().endswith(IMG_EXTS) and os.path.isfile(os.path.join(p, f))])\n",
        "        if n > 0:\n",
        "            class_counts_clean[cls] = n\n",
        "\n",
        "total_images_after_cleaning = sum(class_counts_clean.values())\n",
        "print(\"Class counts after cleaning:\", class_counts_clean)\n",
        "print(\"Total images after cleaning:\", total_images_after_cleaning)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PgXzkT8noiRt",
        "outputId": "0bbf5ddf-5f50-4c90-f250-a94d1d11eb1f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[?25l   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m0.0/296.7 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K   \u001b[91mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[91mâ•¸\u001b[0m\u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m122.9/296.7 kB\u001b[0m \u001b[31m4.3 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m296.7/296.7 kB\u001b[0m \u001b[31m4.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hUsing merged dataset from: /content/drive/MyDrive/Bacteria Paper/datasets/merged_dataset\n",
            "Total images before cleaning: 1588\n",
            "Removed near-duplicates: 40\n",
            "Removed blurry: 437\n",
            "Class counts after cleaning: {'Actinophrys': 24, 'Amoeba': 48, 'Arcella': 21, 'Aspidisca': 20, 'Ceratium': 32, 'Codosiga': 21, 'Colpoda': 27, 'Epistylis': 27, 'Euglena': 131, 'Euglypha': 12, 'Gonyaulax': 24, 'Gymnodinium': 20, 'Hydra': 44, 'Keratella_quadrala': 36, 'Noctiluca': 36, 'Paramecium': 146, 'Phacus': 15, 'Rod_bacteria': 68, 'Rotifera': 32, 'Siprostomum': 30, 'Spherical_bacteria': 72, 'Spiral_bacteria': 58, 'Stentor': 36, 'Stylonychia': 21, 'Synchaeta': 38, 'Vorticella': 15, 'Yeast': 57}\n",
            "Total images after cleaning: 1111\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os, shutil, random\n",
        "from collections import defaultdict\n",
        "\n",
        "# ====== Paths (edit only if you want a different output root) ======\n",
        "DST = \"/content/drive/MyDrive/Bacteria Paper/datasets/merged_dataset\"   # source (classes as subfolders)\n",
        "OUT_ROOT = \"/content/drive/MyDrive/Bacteria Paper/datasets/Processed\"   # destination root\n",
        "TRAIN_DIR = os.path.join(OUT_ROOT, \"train\")\n",
        "TEST_DIR  = os.path.join(OUT_ROOT, \"test\")\n",
        "\n",
        "# ====== Split config ======\n",
        "TEST_SIZE = 0.20           # Test = 20%, Train+Val = 80%\n",
        "SEED = 42                  # for reproducibility\n",
        "IMG_EXTS = {'.jpg', '.jpeg', '.png', '.bmp', '.tif', '.tiff', '.webp'}\n",
        "CLEAR_DEST = True          # delete OUT_ROOT/train and OUT_ROOT/test before writing\n",
        "\n",
        "random.seed(SEED)\n",
        "\n",
        "def is_image(fname):\n",
        "    return os.path.splitext(fname)[1].lower() in IMG_EXTS\n",
        "\n",
        "def list_class_files(root):\n",
        "    \"\"\"Return dict: class_name -> [absolute image paths]\"\"\"\n",
        "    classes = [d for d in os.listdir(root) if os.path.isdir(os.path.join(root, d))]\n",
        "    data = {}\n",
        "    for cls in sorted(classes):\n",
        "        cls_dir = os.path.join(root, cls)\n",
        "        files = [os.path.join(cls_dir, f) for f in os.listdir(cls_dir)\n",
        "                 if os.path.isfile(os.path.join(cls_dir, f)) and is_image(f)]\n",
        "        if files:\n",
        "            data[cls] = sorted(files)\n",
        "    if not data:\n",
        "        raise RuntimeError(f\"No class folders with images found under: {root}\")\n",
        "    return data\n",
        "\n",
        "def safe_mkdir(p, clear=False):\n",
        "    if clear and os.path.isdir(p):\n",
        "        shutil.rmtree(p)\n",
        "    os.makedirs(p, exist_ok=True)\n",
        "\n",
        "def stratified_split_per_class(files, test_size=TEST_SIZE):\n",
        "    \"\"\"\n",
        "    Return train_list, test_list with robust rounding for small classes:\n",
        "      - if n==1 -> all train\n",
        "      - if n in [2..4] -> test = 1\n",
        "      - else -> test = round(n * test_size), min 1, max n-1\n",
        "    \"\"\"\n",
        "    n = len(files)\n",
        "    idx = list(range(n))\n",
        "    random.shuffle(idx)\n",
        "\n",
        "    if n == 1:\n",
        "        ttest = 0\n",
        "    elif 2 <= n <= 4:\n",
        "        ttest = 1\n",
        "    else:\n",
        "        ttest = max(1, min(n-1, int(round(n * test_size))))\n",
        "\n",
        "    test_idx = set(idx[:ttest])\n",
        "    test_files  = [files[i] for i in range(n) if i in test_idx]\n",
        "    train_files = [files[i] for i in range(n) if i not in test_idx]\n",
        "    return train_files, test_files\n",
        "\n",
        "def copy_files(file_list, dst_dir):\n",
        "    os.makedirs(dst_dir, exist_ok=True)\n",
        "    for src in file_list:\n",
        "        basename = os.path.basename(src)\n",
        "        # keep original filename; if you prefer sequential naming, you can rename here\n",
        "        shutil.copy2(src, os.path.join(dst_dir, basename))\n",
        "\n",
        "def count_dir_images(root):\n",
        "    total = 0\n",
        "    per_class = {}\n",
        "    for cls in sorted(d for d in os.listdir(root) if os.path.isdir(os.path.join(root, d))):\n",
        "        cls_dir = os.path.join(root, cls)\n",
        "        n = sum(1 for f in os.listdir(cls_dir)\n",
        "                if os.path.isfile(os.path.join(cls_dir, f)) and is_image(f))\n",
        "        per_class[cls] = n\n",
        "        total += n\n",
        "    return total, per_class\n",
        "\n",
        "# ====== Run ======\n",
        "# Load source data\n",
        "data = list_class_files(DST)\n",
        "\n",
        "# Prepare destination\n",
        "safe_mkdir(OUT_ROOT, clear=False)\n",
        "safe_mkdir(TRAIN_DIR, clear=CLEAR_DEST)\n",
        "safe_mkdir(TEST_DIR,  clear=CLEAR_DEST)\n",
        "\n",
        "# Split & copy\n",
        "summary = defaultdict(lambda: {\"train\":0, \"test\":0})\n",
        "for cls, files in data.items():\n",
        "    tr, te = stratified_split_per_class(files, TEST_SIZE)\n",
        "    copy_files(tr, os.path.join(TRAIN_DIR, cls))\n",
        "    copy_files(te, os.path.join(TEST_DIR,  cls))\n",
        "    summary[cls][\"train\"] = len(tr)\n",
        "    summary[cls][\"test\"]  = len(te)\n",
        "\n",
        "# Report\n",
        "train_total, train_per = count_dir_images(TRAIN_DIR)\n",
        "test_total,  test_per  = count_dir_images(TEST_DIR)\n",
        "\n",
        "print(\"\\n===== Split Summary (per class) =====\")\n",
        "for cls in sorted(summary):\n",
        "    print(f\"  - {cls}: train {summary[cls]['train']:4d} | test {summary[cls]['test']:4d}\")\n",
        "\n",
        "print(\"\\n===== Totals =====\")\n",
        "print(f\"Train images: {train_total}\")\n",
        "print(f\"Test  images: {test_total}\")\n",
        "print(f\"All (train+test): {train_total + test_total}\")\n",
        "\n",
        "# Optional: quick sanity check that totals match source\n",
        "src_total = sum(len(v) for v in data.values())\n",
        "if (train_total + test_total) != src_total:\n",
        "    print(f\"âš ï¸ Mismatch: source={src_total}, written={train_total + test_total} (check file filters/extensions)\")\n",
        "else:\n",
        "    print(\"âœ… Split complete and totals match source.\")\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "t8Hq8MtcopWw",
        "outputId": "8582bbbb-f859-4f54-9d4c-3585e68a0214"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "===== Split Summary (per class) =====\n",
            "  - Actinophrys: train   19 | test    5\n",
            "  - Amoeba: train   38 | test   10\n",
            "  - Arcella: train   17 | test    4\n",
            "  - Aspidisca: train   16 | test    4\n",
            "  - Ceratium: train   26 | test    6\n",
            "  - Codosiga: train   17 | test    4\n",
            "  - Colpoda: train   22 | test    5\n",
            "  - Epistylis: train   22 | test    5\n",
            "  - Euglena: train  105 | test   26\n",
            "  - Euglypha: train   10 | test    2\n",
            "  - Gonyaulax: train   19 | test    5\n",
            "  - Gymnodinium: train   16 | test    4\n",
            "  - Hydra: train   35 | test    9\n",
            "  - Keratella_quadrala: train   29 | test    7\n",
            "  - Noctiluca: train   29 | test    7\n",
            "  - Paramecium: train  117 | test   29\n",
            "  - Phacus: train   12 | test    3\n",
            "  - Rod_bacteria: train   54 | test   14\n",
            "  - Rotifera: train   26 | test    6\n",
            "  - Siprostomum: train   24 | test    6\n",
            "  - Spherical_bacteria: train   58 | test   14\n",
            "  - Spiral_bacteria: train   46 | test   12\n",
            "  - Stentor: train   29 | test    7\n",
            "  - Stylonychia: train   17 | test    4\n",
            "  - Synchaeta: train   30 | test    8\n",
            "  - Vorticella: train   12 | test    3\n",
            "  - Yeast: train   46 | test   11\n",
            "\n",
            "===== Totals =====\n",
            "Train images: 891\n",
            "Test  images: 220\n",
            "All (train+test): 1111\n",
            "âœ… Split complete and totals match source.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import numpy as np\n",
        "from PIL import Image, ImageEnhance\n",
        "from tqdm import tqdm\n",
        "\n",
        "# --- settings ---\n",
        "SRC_DIR = \"/content/drive/MyDrive/Bacteria Paper/datasets/Processed/train\"      # your current train dir\n",
        "DST_DIR = \"/content/drive/MyDrive/Bacteria Paper/datasets/Processed/train_320\"  # NEW output dir (will be created)\n",
        "IMAGE_SIZE = (320, 320)\n",
        "VALID_EXTS = {\".jpg\", \".jpeg\", \".png\", \".bmp\", \".tif\", \".tiff\", \".webp\"}\n",
        "\n",
        "def is_image(fname):\n",
        "    return os.path.splitext(fname.lower())[1] in VALID_EXTS\n",
        "\n",
        "def calculate_mean_std(image_dir, image_size=IMAGE_SIZE):\n",
        "    \"\"\"\n",
        "    Compute dataset mean/std AFTER a deterministic resize to image_size.\n",
        "    Returns mean, std as 3-element numpy arrays in 0..1 scale (RGB).\n",
        "    \"\"\"\n",
        "    channel_sum = np.zeros(3, dtype=np.float64)\n",
        "    channel_sqsum = np.zeros(3, dtype=np.float64)\n",
        "    pixel_count = 0\n",
        "\n",
        "    class_names = [d for d in os.listdir(image_dir) if os.path.isdir(os.path.join(image_dir, d))]\n",
        "    for cls in class_names:\n",
        "        class_path = os.path.join(image_dir, cls)\n",
        "        files = [f for f in os.listdir(class_path) if is_image(f)]\n",
        "        for img_name in tqdm(files, desc=f\"Calculating stats in {cls}\"):\n",
        "            img_path = os.path.join(class_path, img_name)\n",
        "            try:\n",
        "                img = Image.open(img_path).convert(\"RGB\").resize(image_size, resample=Image.BICUBIC)\n",
        "                arr = np.asarray(img, dtype=np.float32) / 255.0   # [H, W, 3]\n",
        "                h, w, _ = arr.shape\n",
        "                pixel_count += h * w\n",
        "                channel_sum += arr.reshape(-1, 3).sum(axis=0)\n",
        "                channel_sqsum += (arr.reshape(-1, 3) ** 2).sum(axis=0)\n",
        "            except Exception as e:\n",
        "                print(f\"Skipping {img_path}: {e}\")\n",
        "\n",
        "    mean = channel_sum / pixel_count\n",
        "    var = channel_sqsum / pixel_count - mean**2\n",
        "    std = np.sqrt(np.maximum(var, 1e-12))\n",
        "    return mean, std\n",
        "\n",
        "def preprocess_training_images(src_dir, dst_dir, image_size=IMAGE_SIZE,\n",
        "                               sharpness=1.5, contrast=1.2):\n",
        "    \"\"\"\n",
        "    Resize to image_size and gently enhance sharpness/contrast.\n",
        "    Writes results to dst_dir, preserving class subfolders and filenames.\n",
        "    NO normalization on-disk (normalize in training transforms).\n",
        "    \"\"\"\n",
        "    os.makedirs(dst_dir, exist_ok=True)\n",
        "    class_names = [d for d in os.listdir(src_dir) if os.path.isdir(os.path.join(src_dir, d))]\n",
        "    for cls in class_names:\n",
        "        src_cls = os.path.join(src_dir, cls)\n",
        "        dst_cls = os.path.join(dst_dir, cls)\n",
        "        os.makedirs(dst_cls, exist_ok=True)\n",
        "\n",
        "        files = [f for f in os.listdir(src_cls) if is_image(f)]\n",
        "        for img_name in tqdm(files, desc=f\"Preprocessing {cls}\"):\n",
        "            src_path = os.path.join(src_cls, img_name)\n",
        "            dst_path = os.path.join(dst_cls, img_name)\n",
        "            try:\n",
        "                image = Image.open(src_path).convert(\"RGB\")\n",
        "                # resize first (bicubic)\n",
        "                image = image.resize(image_size, resample=Image.BICUBIC)\n",
        "                # gentle enhancements (same as your script)\n",
        "                image = ImageEnhance.Sharpness(image).enhance(sharpness)\n",
        "                image = ImageEnhance.Contrast(image).enhance(contrast)\n",
        "                image.save(dst_path)  # write processed file\n",
        "            except Exception as e:\n",
        "                print(f\"Error processing {src_path}: {e}\")\n",
        "\n",
        "# 1) compute mean/std at 320x320 on your ORIGINAL train dir\n",
        "mean, std = calculate_mean_std(SRC_DIR, image_size=IMAGE_SIZE)\n",
        "print(\"âœ… Dataset Mean @320:\", mean)\n",
        "print(\"âœ… Dataset Std  @320:\", std)\n",
        "\n",
        "# 2) write resized/enhanced images to a new train_320 dir\n",
        "preprocess_training_images(SRC_DIR, DST_DIR, image_size=IMAGE_SIZE)\n",
        "print(f\"âœ… Preprocessing completed: {DST_DIR}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LvST95pNo5la",
        "outputId": "d3a89c88-cca4-491b-ce0e-6b0b065f4be2"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Calculating stats in Actinophrys: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 19/19 [00:00<00:00, 48.96it/s]\n",
            "Calculating stats in Amoeba: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 38/38 [00:00<00:00, 48.01it/s]\n",
            "Calculating stats in Arcella: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 17/17 [00:00<00:00, 68.16it/s]\n",
            "Calculating stats in Aspidisca: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 16/16 [00:00<00:00, 68.32it/s]\n",
            "Calculating stats in Ceratium: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 26/26 [00:00<00:00, 74.99it/s]\n",
            "Calculating stats in Codosiga: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 17/17 [00:00<00:00, 63.22it/s]\n",
            "Calculating stats in Colpoda: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 22/22 [00:00<00:00, 52.71it/s]\n",
            "Calculating stats in Epistylis: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 22/22 [00:00<00:00, 63.79it/s]\n",
            "Calculating stats in Euglena: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 105/105 [00:02<00:00, 46.58it/s]\n",
            "Calculating stats in Euglypha: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 10/10 [00:00<00:00, 65.12it/s]\n",
            "Calculating stats in Gonyaulax: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 19/19 [00:00<00:00, 80.17it/s]\n",
            "Calculating stats in Gymnodinium: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 16/16 [00:00<00:00, 68.34it/s]\n",
            "Calculating stats in Hydra: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 35/35 [00:00<00:00, 51.14it/s]\n",
            "Calculating stats in Keratella_quadrala: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 29/29 [00:00<00:00, 76.00it/s]\n",
            "Calculating stats in Noctiluca: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 29/29 [00:00<00:00, 75.93it/s]\n",
            "Calculating stats in Paramecium: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 117/117 [00:02<00:00, 42.33it/s]\n",
            "Calculating stats in Phacus: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 12/12 [00:00<00:00, 83.37it/s]\n",
            "Calculating stats in Rod_bacteria: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 54/54 [00:01<00:00, 34.31it/s]\n",
            "Calculating stats in Rotifera: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 26/26 [00:00<00:00, 57.25it/s]\n",
            "Calculating stats in Siprostomum: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 24/24 [00:00<00:00, 46.44it/s]\n",
            "Calculating stats in Spherical_bacteria: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 58/58 [00:01<00:00, 30.66it/s]\n",
            "Calculating stats in Spiral_bacteria: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 46/46 [00:00<00:00, 57.60it/s]\n",
            "Calculating stats in Stentor: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 29/29 [00:00<00:00, 64.41it/s]\n",
            "Calculating stats in Stylonychia: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 17/17 [00:00<00:00, 60.48it/s]\n",
            "Calculating stats in Synchaeta: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 30/30 [00:00<00:00, 65.52it/s]\n",
            "Calculating stats in Vorticella: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 12/12 [00:00<00:00, 62.80it/s]\n",
            "Calculating stats in Yeast: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 46/46 [00:00<00:00, 48.25it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "âœ… Dataset Mean @320: [0.54200659 0.57473042 0.55953813]\n",
            "âœ… Dataset Std  @320: [0.2644171  0.25311802 0.26218295]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Preprocessing Actinophrys: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 19/19 [00:00<00:00, 43.32it/s]\n",
            "Preprocessing Amoeba: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 38/38 [00:01<00:00, 33.16it/s]\n",
            "Preprocessing Arcella: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 17/17 [00:00<00:00, 40.57it/s]\n",
            "Preprocessing Aspidisca: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 16/16 [00:00<00:00, 41.26it/s]\n",
            "Preprocessing Ceratium: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 26/26 [00:00<00:00, 41.53it/s]\n",
            "Preprocessing Codosiga: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 17/17 [00:00<00:00, 41.21it/s]\n",
            "Preprocessing Colpoda: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 22/22 [00:00<00:00, 33.49it/s]\n",
            "Preprocessing Epistylis: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 22/22 [00:00<00:00, 37.26it/s]\n",
            "Preprocessing Euglena: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 105/105 [00:03<00:00, 28.22it/s]\n",
            "Preprocessing Euglypha: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 10/10 [00:00<00:00, 32.41it/s]\n",
            "Preprocessing Gonyaulax: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 19/19 [00:00<00:00, 32.04it/s]\n",
            "Preprocessing Gymnodinium: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 16/16 [00:00<00:00, 28.53it/s]\n",
            "Preprocessing Hydra: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 35/35 [00:01<00:00, 31.27it/s]\n",
            "Preprocessing Keratella_quadrala: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 29/29 [00:00<00:00, 40.43it/s]\n",
            "Preprocessing Noctiluca: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 29/29 [00:00<00:00, 42.21it/s]\n",
            "Preprocessing Paramecium: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 117/117 [00:03<00:00, 30.79it/s]\n",
            "Preprocessing Phacus: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 12/12 [00:00<00:00, 35.72it/s]\n",
            "Preprocessing Rod_bacteria: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 54/54 [00:01<00:00, 29.16it/s]\n",
            "Preprocessing Rotifera: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 26/26 [00:00<00:00, 40.78it/s]\n",
            "Preprocessing Siprostomum: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 24/24 [00:00<00:00, 41.30it/s]\n",
            "Preprocessing Spherical_bacteria: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 58/58 [00:02<00:00, 25.03it/s]\n",
            "Preprocessing Spiral_bacteria: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 46/46 [00:01<00:00, 29.00it/s]\n",
            "Preprocessing Stentor: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 29/29 [00:00<00:00, 30.59it/s]\n",
            "Preprocessing Stylonychia: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 17/17 [00:00<00:00, 41.20it/s]\n",
            "Preprocessing Synchaeta: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 30/30 [00:00<00:00, 39.51it/s]\n",
            "Preprocessing Vorticella: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 12/12 [00:00<00:00, 43.31it/s]\n",
            "Preprocessing Yeast: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 46/46 [00:01<00:00, 27.32it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "âœ… Preprocessing completed: /content/drive/MyDrive/Bacteria Paper/datasets/Processed/train_320\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "from PIL import Image\n",
        "from tqdm import tqdm\n",
        "\n",
        "SRC_DIR = \"/content/drive/MyDrive/Bacteria Paper/datasets/Processed/test\"\n",
        "DST_DIR = \"/content/drive/MyDrive/Bacteria Paper/datasets/Processed/test_320\"\n",
        "IMAGE_SIZE = (320, 320)\n",
        "\n",
        "VALID_EXTS = {\".jpg\", \".jpeg\", \".png\", \".bmp\", \".tif\", \".tiff\", \".webp\"}\n",
        "def is_image(fname): return os.path.splitext(fname.lower())[1] in VALID_EXTS\n",
        "\n",
        "os.makedirs(DST_DIR, exist_ok=True)\n",
        "classes = [d for d in os.listdir(SRC_DIR) if os.path.isdir(os.path.join(SRC_DIR, d))]\n",
        "\n",
        "for cls in classes:\n",
        "    src_cls = os.path.join(SRC_DIR, cls)\n",
        "    dst_cls = os.path.join(DST_DIR, cls)\n",
        "    os.makedirs(dst_cls, exist_ok=True)\n",
        "\n",
        "    files = [f for f in os.listdir(src_cls) if is_image(f)]\n",
        "    for img_name in tqdm(files, desc=f\"Resizing {cls}\"):\n",
        "        src_path = os.path.join(src_cls, img_name)\n",
        "        dst_path = os.path.join(dst_cls, img_name)\n",
        "        try:\n",
        "            img = Image.open(src_path).convert(\"RGB\")\n",
        "            img = img.resize(IMAGE_SIZE, resample=Image.BICUBIC)\n",
        "            img.save(dst_path)\n",
        "        except Exception as e:\n",
        "            print(f\"Error processing {src_path}: {e}\")\n",
        "\n",
        "print(f\"âœ… Test images written to {DST_DIR} at 320Ã—320.\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RWlPI6xDpAN6",
        "outputId": "329f845e-89c7-40dd-e26c-f6748891cf06"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Resizing Actinophrys: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 5/5 [00:00<00:00, 43.28it/s]\n",
            "Resizing Amoeba: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 10/10 [00:00<00:00, 42.13it/s]\n",
            "Resizing Arcella: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 4/4 [00:00<00:00, 46.52it/s]\n",
            "Resizing Aspidisca: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 4/4 [00:00<00:00, 68.62it/s]\n",
            "Resizing Ceratium: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 6/6 [00:00<00:00, 69.65it/s]\n",
            "Resizing Codosiga: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 4/4 [00:00<00:00, 57.88it/s]\n",
            "Resizing Colpoda: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 5/5 [00:00<00:00, 61.10it/s]\n",
            "Resizing Epistylis: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 5/5 [00:00<00:00, 25.82it/s]\n",
            "Resizing Euglena: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 26/26 [00:00<00:00, 43.60it/s]\n",
            "Resizing Euglypha: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2/2 [00:00<00:00, 57.44it/s]\n",
            "Resizing Gonyaulax: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 5/5 [00:00<00:00, 63.39it/s]\n",
            "Resizing Gymnodinium: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 4/4 [00:00<00:00, 61.16it/s]\n",
            "Resizing Hydra: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 9/9 [00:00<00:00, 47.39it/s]\n",
            "Resizing Keratella_quadrala: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 7/7 [00:00<00:00, 63.29it/s]\n",
            "Resizing Noctiluca: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 7/7 [00:00<00:00, 46.37it/s]\n",
            "Resizing Paramecium: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 29/29 [00:02<00:00, 14.11it/s]\n",
            "Resizing Phacus: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 3/3 [00:00<00:00, 39.24it/s]\n",
            "Resizing Rod_bacteria: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 14/14 [00:00<00:00, 24.86it/s]\n",
            "Resizing Rotifera: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 6/6 [00:00<00:00, 12.67it/s]\n",
            "Resizing Siprostomum: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 6/6 [00:00<00:00, 16.41it/s]\n",
            "Resizing Spherical_bacteria: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 14/14 [00:00<00:00, 20.45it/s]\n",
            "Resizing Spiral_bacteria: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 12/12 [00:00<00:00, 23.28it/s]\n",
            "Resizing Stentor: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 7/7 [00:00<00:00, 32.48it/s]\n",
            "Resizing Stylonychia: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 4/4 [00:00<00:00, 38.85it/s]\n",
            "Resizing Synchaeta: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 8/8 [00:00<00:00, 17.98it/s]\n",
            "Resizing Vorticella: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 3/3 [00:00<00:00, 26.18it/s]\n",
            "Resizing Yeast: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 11/11 [00:00<00:00, 26.17it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "âœ… Test images written to /content/drive/MyDrive/Bacteria Paper/datasets/Processed/test_320 at 320Ã—320.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# ===================== ResNet50 + K-Fold + Albumentations \"Smart Aug\" (no MixUp/CutMix) =====================\n",
        "import os, time, random, numpy as np, torch\n",
        "from torch import nn, optim\n",
        "from torch.utils.data import DataLoader, Dataset\n",
        "from torchvision import datasets, models\n",
        "from torchvision.models import ResNet50_Weights\n",
        "from sklearn.metrics import accuracy_score, f1_score, classification_report\n",
        "from sklearn.model_selection import StratifiedKFold\n",
        "from PIL import Image\n",
        "from torchvision.transforms.functional import to_tensor\n",
        "\n",
        "# Albumentations\n",
        "import albumentations as A\n",
        "from albumentations.pytorch import ToTensorV2\n",
        "import cv2  # for border_mode constants\n",
        "\n",
        "# ---------- Paths ----------\n",
        "train_dir = \"/content/drive/MyDrive/Bacteria Paper/datasets/Processed/train_320\"\n",
        "test_dir  = \"/content/drive/MyDrive/Bacteria Paper/datasets/Processed/test_320\"\n",
        "\n",
        "# ---------- Your dataset mean/std (0â€“1 scale; RGB) ----------\n",
        "DATA_MEAN = [0.54200659, 0.57473042, 0.55953813]   # <-- REPLACE with your computed values\n",
        "DATA_STD  = [0.2644171,  0.25311802, 0.26218295]   # <-- REPLACE with your computed values\n",
        "\n",
        "# ---------- Config ----------\n",
        "SEED = 42\n",
        "IMG_SIZE = 320              # try 288 or 320 if VRAM allows\n",
        "N_SPLITS = 5\n",
        "EPOCHS = 60\n",
        "PATIENCE = 15\n",
        "\n",
        "HEAD_WARMUP_EPOCHS   = 6     # train only FC head initially\n",
        "UNFREEZE_LAYER4_EPOCH = 7    # unfreeze layer4\n",
        "UNFREEZE_L34_EPOCH    = 18   # unfreeze layer3+4\n",
        "\n",
        "LR_HEAD = 1e-3\n",
        "LR_BB   = 3e-4\n",
        "WEIGHT_DECAY = 5e-5\n",
        "BATCH_GPU, BATCH_CPU = 64, 16\n",
        "NUM_WORKERS = min(8, os.cpu_count() or 2)\n",
        "\n",
        "# Smart aug knobs\n",
        "ROT_LIMIT = 20            # gentle rotation\n",
        "HFLIP_P   = 0.5\n",
        "VFLIP_P   = 0.2\n",
        "CLAHE_P   = 0.2\n",
        "GBLUR_P   = 0.25\n",
        "MBLUR_P   = 0.15\n",
        "GAMMA_P   = 0.3\n",
        "HSV_P     = 0.3\n",
        "RRC_SCALE = (0.85, 1.0)   # RandomResizedCrop-like zoom range\n",
        "RRC_RATIO = (0.9, 1.1)\n",
        "\n",
        "# ---------- Seed & Device ----------\n",
        "def set_seed(s):\n",
        "    random.seed(s); np.random.seed(s); torch.manual_seed(s); torch.cuda.manual_seed_all(s)\n",
        "set_seed(SEED)\n",
        "torch.backends.cudnn.benchmark = True\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "print(\"Device:\", device)\n",
        "\n",
        "# ---------- Albumentations pipelines (version-proof) ----------\n",
        "print(\"Albumentations version:\", A.__version__)  # for visibility\n",
        "\n",
        "_first_train = []\n",
        "try:\n",
        "    # Validate signature once; if it errors we will fallback\n",
        "    _ = A.RandomResizedCrop(height=IMG_SIZE, width=IMG_SIZE, scale=RRC_SCALE, ratio=RRC_RATIO, p=1.0)\n",
        "    _first_train.append(\n",
        "        A.RandomResizedCrop(height=IMG_SIZE, width=IMG_SIZE, scale=RRC_SCALE, ratio=RRC_RATIO, p=1.0)\n",
        "    )\n",
        "except Exception as e:\n",
        "    print(\"[WARN] RandomResizedCrop not usable in this Albumentations build. \"\n",
        "          \"Using Resize+RandomCrop fallback. Error:\", e)\n",
        "    _first_train += [\n",
        "        A.Resize(height=int(IMG_SIZE * 1.15), width=int(IMG_SIZE * 1.15)),\n",
        "        A.RandomCrop(height=IMG_SIZE, width=IMG_SIZE),\n",
        "    ]\n",
        "\n",
        "train_aug = A.Compose(\n",
        "    _first_train + [\n",
        "        A.HorizontalFlip(p=HFLIP_P),\n",
        "        A.VerticalFlip(p=VFLIP_P),\n",
        "        A.Rotate(limit=ROT_LIMIT, p=0.5, border_mode=cv2.BORDER_REFLECT_101),\n",
        "        A.RandomBrightnessContrast(p=0.5),\n",
        "        A.RandomGamma(p=GAMMA_P),\n",
        "        A.HueSaturationValue(p=HSV_P),\n",
        "        A.GaussianBlur(blur_limit=(3, 5), p=GBLUR_P),\n",
        "        A.MotionBlur(blur_limit=5, p=MBLUR_P),\n",
        "        A.CLAHE(p=CLAHE_P),\n",
        "        A.Normalize(mean=DATA_MEAN, std=DATA_STD),\n",
        "        ToTensorV2(),\n",
        "    ]\n",
        ")\n",
        "\n",
        "val_aug = A.Compose([\n",
        "    A.Resize(height=int(IMG_SIZE * 1.15), width=int(IMG_SIZE * 1.15)),\n",
        "    A.CenterCrop(height=IMG_SIZE, width=IMG_SIZE),\n",
        "    A.Normalize(mean=DATA_MEAN, std=DATA_STD),\n",
        "    ToTensorV2(),\n",
        "])\n",
        "\n",
        "# ---------- Dataset wrapper using Albumentations ----------\n",
        "class AlbDataset(Dataset):\n",
        "    def __init__(self, base, indices=None, transform=None):\n",
        "        self.base = base\n",
        "        self.samples = base.samples\n",
        "        self.indices = list(indices) if indices is not None else list(range(len(self.samples)))\n",
        "        self.targets = np.array([self.base.targets[i] for i in self.indices], dtype=np.int64)\n",
        "        self.transform = transform\n",
        "    def __len__(self): return len(self.indices)\n",
        "    def __getitem__(self, i):\n",
        "        idx = self.indices[i]\n",
        "        path, label = self.samples[idx]\n",
        "        img = np.array(Image.open(path).convert(\"RGB\"))\n",
        "        if self.transform:\n",
        "            img = self.transform(image=img)[\"image\"]\n",
        "        else:\n",
        "            img = to_tensor(Image.fromarray(img)).float()\n",
        "        return img, label\n",
        "\n",
        "# ---------- Data ----------\n",
        "base_full = datasets.ImageFolder(root=train_dir, transform=None)\n",
        "class_names = base_full.classes\n",
        "num_classes = len(class_names)\n",
        "all_targets = np.array(base_full.targets, dtype=np.int64)\n",
        "print(f\"Train images: {len(base_full)} | Classes ({num_classes}): {class_names}\")\n",
        "\n",
        "# ---------- Model / Optim ----------\n",
        "def build_resnet50(num_classes):\n",
        "    m = models.resnet50(weights=ResNet50_Weights.DEFAULT)\n",
        "    in_f = m.fc.in_features\n",
        "    m.fc = nn.Linear(in_f, num_classes)\n",
        "    m = m.to(device)\n",
        "    if device.type == \"cuda\":\n",
        "        m = m.to(memory_format=torch.channels_last)\n",
        "    return m\n",
        "\n",
        "def set_trainable(m, mode):\n",
        "    # mode: \"head\" | \"layer4\" | \"layer3_4\"\n",
        "    for p in m.parameters(): p.requires_grad = False\n",
        "    if mode == \"head\":\n",
        "        for p in m.fc.parameters(): p.requires_grad = True\n",
        "    elif mode == \"layer4\":\n",
        "        for n,p in m.named_parameters():\n",
        "            if n.startswith(\"layer4.\"): p.requires_grad = True\n",
        "        for p in m.fc.parameters(): p.requires_grad = True\n",
        "    elif mode == \"layer3_4\":\n",
        "        for n,p in m.named_parameters():\n",
        "            if n.startswith((\"layer3.\",\"layer4.\")): p.requires_grad = True\n",
        "        for p in m.fc.parameters(): p.requires_grad = True\n",
        "\n",
        "def make_optimizer(model):\n",
        "    head_params, bb_params = [], []\n",
        "    for n,p in model.named_parameters():\n",
        "        if not p.requires_grad: continue\n",
        "        (head_params if n.startswith(\"fc.\") else bb_params).append(p)\n",
        "    return optim.AdamW(\n",
        "        [{'params': head_params, 'lr': LR_HEAD},\n",
        "         {'params': bb_params,   'lr': LR_BB}],\n",
        "        weight_decay=WEIGHT_DECAY\n",
        "    )\n",
        "\n",
        "# ---------- Train/Eval ----------\n",
        "@torch.no_grad()\n",
        "def evaluate(model, loader, criterion):\n",
        "    model.eval()\n",
        "    tot, y_true, y_pred = 0.0, [], []\n",
        "    for xb,yb in loader:\n",
        "        xb = xb.to(device, non_blocking=True); yb = yb.to(device, non_blocking=True)\n",
        "        with torch.autocast(device_type='cuda', enabled=(device.type=='cuda')):\n",
        "            logits = model(xb)\n",
        "            loss = criterion(logits, yb)\n",
        "        tot += loss.item()\n",
        "        y_pred += logits.argmax(1).cpu().tolist()\n",
        "        y_true += yb.cpu().tolist()\n",
        "    loss = tot / max(1, len(loader))\n",
        "    acc  = accuracy_score(y_true, y_pred)\n",
        "    f1m  = f1_score(y_true, y_pred, average=\"macro\", zero_division=0)\n",
        "    return loss, acc, f1m\n",
        "\n",
        "def train_one_epoch(model, loader, optimizer, criterion):\n",
        "    model.train()\n",
        "    tot, y_true, y_pred = 0.0, [], []\n",
        "    for xb,yb in loader:\n",
        "        xb = xb.to(device, non_blocking=True); yb = yb.to(device, non_blocking=True)\n",
        "        optimizer.zero_grad(set_to_none=True)\n",
        "        with torch.autocast(device_type='cuda', enabled=(device.type=='cuda')):\n",
        "            logits = model(xb)\n",
        "            loss = criterion(logits, yb)\n",
        "        loss.backward(); optimizer.step()\n",
        "        tot += loss.item()\n",
        "        with torch.no_grad():\n",
        "            y_pred += logits.argmax(1).cpu().tolist()\n",
        "            y_true += yb.cpu().tolist()\n",
        "    loss = tot / max(1, len(loader))\n",
        "    acc  = accuracy_score(y_true, y_pred)\n",
        "    f1m  = f1_score(y_true, y_pred, average=\"macro\", zero_division=0)\n",
        "    return loss, acc, f1m\n",
        "\n",
        "def make_loader(ds, shuffle):\n",
        "    return DataLoader(\n",
        "        ds,\n",
        "        batch_size=(BATCH_GPU if device.type=='cuda' else BATCH_CPU),\n",
        "        shuffle=shuffle, num_workers=NUM_WORKERS,\n",
        "        pin_memory=(device.type=='cuda'),\n",
        "        persistent_workers=(NUM_WORKERS>0 and device.type=='cuda')\n",
        "    )\n",
        "\n",
        "def run_fold(fold, tr_idx, va_idx):\n",
        "    ds_tr = AlbDataset(base_full, tr_idx, transform=train_aug)\n",
        "    ds_va = AlbDataset(base_full, va_idx, transform=val_aug)\n",
        "    dl_tr = make_loader(ds_tr, shuffle=True)\n",
        "    dl_va = make_loader(ds_va, shuffle=False)\n",
        "\n",
        "    model = build_resnet50(num_classes)\n",
        "    set_trainable(model, \"head\")\n",
        "    optimizer = make_optimizer(model)\n",
        "    scheduler = optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max=EPOCHS)\n",
        "    criterion = nn.CrossEntropyLoss()\n",
        "\n",
        "    best = {\"f1\": -1, \"ep\": -1, \"state\": None, \"val_acc\": 0.0}\n",
        "    wait = 0\n",
        "\n",
        "    for ep in range(1, EPOCHS+1):\n",
        "        # gradual unfreeze\n",
        "        if ep == UNFREEZE_LAYER4_EPOCH:\n",
        "            set_trainable(model, \"layer4\"); optimizer = make_optimizer(model)\n",
        "        if ep == UNFREEZE_L34_EPOCH:\n",
        "            set_trainable(model, \"layer3_4\"); optimizer = make_optimizer(model)\n",
        "\n",
        "        t0 = time.time()\n",
        "        tr_loss, tr_acc, tr_f1 = train_one_epoch(model, dl_tr, optimizer, criterion)\n",
        "        va_loss, va_acc, va_f1 = evaluate(model, dl_va, criterion)\n",
        "        scheduler.step()\n",
        "\n",
        "        print(f\"[Fold {fold}] Ep {ep:02d}/{EPOCHS} | \"\n",
        "              f\"Train loss {tr_loss:.4f} acc {tr_acc*100:5.2f}% f1M {tr_f1:.4f} | \"\n",
        "              f\"Val loss {va_loss:.4f} acc {va_acc*100:5.2f}% f1M {va_f1:.4f} | {time.time()-t0:.1f}s\")\n",
        "\n",
        "        if va_f1 > best[\"f1\"]:\n",
        "            best.update({\"f1\": va_f1, \"ep\": ep, \"state\": {k:v.cpu() for k,v in model.state_dict().items()}, \"val_acc\": va_acc})\n",
        "            wait = 0\n",
        "        else:\n",
        "            wait += 1\n",
        "            if wait >= PATIENCE:\n",
        "                print(f\"[Fold {fold}] Early stop @ {ep} (best @ {best['ep']}, F1M={best['f1']:.4f})\")\n",
        "                break\n",
        "\n",
        "    ckpt = f\"resnet50_alb_smart_fold{fold}.pt\"\n",
        "    if best[\"state\"] is not None:\n",
        "        torch.save(best[\"state\"], ckpt)\n",
        "\n",
        "    # Final val from best\n",
        "    model.load_state_dict(torch.load(ckpt, map_location=device))\n",
        "    va_loss, va_acc, va_f1 = evaluate(model, dl_va, criterion)\n",
        "    print(f\"[Fold {fold}] Best Val â†’ acc {va_acc*100:5.2f}% | f1M {va_f1:.4f} (epoch {best['ep']})\")\n",
        "    return {\"fold\": fold, \"val_acc\": va_acc, \"val_f1\": va_f1, \"epoch\": best[\"ep\"], \"ckpt\": ckpt}\n",
        "\n",
        "# ---------- K-Fold ----------\n",
        "skf = StratifiedKFold(n_splits=N_SPLITS, shuffle=True, random_state=SEED)\n",
        "fold_summaries = []\n",
        "for fold, (tr_idx, va_idx) in enumerate(skf.split(np.arange(len(all_targets)), all_targets), start=1):\n",
        "    print(f\"\\n===== K-Fold {fold}/{N_SPLITS} ===== (train {len(tr_idx)} | val {len(va_idx)})\")\n",
        "    fs = run_fold(fold, tr_idx, va_idx)\n",
        "    fold_summaries.append(fs)\n",
        "\n",
        "cv_accs = [fs[\"val_acc\"] for fs in fold_summaries]\n",
        "cv_f1s  = [fs[\"val_f1\"] for fs in fold_summaries]\n",
        "print(\"\\n===== CV Summary (best epoch per fold) =====\")\n",
        "print(\"Val Accs:\", [f\"{a*100:0.2f}%\" for a in cv_accs], f\"| MeanÂ±Std = {np.mean(cv_accs)*100:0.2f}% Â± {np.std(cv_accs)*100:0.2f}%\")\n",
        "print(\"Val F1M :\", [f\"{f:0.4f}\" for f in cv_f1s],        f\"| MeanÂ±Std = {np.mean(cv_f1s):0.4f} Â± {np.std(cv_f1s):0.4f}\")\n",
        "\n",
        "# ---------- Full-train on train/, evaluate on test/ ----------\n",
        "if os.path.isdir(test_dir):\n",
        "    full_base  = datasets.ImageFolder(root=train_dir, transform=None)\n",
        "    test_base  = datasets.ImageFolder(root=test_dir, transform=None)\n",
        "    full_ds    = AlbDataset(full_base, transform=train_aug)\n",
        "    test_ds    = AlbDataset(test_base, transform=val_aug)\n",
        "\n",
        "    full_loader = DataLoader(full_ds, batch_size=(BATCH_GPU if device.type=='cuda' else BATCH_CPU),\n",
        "                             shuffle=True, num_workers=NUM_WORKERS, pin_memory=(device.type=='cuda'),\n",
        "                             persistent_workers=(NUM_WORKERS>0 and device.type=='cuda'))\n",
        "    test_loader = DataLoader(test_ds, batch_size=(BATCH_GPU if device.type=='cuda' else BATCH_CPU),\n",
        "                             shuffle=False, num_workers=NUM_WORKERS, pin_memory=(device.type=='cuda'),\n",
        "                             persistent_workers=(NUM_WORKERS>0 and device.type=='cuda'))\n",
        "\n",
        "    model = build_resnet50(num_classes)\n",
        "    set_trainable(model, \"head\")\n",
        "    optimizer = make_optimizer(model)\n",
        "    scheduler = optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max=EPOCHS)\n",
        "    criterion = nn.CrossEntropyLoss()\n",
        "\n",
        "    best_loss, wait = 1e9, 0\n",
        "    best_full_path = \"resnet50_alb_smart_fulltrain.pt\"\n",
        "    for ep in range(1, EPOCHS+1):\n",
        "        if ep == UNFREEZE_LAYER4_EPOCH:\n",
        "            set_trainable(model, \"layer4\"); optimizer = make_optimizer(model)\n",
        "        if ep == UNFREEZE_L34_EPOCH:\n",
        "            set_trainable(model, \"layer3_4\"); optimizer = make_optimizer(model)\n",
        "\n",
        "        t0 = time.time()\n",
        "        tr_loss, tr_acc, tr_f1 = train_one_epoch(model, full_loader, optimizer, criterion)\n",
        "        scheduler.step()\n",
        "        print(f\"[FULL] Ep {ep:02d}/{EPOCHS} | loss {tr_loss:.4f} acc {tr_acc*100:5.2f}% f1M {tr_f1:.4f} | {time.time()-t0:.1f}s\")\n",
        "\n",
        "        if tr_loss < best_loss:\n",
        "            best_loss, wait = tr_loss, 0; torch.save(model.state_dict(), best_full_path)\n",
        "        else:\n",
        "            wait += 1\n",
        "            if wait >= PATIENCE:\n",
        "                print(f\"[FULL] â†’ Early stop @ {ep}\")\n",
        "                break\n",
        "\n",
        "    # Evaluate best\n",
        "    model.load_state_dict(torch.load(best_full_path, map_location=device))\n",
        "    test_loss, test_acc, test_f1 = evaluate(model, test_loader, criterion)\n",
        "    print(f\"\\nTEST â†’ loss {test_loss:.4f} | acc {test_acc*100:5.2f}% | f1M {test_f1:.4f}\")\n",
        "\n",
        "    @torch.no_grad()\n",
        "    def per_class_report():\n",
        "        model.eval()\n",
        "        y_true, y_pred = [], []\n",
        "        for xb, yb in test_loader:\n",
        "            xb = xb.to(device, non_blocking=True)\n",
        "            with torch.autocast(device_type='cuda', enabled=(device.type=='cuda')):\n",
        "                logits = model(xb)\n",
        "            y_pred.extend(logits.argmax(1).cpu().tolist())\n",
        "            y_true.extend(yb.numpy().tolist())\n",
        "        print(\"\\nPer-class report:\\n\",\n",
        "              classification_report(y_true, y_pred, target_names=test_base.classes, digits=4, zero_division=0))\n",
        "    per_class_report()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ihO479BnuXan",
        "outputId": "3f8b611a-3286-494e-fdd3-4452519e9e71"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Device: cuda\n",
            "Albumentations version: 2.0.8\n",
            "[WARN] RandomResizedCrop not usable in this Albumentations build. Using Resize+RandomCrop fallback. Error: 1 validation error for InitSchema\n",
            "size\n",
            "  Field required [type=missing, input_value={'scale': (0.85, 1.0), 'r...: None, 'strict': False}, input_type=dict]\n",
            "    For further information visit https://errors.pydantic.dev/2.11/v/missing\n",
            "Train images: 891 | Classes (27): ['Actinophrys', 'Amoeba', 'Arcella', 'Aspidisca', 'Ceratium', 'Codosiga', 'Colpoda', 'Epistylis', 'Euglena', 'Euglypha', 'Gonyaulax', 'Gymnodinium', 'Hydra', 'Keratella_quadrala', 'Noctiluca', 'Paramecium', 'Phacus', 'Rod_bacteria', 'Rotifera', 'Siprostomum', 'Spherical_bacteria', 'Spiral_bacteria', 'Stentor', 'Stylonychia', 'Synchaeta', 'Vorticella', 'Yeast']\n",
            "\n",
            "===== K-Fold 1/5 ===== (train 712 | val 179)\n",
            "Downloading: \"https://download.pytorch.org/models/resnet50-11ad3fa6.pth\" to /root/.cache/torch/hub/checkpoints/resnet50-11ad3fa6.pth\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 97.8M/97.8M [00:00<00:00, 214MB/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[Fold 1] Ep 01/60 | Train loss 3.1114 acc 13.34% f1M 0.0256 | Val loss 2.8543 acc 18.99% f1M 0.0221 | 378.0s\n",
            "[Fold 1] Ep 02/60 | Train loss 2.7095 acc 23.31% f1M 0.0495 | Val loss 2.5854 acc 28.49% f1M 0.0972 | 8.5s\n",
            "[Fold 1] Ep 03/60 | Train loss 2.4163 acc 34.69% f1M 0.1621 | Val loss 2.3792 acc 39.66% f1M 0.2679 | 6.9s\n",
            "[Fold 1] Ep 04/60 | Train loss 2.2208 acc 42.70% f1M 0.2917 | Val loss 2.1971 acc 43.02% f1M 0.3114 | 8.6s\n",
            "[Fold 1] Ep 05/60 | Train loss 1.9934 acc 52.39% f1M 0.4328 | Val loss 2.0583 acc 54.75% f1M 0.4427 | 7.0s\n",
            "[Fold 1] Ep 06/60 | Train loss 1.8827 acc 57.72% f1M 0.5046 | Val loss 1.9509 acc 56.42% f1M 0.4866 | 8.2s\n",
            "[Fold 1] Ep 07/60 | Train loss 1.4228 acc 64.89% f1M 0.6071 | Val loss 1.3812 acc 63.13% f1M 0.6015 | 10.8s\n",
            "[Fold 1] Ep 08/60 | Train loss 0.6449 acc 82.02% f1M 0.8128 | Val loss 1.0989 acc 69.83% f1M 0.6772 | 6.8s\n",
            "[Fold 1] Ep 09/60 | Train loss 0.4167 acc 89.61% f1M 0.8894 | Val loss 0.9984 acc 75.98% f1M 0.7643 | 8.1s\n",
            "[Fold 1] Ep 10/60 | Train loss 0.2552 acc 92.84% f1M 0.9281 | Val loss 0.9769 acc 77.65% f1M 0.8121 | 6.7s\n",
            "[Fold 1] Ep 11/60 | Train loss 0.2241 acc 93.68% f1M 0.9332 | Val loss 1.0789 acc 76.54% f1M 0.7813 | 8.2s\n",
            "[Fold 1] Ep 12/60 | Train loss 0.2143 acc 94.66% f1M 0.9514 | Val loss 1.1089 acc 77.09% f1M 0.8073 | 6.7s\n",
            "[Fold 1] Ep 13/60 | Train loss 0.1464 acc 97.05% f1M 0.9727 | Val loss 1.0647 acc 76.54% f1M 0.8198 | 8.4s\n",
            "[Fold 1] Ep 14/60 | Train loss 0.1316 acc 96.91% f1M 0.9765 | Val loss 1.1128 acc 75.98% f1M 0.8028 | 6.8s\n",
            "[Fold 1] Ep 15/60 | Train loss 0.1266 acc 97.75% f1M 0.9813 | Val loss 1.1323 acc 75.42% f1M 0.8068 | 7.9s\n",
            "[Fold 1] Ep 16/60 | Train loss 0.1107 acc 98.03% f1M 0.9864 | Val loss 1.1949 acc 75.42% f1M 0.7904 | 7.1s\n",
            "[Fold 1] Ep 17/60 | Train loss 0.1470 acc 96.77% f1M 0.9736 | Val loss 1.2268 acc 73.18% f1M 0.7859 | 7.4s\n",
            "[Fold 1] Ep 18/60 | Train loss 0.1259 acc 96.49% f1M 0.9588 | Val loss 1.5111 acc 72.07% f1M 0.7348 | 10.1s\n",
            "[Fold 1] Ep 19/60 | Train loss 0.1051 acc 96.63% f1M 0.9653 | Val loss 1.2975 acc 70.39% f1M 0.7538 | 7.0s\n",
            "[Fold 1] Ep 20/60 | Train loss 0.3020 acc 95.65% f1M 0.9659 | Val loss 1.3557 acc 67.60% f1M 0.6818 | 8.3s\n",
            "[Fold 1] Ep 21/60 | Train loss 0.1091 acc 96.77% f1M 0.9715 | Val loss 1.3246 acc 72.63% f1M 0.7251 | 6.8s\n",
            "[Fold 1] Ep 22/60 | Train loss 0.0904 acc 97.47% f1M 0.9784 | Val loss 1.5329 acc 68.16% f1M 0.6966 | 8.3s\n",
            "[Fold 1] Ep 23/60 | Train loss 0.1045 acc 97.19% f1M 0.9802 | Val loss 1.3436 acc 73.74% f1M 0.7858 | 7.3s\n",
            "[Fold 1] Ep 24/60 | Train loss 0.2215 acc 98.03% f1M 0.9867 | Val loss 1.2383 acc 72.07% f1M 0.7564 | 7.6s\n",
            "[Fold 1] Ep 25/60 | Train loss 0.1610 acc 95.65% f1M 0.9599 | Val loss 1.2991 acc 70.39% f1M 0.7686 | 8.0s\n",
            "[Fold 1] Ep 26/60 | Train loss 0.1753 acc 97.19% f1M 0.9688 | Val loss 1.3574 acc 72.07% f1M 0.7697 | 7.1s\n",
            "[Fold 1] Ep 27/60 | Train loss 0.1260 acc 96.35% f1M 0.9669 | Val loss 1.2682 acc 70.39% f1M 0.7342 | 8.2s\n",
            "[Fold 1] Ep 28/60 | Train loss 0.0775 acc 97.61% f1M 0.9813 | Val loss 1.3152 acc 71.51% f1M 0.7525 | 6.8s\n",
            "[Fold 1] Early stop @ 28 (best @ 13, F1M=0.8198)\n",
            "[Fold 1] Best Val â†’ acc 76.54% | f1M 0.8198 (epoch 13)\n",
            "\n",
            "===== K-Fold 2/5 ===== (train 713 | val 178)\n",
            "[Fold 2] Ep 01/60 | Train loss 3.0654 acc 14.03% f1M 0.0317 | Val loss 2.8449 acc 18.54% f1M 0.0318 | 9.5s\n",
            "[Fold 2] Ep 02/60 | Train loss 2.7202 acc 20.06% f1M 0.0372 | Val loss 2.5958 acc 26.40% f1M 0.0748 | 7.4s\n",
            "[Fold 2] Ep 03/60 | Train loss 2.4248 acc 32.68% f1M 0.1610 | Val loss 2.4015 acc 40.45% f1M 0.2755 | 6.9s\n",
            "[Fold 2] Ep 04/60 | Train loss 2.2374 acc 43.90% f1M 0.3363 | Val loss 2.2206 acc 42.13% f1M 0.2927 | 7.7s\n",
            "[Fold 2] Ep 05/60 | Train loss 2.0050 acc 52.59% f1M 0.4382 | Val loss 2.0875 acc 51.69% f1M 0.4306 | 6.4s\n",
            "[Fold 2] Ep 06/60 | Train loss 1.8439 acc 57.64% f1M 0.5149 | Val loss 1.9738 acc 54.49% f1M 0.4624 | 7.6s\n",
            "[Fold 2] Ep 07/60 | Train loss 1.3115 acc 66.34% f1M 0.6285 | Val loss 1.1405 acc 69.10% f1M 0.6389 | 7.8s\n",
            "[Fold 2] Ep 08/60 | Train loss 0.6295 acc 83.17% f1M 0.8248 | Val loss 0.9904 acc 74.16% f1M 0.6969 | 8.1s\n",
            "[Fold 2] Ep 09/60 | Train loss 0.4308 acc 88.92% f1M 0.8886 | Val loss 0.8771 acc 73.03% f1M 0.6813 | 6.5s\n",
            "[Fold 2] Ep 10/60 | Train loss 0.2746 acc 94.53% f1M 0.9434 | Val loss 0.8867 acc 80.34% f1M 0.7915 | 7.9s\n",
            "[Fold 2] Ep 11/60 | Train loss 0.1862 acc 95.37% f1M 0.9578 | Val loss 0.9847 acc 77.53% f1M 0.7381 | 6.6s\n",
            "[Fold 2] Ep 12/60 | Train loss 0.1356 acc 96.77% f1M 0.9744 | Val loss 0.9598 acc 76.97% f1M 0.7468 | 8.1s\n",
            "[Fold 2] Ep 13/60 | Train loss 0.1188 acc 96.49% f1M 0.9679 | Val loss 0.9956 acc 78.65% f1M 0.7800 | 6.6s\n",
            "[Fold 2] Ep 14/60 | Train loss 0.1198 acc 96.91% f1M 0.9673 | Val loss 0.9704 acc 78.65% f1M 0.7533 | 7.9s\n",
            "[Fold 2] Ep 15/60 | Train loss 0.1069 acc 97.34% f1M 0.9700 | Val loss 1.0224 acc 76.97% f1M 0.7416 | 7.2s\n",
            "[Fold 2] Ep 16/60 | Train loss 0.1241 acc 97.90% f1M 0.9805 | Val loss 0.9869 acc 79.21% f1M 0.7756 | 7.9s\n",
            "[Fold 2] Ep 17/60 | Train loss 0.1259 acc 96.07% f1M 0.9693 | Val loss 1.0648 acc 72.47% f1M 0.6717 | 7.6s\n",
            "[Fold 2] Ep 18/60 | Train loss 0.1338 acc 95.93% f1M 0.9638 | Val loss 1.2678 acc 74.72% f1M 0.7327 | 8.4s\n",
            "[Fold 2] Ep 19/60 | Train loss 0.1458 acc 95.51% f1M 0.9524 | Val loss 0.9903 acc 76.97% f1M 0.7782 | 8.4s\n",
            "[Fold 2] Ep 20/60 | Train loss 0.1324 acc 96.49% f1M 0.9714 | Val loss 1.1568 acc 76.40% f1M 0.7721 | 6.9s\n",
            "[Fold 2] Ep 21/60 | Train loss 0.0920 acc 97.19% f1M 0.9697 | Val loss 1.1515 acc 71.91% f1M 0.6997 | 8.1s\n",
            "[Fold 2] Ep 22/60 | Train loss 0.0976 acc 97.34% f1M 0.9699 | Val loss 1.2150 acc 74.16% f1M 0.7248 | 6.8s\n",
            "[Fold 2] Ep 23/60 | Train loss 0.0808 acc 98.60% f1M 0.9860 | Val loss 1.2115 acc 74.16% f1M 0.7152 | 8.1s\n",
            "[Fold 2] Ep 24/60 | Train loss 0.1087 acc 98.18% f1M 0.9782 | Val loss 1.0739 acc 75.84% f1M 0.7464 | 6.8s\n",
            "[Fold 2] Ep 25/60 | Train loss 0.0817 acc 98.32% f1M 0.9876 | Val loss 1.2234 acc 75.84% f1M 0.7533 | 8.2s\n",
            "[Fold 2] Early stop @ 25 (best @ 10, F1M=0.7915)\n",
            "[Fold 2] Best Val â†’ acc 80.34% | f1M 0.7915 (epoch 10)\n",
            "\n",
            "===== K-Fold 3/5 ===== (train 713 | val 178)\n",
            "[Fold 3] Ep 01/60 | Train loss 3.0642 acc 12.76% f1M 0.0275 | Val loss 2.8778 acc 17.42% f1M 0.0226 | 8.5s\n",
            "[Fold 3] Ep 02/60 | Train loss 2.7280 acc 18.79% f1M 0.0375 | Val loss 2.6396 acc 25.84% f1M 0.0622 | 6.8s\n",
            "[Fold 3] Ep 03/60 | Train loss 2.4745 acc 39.83% f1M 0.2563 | Val loss 2.4509 acc 41.01% f1M 0.3254 | 8.0s\n",
            "[Fold 3] Ep 04/60 | Train loss 2.1882 acc 47.69% f1M 0.3728 | Val loss 2.2799 acc 41.01% f1M 0.3102 | 6.3s\n",
            "[Fold 3] Ep 05/60 | Train loss 1.9941 acc 47.69% f1M 0.3890 | Val loss 2.1377 acc 43.26% f1M 0.3437 | 7.8s\n",
            "[Fold 3] Ep 06/60 | Train loss 1.8293 acc 59.75% f1M 0.5404 | Val loss 2.0381 acc 52.25% f1M 0.4218 | 6.6s\n",
            "[Fold 3] Ep 07/60 | Train loss 1.2502 acc 67.32% f1M 0.6500 | Val loss 1.3512 acc 58.43% f1M 0.5757 | 8.0s\n",
            "[Fold 3] Ep 08/60 | Train loss 0.5918 acc 84.85% f1M 0.8500 | Val loss 1.3355 acc 65.17% f1M 0.6269 | 6.5s\n",
            "[Fold 3] Ep 09/60 | Train loss 0.3718 acc 89.76% f1M 0.9042 | Val loss 1.1673 acc 66.85% f1M 0.6592 | 8.0s\n",
            "[Fold 3] Ep 10/60 | Train loss 0.2425 acc 93.97% f1M 0.9407 | Val loss 1.2205 acc 68.54% f1M 0.6694 | 7.0s\n",
            "[Fold 3] Ep 11/60 | Train loss 0.1759 acc 95.93% f1M 0.9600 | Val loss 1.2771 acc 68.54% f1M 0.6862 | 7.6s\n",
            "[Fold 3] Ep 12/60 | Train loss 0.1489 acc 95.79% f1M 0.9628 | Val loss 1.1566 acc 73.03% f1M 0.7233 | 7.0s\n",
            "[Fold 3] Ep 13/60 | Train loss 0.1196 acc 97.05% f1M 0.9725 | Val loss 1.2627 acc 69.10% f1M 0.6880 | 7.4s\n",
            "[Fold 3] Ep 14/60 | Train loss 0.1017 acc 97.90% f1M 0.9776 | Val loss 1.3182 acc 69.66% f1M 0.6964 | 7.2s\n",
            "[Fold 3] Ep 15/60 | Train loss 0.1312 acc 97.05% f1M 0.9729 | Val loss 1.3198 acc 70.79% f1M 0.7113 | 7.0s\n",
            "[Fold 3] Ep 16/60 | Train loss 0.0916 acc 98.46% f1M 0.9830 | Val loss 1.3053 acc 69.10% f1M 0.6888 | 7.5s\n",
            "[Fold 3] Ep 17/60 | Train loss 0.0693 acc 97.62% f1M 0.9762 | Val loss 1.4120 acc 71.35% f1M 0.7169 | 6.7s\n",
            "[Fold 3] Ep 18/60 | Train loss 0.1526 acc 96.35% f1M 0.9585 | Val loss 1.5548 acc 64.61% f1M 0.6441 | 7.9s\n",
            "[Fold 3] Ep 19/60 | Train loss 0.1271 acc 96.77% f1M 0.9668 | Val loss 1.5823 acc 68.54% f1M 0.6834 | 7.0s\n",
            "[Fold 3] Ep 20/60 | Train loss 0.1256 acc 96.77% f1M 0.9677 | Val loss 1.4418 acc 73.03% f1M 0.7148 | 8.0s\n",
            "[Fold 3] Ep 21/60 | Train loss 0.1183 acc 96.21% f1M 0.9586 | Val loss 1.2885 acc 72.47% f1M 0.6863 | 6.8s\n",
            "[Fold 3] Ep 22/60 | Train loss 0.0797 acc 97.62% f1M 0.9753 | Val loss 1.2553 acc 73.03% f1M 0.6855 | 8.0s\n",
            "[Fold 3] Ep 23/60 | Train loss 0.0732 acc 97.90% f1M 0.9852 | Val loss 1.2369 acc 73.03% f1M 0.7160 | 6.7s\n",
            "[Fold 3] Ep 24/60 | Train loss 0.0569 acc 98.60% f1M 0.9887 | Val loss 1.4343 acc 68.54% f1M 0.6875 | 8.1s\n",
            "[Fold 3] Ep 25/60 | Train loss 0.0556 acc 98.18% f1M 0.9836 | Val loss 1.3546 acc 71.35% f1M 0.7022 | 6.9s\n",
            "[Fold 3] Ep 26/60 | Train loss 0.0734 acc 98.88% f1M 0.9904 | Val loss 1.6159 acc 69.66% f1M 0.6753 | 8.4s\n",
            "[Fold 3] Ep 27/60 | Train loss 0.1057 acc 98.32% f1M 0.9779 | Val loss 1.7555 acc 66.85% f1M 0.6384 | 7.3s\n",
            "[Fold 3] Early stop @ 27 (best @ 12, F1M=0.7233)\n",
            "[Fold 3] Best Val â†’ acc 73.03% | f1M 0.7233 (epoch 12)\n",
            "\n",
            "===== K-Fold 4/5 ===== (train 713 | val 178)\n",
            "[Fold 4] Ep 01/60 | Train loss 3.0600 acc 15.29% f1M 0.0265 | Val loss 2.8239 acc 17.98% f1M 0.0230 | 7.2s\n",
            "[Fold 4] Ep 02/60 | Train loss 2.7244 acc 21.04% f1M 0.0391 | Val loss 2.5623 acc 23.60% f1M 0.0866 | 8.0s\n",
            "[Fold 4] Ep 03/60 | Train loss 2.4080 acc 34.22% f1M 0.1736 | Val loss 2.3478 acc 40.45% f1M 0.2954 | 6.6s\n",
            "[Fold 4] Ep 04/60 | Train loss 2.1733 acc 50.35% f1M 0.3949 | Val loss 2.1786 acc 43.26% f1M 0.3435 | 8.1s\n",
            "[Fold 4] Ep 05/60 | Train loss 1.9897 acc 50.35% f1M 0.3981 | Val loss 2.0263 acc 44.94% f1M 0.3792 | 6.6s\n",
            "[Fold 4] Ep 06/60 | Train loss 1.8550 acc 55.96% f1M 0.4702 | Val loss 1.9268 acc 57.30% f1M 0.5080 | 8.0s\n",
            "[Fold 4] Ep 07/60 | Train loss 1.3173 acc 65.92% f1M 0.5948 | Val loss 1.3082 acc 64.61% f1M 0.6056 | 6.8s\n",
            "[Fold 4] Ep 08/60 | Train loss 0.6811 acc 80.08% f1M 0.7829 | Val loss 0.9366 acc 69.66% f1M 0.7128 | 8.4s\n",
            "[Fold 4] Ep 09/60 | Train loss 0.4537 acc 89.06% f1M 0.8988 | Val loss 0.8878 acc 75.84% f1M 0.7603 | 7.3s\n",
            "[Fold 4] Ep 10/60 | Train loss 0.3259 acc 92.57% f1M 0.9311 | Val loss 0.8133 acc 79.21% f1M 0.7955 | 7.3s\n",
            "[Fold 4] Ep 11/60 | Train loss 0.2111 acc 94.25% f1M 0.9392 | Val loss 0.8750 acc 72.47% f1M 0.7434 | 7.6s\n",
            "[Fold 4] Ep 12/60 | Train loss 0.2018 acc 95.23% f1M 0.9540 | Val loss 0.8138 acc 76.97% f1M 0.7754 | 7.2s\n",
            "[Fold 4] Ep 13/60 | Train loss 0.1585 acc 95.23% f1M 0.9559 | Val loss 0.8780 acc 76.97% f1M 0.7819 | 7.9s\n",
            "[Fold 4] Ep 14/60 | Train loss 0.1281 acc 97.76% f1M 0.9793 | Val loss 0.8932 acc 76.40% f1M 0.7786 | 6.5s\n",
            "[Fold 4] Ep 15/60 | Train loss 0.0737 acc 98.60% f1M 0.9875 | Val loss 0.8226 acc 79.21% f1M 0.7940 | 7.8s\n",
            "[Fold 4] Ep 16/60 | Train loss 0.0903 acc 98.46% f1M 0.9855 | Val loss 0.8136 acc 78.65% f1M 0.8081 | 7.8s\n",
            "[Fold 4] Ep 17/60 | Train loss 0.0967 acc 98.18% f1M 0.9817 | Val loss 0.9444 acc 76.40% f1M 0.7663 | 10.2s\n",
            "[Fold 4] Ep 18/60 | Train loss 0.1182 acc 96.77% f1M 0.9653 | Val loss 1.4405 acc 67.42% f1M 0.6808 | 7.2s\n",
            "[Fold 4] Ep 19/60 | Train loss 0.1968 acc 93.97% f1M 0.9393 | Val loss 1.3812 acc 68.54% f1M 0.7224 | 7.7s\n",
            "[Fold 4] Ep 20/60 | Train loss 0.1340 acc 96.49% f1M 0.9728 | Val loss 1.1361 acc 75.28% f1M 0.7806 | 7.9s\n",
            "[Fold 4] Ep 21/60 | Train loss 0.1054 acc 97.62% f1M 0.9784 | Val loss 1.0804 acc 76.97% f1M 0.7867 | 7.0s\n",
            "[Fold 4] Ep 22/60 | Train loss 0.1287 acc 96.21% f1M 0.9616 | Val loss 1.1212 acc 73.03% f1M 0.7422 | 8.0s\n",
            "[Fold 4] Ep 23/60 | Train loss 0.1234 acc 96.91% f1M 0.9699 | Val loss 1.1051 acc 77.53% f1M 0.7816 | 6.7s\n",
            "[Fold 4] Ep 24/60 | Train loss 0.0915 acc 97.76% f1M 0.9768 | Val loss 1.0566 acc 78.09% f1M 0.7973 | 8.2s\n",
            "[Fold 4] Ep 25/60 | Train loss 0.0880 acc 98.32% f1M 0.9879 | Val loss 1.0203 acc 77.53% f1M 0.7892 | 6.7s\n",
            "[Fold 4] Ep 26/60 | Train loss 0.2317 acc 95.51% f1M 0.9660 | Val loss 1.1037 acc 71.91% f1M 0.7449 | 8.5s\n",
            "[Fold 4] Ep 27/60 | Train loss 0.1240 acc 96.77% f1M 0.9674 | Val loss 1.1218 acc 74.72% f1M 0.7630 | 9.3s\n",
            "[Fold 4] Ep 28/60 | Train loss 0.1131 acc 97.19% f1M 0.9756 | Val loss 1.1934 acc 72.47% f1M 0.7293 | 7.4s\n",
            "[Fold 4] Ep 29/60 | Train loss 0.0628 acc 97.90% f1M 0.9802 | Val loss 1.1845 acc 74.72% f1M 0.7667 | 8.1s\n",
            "[Fold 4] Ep 30/60 | Train loss 0.0636 acc 98.32% f1M 0.9886 | Val loss 1.2247 acc 76.40% f1M 0.7726 | 6.8s\n",
            "[Fold 4] Ep 31/60 | Train loss 0.0892 acc 97.76% f1M 0.9796 | Val loss 0.9209 acc 78.09% f1M 0.7874 | 8.3s\n",
            "[Fold 4] Early stop @ 31 (best @ 16, F1M=0.8081)\n",
            "[Fold 4] Best Val â†’ acc 78.65% | f1M 0.8081 (epoch 16)\n",
            "\n",
            "===== K-Fold 5/5 ===== (train 713 | val 178)\n",
            "[Fold 5] Ep 01/60 | Train loss 3.0745 acc 13.60% f1M 0.0274 | Val loss 2.8821 acc 17.42% f1M 0.0277 | 7.5s\n",
            "[Fold 5] Ep 02/60 | Train loss 2.6963 acc 21.46% f1M 0.0333 | Val loss 2.6442 acc 23.60% f1M 0.0922 | 7.5s\n",
            "[Fold 5] Ep 03/60 | Train loss 2.4001 acc 32.82% f1M 0.1830 | Val loss 2.4281 acc 36.52% f1M 0.2030 | 6.9s\n",
            "[Fold 5] Ep 04/60 | Train loss 2.1898 acc 45.30% f1M 0.3436 | Val loss 2.2605 acc 45.51% f1M 0.3810 | 7.4s\n",
            "[Fold 5] Ep 05/60 | Train loss 1.9849 acc 56.94% f1M 0.4881 | Val loss 2.1235 acc 50.00% f1M 0.4201 | 7.3s\n",
            "[Fold 5] Ep 06/60 | Train loss 1.8266 acc 56.94% f1M 0.4962 | Val loss 2.0338 acc 49.44% f1M 0.4128 | 6.9s\n",
            "[Fold 5] Ep 07/60 | Train loss 1.2554 acc 67.04% f1M 0.6461 | Val loss 1.3413 acc 62.36% f1M 0.6313 | 7.9s\n",
            "[Fold 5] Ep 08/60 | Train loss 0.6381 acc 81.63% f1M 0.8151 | Val loss 1.1183 acc 69.66% f1M 0.6990 | 6.7s\n",
            "[Fold 5] Ep 09/60 | Train loss 0.3668 acc 90.60% f1M 0.9059 | Val loss 1.0688 acc 73.03% f1M 0.7469 | 8.1s\n",
            "[Fold 5] Ep 10/60 | Train loss 0.2646 acc 92.71% f1M 0.9328 | Val loss 1.0262 acc 75.28% f1M 0.7756 | 7.3s\n",
            "[Fold 5] Ep 11/60 | Train loss 0.1727 acc 96.63% f1M 0.9673 | Val loss 0.9663 acc 75.84% f1M 0.7877 | 8.1s\n",
            "[Fold 5] Ep 12/60 | Train loss 0.1425 acc 96.91% f1M 0.9680 | Val loss 1.2721 acc 74.72% f1M 0.7530 | 6.7s\n",
            "[Fold 5] Ep 13/60 | Train loss 0.1474 acc 96.63% f1M 0.9678 | Val loss 1.1963 acc 74.72% f1M 0.7357 | 8.4s\n",
            "[Fold 5] Ep 14/60 | Train loss 0.1376 acc 96.21% f1M 0.9635 | Val loss 1.1396 acc 76.97% f1M 0.7584 | 6.7s\n",
            "[Fold 5] Ep 15/60 | Train loss 0.1129 acc 97.05% f1M 0.9712 | Val loss 1.1458 acc 73.03% f1M 0.7030 | 8.0s\n",
            "[Fold 5] Ep 16/60 | Train loss 0.1153 acc 97.48% f1M 0.9722 | Val loss 1.0882 acc 75.84% f1M 0.7513 | 7.2s\n",
            "[Fold 5] Ep 17/60 | Train loss 0.0962 acc 97.76% f1M 0.9784 | Val loss 1.1613 acc 75.84% f1M 0.7758 | 7.5s\n",
            "[Fold 5] Ep 18/60 | Train loss 0.1574 acc 95.79% f1M 0.9661 | Val loss 1.4543 acc 75.28% f1M 0.7430 | 7.7s\n",
            "[Fold 5] Ep 19/60 | Train loss 0.1148 acc 96.63% f1M 0.9653 | Val loss 1.2081 acc 76.97% f1M 0.7702 | 7.2s\n",
            "[Fold 5] Ep 20/60 | Train loss 0.1281 acc 96.77% f1M 0.9727 | Val loss 1.0869 acc 78.09% f1M 0.7932 | 8.5s\n",
            "[Fold 5] Ep 21/60 | Train loss 0.1217 acc 97.34% f1M 0.9760 | Val loss 1.2760 acc 76.97% f1M 0.7928 | 7.2s\n",
            "[Fold 5] Ep 22/60 | Train loss 0.1204 acc 96.35% f1M 0.9711 | Val loss 1.1159 acc 74.16% f1M 0.7306 | 8.6s\n",
            "[Fold 5] Ep 23/60 | Train loss 0.1181 acc 96.49% f1M 0.9716 | Val loss 1.2265 acc 76.97% f1M 0.8063 | 7.0s\n",
            "[Fold 5] Ep 24/60 | Train loss 0.1802 acc 96.35% f1M 0.9719 | Val loss 1.2473 acc 74.16% f1M 0.7269 | 9.2s\n",
            "[Fold 5] Ep 25/60 | Train loss 0.1168 acc 96.49% f1M 0.9724 | Val loss 1.6811 acc 71.35% f1M 0.7458 | 7.4s\n",
            "[Fold 5] Ep 26/60 | Train loss 0.1370 acc 96.63% f1M 0.9716 | Val loss 1.5764 acc 74.16% f1M 0.7686 | 7.7s\n",
            "[Fold 5] Ep 27/60 | Train loss 0.1382 acc 97.34% f1M 0.9763 | Val loss 1.4090 acc 75.84% f1M 0.7992 | 8.0s\n",
            "[Fold 5] Ep 28/60 | Train loss 0.1341 acc 96.91% f1M 0.9752 | Val loss 1.2225 acc 74.16% f1M 0.7883 | 6.8s\n",
            "[Fold 5] Ep 29/60 | Train loss 0.1099 acc 97.19% f1M 0.9758 | Val loss 1.2495 acc 76.97% f1M 0.8017 | 8.0s\n",
            "[Fold 5] Ep 30/60 | Train loss 0.0653 acc 97.90% f1M 0.9817 | Val loss 1.2851 acc 75.84% f1M 0.7639 | 6.8s\n",
            "[Fold 5] Ep 31/60 | Train loss 0.0598 acc 97.90% f1M 0.9801 | Val loss 1.3452 acc 74.16% f1M 0.7577 | 8.3s\n",
            "[Fold 5] Ep 32/60 | Train loss 0.0974 acc 98.04% f1M 0.9821 | Val loss 1.4117 acc 74.72% f1M 0.7746 | 6.9s\n",
            "[Fold 5] Ep 33/60 | Train loss 0.0815 acc 97.76% f1M 0.9817 | Val loss 1.5521 acc 74.16% f1M 0.7749 | 8.2s\n",
            "[Fold 5] Ep 34/60 | Train loss 0.1279 acc 97.34% f1M 0.9694 | Val loss 1.4315 acc 77.53% f1M 0.7968 | 7.0s\n",
            "[Fold 5] Ep 35/60 | Train loss 0.1454 acc 97.34% f1M 0.9788 | Val loss 1.2284 acc 74.16% f1M 0.7825 | 8.3s\n",
            "[Fold 5] Ep 36/60 | Train loss 0.1007 acc 97.90% f1M 0.9831 | Val loss 1.2107 acc 75.84% f1M 0.7949 | 7.2s\n",
            "[Fold 5] Ep 37/60 | Train loss 0.0620 acc 97.62% f1M 0.9823 | Val loss 1.2510 acc 78.09% f1M 0.8067 | 7.8s\n",
            "[Fold 5] Ep 38/60 | Train loss 0.0697 acc 98.60% f1M 0.9914 | Val loss 1.1171 acc 75.28% f1M 0.7916 | 7.8s\n",
            "[Fold 5] Ep 39/60 | Train loss 0.1287 acc 98.32% f1M 0.9876 | Val loss 1.4299 acc 71.91% f1M 0.7437 | 6.9s\n",
            "[Fold 5] Ep 40/60 | Train loss 0.0952 acc 97.62% f1M 0.9810 | Val loss 1.4319 acc 77.53% f1M 0.8231 | 8.2s\n",
            "[Fold 5] Ep 41/60 | Train loss 0.0814 acc 98.18% f1M 0.9856 | Val loss 1.5571 acc 73.60% f1M 0.7493 | 6.7s\n",
            "[Fold 5] Ep 42/60 | Train loss 0.0461 acc 98.32% f1M 0.9814 | Val loss 1.2740 acc 79.21% f1M 0.8113 | 8.2s\n",
            "[Fold 5] Ep 43/60 | Train loss 0.0428 acc 98.74% f1M 0.9914 | Val loss 1.2142 acc 76.97% f1M 0.7977 | 6.8s\n",
            "[Fold 5] Ep 44/60 | Train loss 0.0756 acc 97.48% f1M 0.9755 | Val loss 1.4661 acc 72.47% f1M 0.7151 | 8.4s\n",
            "[Fold 5] Ep 45/60 | Train loss 0.0731 acc 97.90% f1M 0.9807 | Val loss 1.2826 acc 78.65% f1M 0.8052 | 6.8s\n",
            "[Fold 5] Ep 46/60 | Train loss 0.0866 acc 98.74% f1M 0.9923 | Val loss 1.1564 acc 76.97% f1M 0.7868 | 8.4s\n",
            "[Fold 5] Ep 47/60 | Train loss 0.0784 acc 98.32% f1M 0.9871 | Val loss 1.1758 acc 75.28% f1M 0.8095 | 7.4s\n",
            "[Fold 5] Ep 48/60 | Train loss 0.0731 acc 98.04% f1M 0.9827 | Val loss 1.0795 acc 75.84% f1M 0.7920 | 7.7s\n",
            "[Fold 5] Ep 49/60 | Train loss 0.0952 acc 97.90% f1M 0.9792 | Val loss 1.2813 acc 75.28% f1M 0.8037 | 8.0s\n",
            "[Fold 5] Ep 50/60 | Train loss 0.0992 acc 97.48% f1M 0.9804 | Val loss 1.3604 acc 75.28% f1M 0.8023 | 6.8s\n",
            "[Fold 5] Ep 51/60 | Train loss 0.0735 acc 98.74% f1M 0.9880 | Val loss 1.4451 acc 72.47% f1M 0.7754 | 8.1s\n",
            "[Fold 5] Ep 52/60 | Train loss 0.0396 acc 99.02% f1M 0.9920 | Val loss 1.3511 acc 75.84% f1M 0.7742 | 6.8s\n",
            "[Fold 5] Ep 53/60 | Train loss 0.0519 acc 98.18% f1M 0.9821 | Val loss 1.5235 acc 69.66% f1M 0.7035 | 8.2s\n",
            "[Fold 5] Ep 54/60 | Train loss 0.0519 acc 98.74% f1M 0.9878 | Val loss 1.3339 acc 75.28% f1M 0.7905 | 6.8s\n",
            "[Fold 5] Ep 55/60 | Train loss 0.0306 acc 99.02% f1M 0.9912 | Val loss 1.3324 acc 76.97% f1M 0.8099 | 8.3s\n",
            "[Fold 5] Early stop @ 55 (best @ 40, F1M=0.8231)\n",
            "[Fold 5] Best Val â†’ acc 77.53% | f1M 0.8231 (epoch 40)\n",
            "\n",
            "===== CV Summary (best epoch per fold) =====\n",
            "Val Accs: ['76.54%', '80.34%', '73.03%', '78.65%', '77.53%'] | MeanÂ±Std = 77.22% Â± 2.44%\n",
            "Val F1M : ['0.8198', '0.7915', '0.7233', '0.8081', '0.8231'] | MeanÂ±Std = 0.7931 Â± 0.0366\n",
            "[FULL] Ep 01/60 | loss 3.0574 acc 12.12% f1M 0.0225 | 9.6s\n",
            "[FULL] Ep 02/60 | loss 2.6091 acc 26.15% f1M 0.0844 | 7.6s\n",
            "[FULL] Ep 03/60 | loss 2.3165 acc 42.20% f1M 0.2912 | 7.9s\n",
            "[FULL] Ep 04/60 | loss 2.0508 acc 49.83% f1M 0.4022 | 8.6s\n",
            "[FULL] Ep 05/60 | loss 1.8480 acc 56.23% f1M 0.4885 | 7.2s\n",
            "[FULL] Ep 06/60 | loss 1.6804 acc 62.40% f1M 0.5848 | 8.6s\n",
            "[FULL] Ep 07/60 | loss 1.1515 acc 69.47% f1M 0.6841 | 9.2s\n",
            "[FULL] Ep 08/60 | loss 0.5578 acc 84.29% f1M 0.8438 | 7.3s\n",
            "[FULL] Ep 09/60 | loss 0.3609 acc 91.13% f1M 0.9139 | 8.7s\n",
            "[FULL] Ep 10/60 | loss 0.2066 acc 94.84% f1M 0.9530 | 7.3s\n",
            "[FULL] Ep 11/60 | loss 0.1646 acc 96.07% f1M 0.9621 | 8.8s\n",
            "[FULL] Ep 12/60 | loss 0.1049 acc 97.98% f1M 0.9783 | 7.9s\n",
            "[FULL] Ep 13/60 | loss 0.1116 acc 96.63% f1M 0.9762 | 7.4s\n",
            "[FULL] Ep 14/60 | loss 0.0867 acc 97.64% f1M 0.9829 | 8.4s\n",
            "[FULL] Ep 15/60 | loss 0.0735 acc 97.98% f1M 0.9825 | 7.1s\n",
            "[FULL] Ep 16/60 | loss 0.0643 acc 98.09% f1M 0.9842 | 8.5s\n",
            "[FULL] Ep 17/60 | loss 0.0746 acc 97.87% f1M 0.9799 | 7.4s\n",
            "[FULL] Ep 18/60 | loss 0.0895 acc 97.42% f1M 0.9687 | 9.7s\n",
            "[FULL] Ep 19/60 | loss 0.1610 acc 94.95% f1M 0.9534 | 8.5s\n",
            "[FULL] Ep 20/60 | loss 0.1051 acc 96.75% f1M 0.9768 | 7.0s\n",
            "[FULL] Ep 21/60 | loss 0.0924 acc 96.63% f1M 0.9771 | 8.6s\n",
            "[FULL] Ep 22/60 | loss 0.0637 acc 97.76% f1M 0.9817 | 7.2s\n",
            "[FULL] Ep 23/60 | loss 0.0509 acc 98.43% f1M 0.9882 | 8.6s\n",
            "[FULL] Ep 24/60 | loss 0.0425 acc 98.54% f1M 0.9878 | 8.1s\n",
            "[FULL] Ep 25/60 | loss 0.0668 acc 98.09% f1M 0.9833 | 7.7s\n",
            "[FULL] Ep 26/60 | loss 0.0516 acc 98.20% f1M 0.9837 | 8.5s\n",
            "[FULL] Ep 27/60 | loss 0.0599 acc 98.09% f1M 0.9852 | 7.3s\n",
            "[FULL] Ep 28/60 | loss 0.0572 acc 97.87% f1M 0.9810 | 8.6s\n",
            "[FULL] Ep 29/60 | loss 0.0594 acc 98.77% f1M 0.9896 | 7.2s\n",
            "[FULL] Ep 30/60 | loss 0.0667 acc 97.98% f1M 0.9838 | 8.3s\n",
            "[FULL] Ep 31/60 | loss 0.0569 acc 98.32% f1M 0.9877 | 8.2s\n",
            "[FULL] Ep 32/60 | loss 0.0518 acc 98.43% f1M 0.9874 | 7.3s\n",
            "[FULL] Ep 33/60 | loss 0.0450 acc 98.09% f1M 0.9836 | 8.8s\n",
            "[FULL] Ep 34/60 | loss 0.0467 acc 98.54% f1M 0.9875 | 7.2s\n",
            "[FULL] Ep 35/60 | loss 0.0524 acc 98.65% f1M 0.9897 | 8.7s\n",
            "[FULL] Ep 36/60 | loss 0.0582 acc 98.09% f1M 0.9857 | 7.5s\n",
            "[FULL] Ep 37/60 | loss 0.0677 acc 98.09% f1M 0.9815 | 8.2s\n",
            "[FULL] Ep 38/60 | loss 0.0497 acc 98.77% f1M 0.9905 | 8.5s\n",
            "[FULL] Ep 39/60 | loss 0.0434 acc 98.65% f1M 0.9848 | 7.3s\n",
            "[FULL] â†’ Early stop @ 39\n",
            "\n",
            "TEST â†’ loss 1.1250 | acc 78.18% | f1M 0.8030\n",
            "\n",
            "Per-class report:\n",
            "                     precision    recall  f1-score   support\n",
            "\n",
            "       Actinophrys     1.0000    1.0000    1.0000         5\n",
            "            Amoeba     0.6667    0.4000    0.5000        10\n",
            "           Arcella     1.0000    0.5000    0.6667         4\n",
            "         Aspidisca     1.0000    0.7500    0.8571         4\n",
            "          Ceratium     0.8571    1.0000    0.9231         6\n",
            "          Codosiga     1.0000    0.7500    0.8571         4\n",
            "           Colpoda     0.6000    0.6000    0.6000         5\n",
            "         Epistylis     0.7143    1.0000    0.8333         5\n",
            "           Euglena     0.8000    0.7692    0.7843        26\n",
            "          Euglypha     0.5000    0.5000    0.5000         2\n",
            "         Gonyaulax     1.0000    0.8000    0.8889         5\n",
            "       Gymnodinium     0.7500    0.7500    0.7500         4\n",
            "             Hydra     0.8750    0.7778    0.8235         9\n",
            "Keratella_quadrala     1.0000    1.0000    1.0000         7\n",
            "         Noctiluca     1.0000    1.0000    1.0000         7\n",
            "        Paramecium     0.7188    0.7931    0.7541        29\n",
            "            Phacus     0.7500    1.0000    0.8571         3\n",
            "      Rod_bacteria     0.6000    0.6429    0.6207        14\n",
            "          Rotifera     1.0000    1.0000    1.0000         6\n",
            "       Siprostomum     0.8571    1.0000    0.9231         6\n",
            "Spherical_bacteria     0.6316    0.8571    0.7273        14\n",
            "   Spiral_bacteria     0.5714    0.3333    0.4211        12\n",
            "           Stentor     1.0000    1.0000    1.0000         7\n",
            "       Stylonychia     0.5714    1.0000    0.7273         4\n",
            "         Synchaeta     1.0000    1.0000    1.0000         8\n",
            "        Vorticella     1.0000    1.0000    1.0000         3\n",
            "             Yeast     0.7000    0.6364    0.6667        11\n",
            "\n",
            "          accuracy                         0.7818       220\n",
            "         macro avg     0.8209    0.8096    0.8030       220\n",
            "      weighted avg     0.7881    0.7818    0.7756       220\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# ===================== ConvNeXt-Tiny + K-Fold + Albumentations \"Smart Aug\" (no MixUp/CutMix) =====================\n",
        "import os, time, random, numpy as np, torch\n",
        "from torch import nn, optim\n",
        "from torch.utils.data import DataLoader, Dataset\n",
        "from torchvision import datasets, models\n",
        "from torchvision.models import ConvNeXt_Tiny_Weights\n",
        "from sklearn.metrics import accuracy_score, f1_score, classification_report\n",
        "from sklearn.model_selection import StratifiedKFold\n",
        "from PIL import Image\n",
        "from torchvision.transforms.functional import to_tensor\n",
        "\n",
        "# Albumentations\n",
        "import albumentations as A\n",
        "from albumentations.pytorch import ToTensorV2\n",
        "import cv2  # for border_mode constants\n",
        "\n",
        "# ---------- Paths ----------\n",
        "train_dir = \"/content/drive/MyDrive/Bacteria Paper/datasets/Processed/train_320\"\n",
        "test_dir  = \"/content/drive/MyDrive/Bacteria Paper/datasets/Processed/test_320\"\n",
        "\n",
        "# ---------- Your dataset mean/std (0â€“1 scale; RGB) ----------\n",
        "DATA_MEAN = [0.54200659, 0.57473042, 0.55953813]   # <-- keep your computed values\n",
        "DATA_STD  = [0.2644171,  0.25311802, 0.26218295]\n",
        "\n",
        "# ---------- Config ----------\n",
        "SEED = 42\n",
        "IMG_SIZE = 320              # ConvNeXt works fine with 288â€“384; 320 is OK\n",
        "N_SPLITS = 5\n",
        "EPOCHS = 60\n",
        "PATIENCE = 20\n",
        "\n",
        "HEAD_WARMUP_EPOCHS   = 6     # train only classifier head initially\n",
        "UNFREEZE_LAYER4_EPOCH = 7    # unfreeze last stage (features[5])\n",
        "UNFREEZE_L34_EPOCH    = 18   # unfreeze last two stages (features[4] + features[5])\n",
        "\n",
        "LR_HEAD = 1e-3\n",
        "LR_BB   = 3e-4\n",
        "WEIGHT_DECAY = 5e-5\n",
        "BATCH_GPU, BATCH_CPU = 64, 16\n",
        "NUM_WORKERS = min(8, os.cpu_count() or 2)\n",
        "\n",
        "# Smart aug knobs\n",
        "ROT_LIMIT = 20\n",
        "HFLIP_P   = 0.5\n",
        "VFLIP_P   = 0.2\n",
        "CLAHE_P   = 0.2\n",
        "GBLUR_P   = 0.25\n",
        "MBLUR_P   = 0.15\n",
        "GAMMA_P   = 0.3\n",
        "HSV_P     = 0.3\n",
        "RRC_SCALE = (0.85, 1.0)\n",
        "RRC_RATIO = (0.9, 1.1)\n",
        "\n",
        "# ---------- Seed & Device ----------\n",
        "def set_seed(s):\n",
        "    random.seed(s); np.random.seed(s); torch.manual_seed(s); torch.cuda.manual_seed_all(s)\n",
        "set_seed(SEED)\n",
        "torch.backends.cudnn.benchmark = True\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "print(\"Device:\", device)\n",
        "\n",
        "# ---------- Albumentations pipelines (version-proof) ----------\n",
        "print(\"Albumentations version:\", A.__version__)  # for visibility\n",
        "\n",
        "_first_train = []\n",
        "try:\n",
        "    # Validate signature once; if it errors we will fallback\n",
        "    _ = A.RandomResizedCrop(height=IMG_SIZE, width=IMG_SIZE, scale=RRC_SCALE, ratio=RRC_RATIO, p=1.0)\n",
        "    _first_train.append(\n",
        "        A.RandomResizedCrop(height=IMG_SIZE, width=IMG_SIZE, scale=RRC_SCALE, ratio=RRC_RATIO, p=1.0)\n",
        "    )\n",
        "except Exception as e:\n",
        "    print(\"[WARN] RandomResizedCrop not usable in this Albumentations build. \"\n",
        "          \"Using Resize+RandomCrop fallback. Error:\", e)\n",
        "    _first_train += [\n",
        "        A.Resize(height=int(IMG_SIZE * 1.15), width=int(IMG_SIZE * 1.15)),\n",
        "        A.RandomCrop(height=IMG_SIZE, width=IMG_SIZE),\n",
        "    ]\n",
        "\n",
        "train_aug = A.Compose(\n",
        "    _first_train + [\n",
        "        A.HorizontalFlip(p=HFLIP_P),\n",
        "        A.VerticalFlip(p=VFLIP_P),\n",
        "        A.Rotate(limit=ROT_LIMIT, p=0.5, border_mode=cv2.BORDER_REFLECT_101),\n",
        "        A.RandomBrightnessContrast(p=0.5),\n",
        "        A.RandomGamma(p=GAMMA_P),\n",
        "        A.HueSaturationValue(p=HSV_P),\n",
        "        A.GaussianBlur(blur_limit=(3, 5), p=GBLUR_P),\n",
        "        A.MotionBlur(blur_limit=5, p=MBLUR_P),\n",
        "        A.CLAHE(p=CLAHE_P),\n",
        "        A.Normalize(mean=DATA_MEAN, std=DATA_STD),\n",
        "        ToTensorV2(),\n",
        "    ]\n",
        ")\n",
        "\n",
        "val_aug = A.Compose([\n",
        "    A.Resize(height=int(IMG_SIZE * 1.15), width=int(IMG_SIZE * 1.15)),\n",
        "    A.CenterCrop(height=IMG_SIZE, width=IMG_SIZE),\n",
        "    A.Normalize(mean=DATA_MEAN, std=DATA_STD),\n",
        "    ToTensorV2(),\n",
        "])\n",
        "\n",
        "# ---------- Dataset wrapper using Albumentations ----------\n",
        "class AlbDataset(Dataset):\n",
        "    def __init__(self, base, indices=None, transform=None):\n",
        "        self.base = base\n",
        "        self.samples = base.samples\n",
        "        self.indices = list(indices) if indices is not None else list(range(len(self.samples)))\n",
        "        self.targets = np.array([self.base.targets[i] for i in self.indices], dtype=np.int64)\n",
        "        self.transform = transform\n",
        "    def __len__(self): return len(self.indices)\n",
        "    def __getitem__(self, i):\n",
        "        idx = self.indices[i]\n",
        "        path, label = self.samples[idx]\n",
        "        img = np.array(Image.open(path).convert(\"RGB\"))\n",
        "        if self.transform:\n",
        "            img = self.transform(image=img)[\"image\"]\n",
        "        else:\n",
        "            img = to_tensor(Image.fromarray(img)).float()\n",
        "        return img, label\n",
        "\n",
        "# ---------- Data ----------\n",
        "base_full = datasets.ImageFolder(root=train_dir, transform=None)\n",
        "class_names = base_full.classes\n",
        "num_classes = len(class_names)\n",
        "all_targets = np.array(base_full.targets, dtype=np.int64)\n",
        "print(f\"Train images: {len(base_full)} | Classes ({num_classes}): {class_names}\")\n",
        "\n",
        "# ---------- Model / Optim (ConvNeXt-Tiny) ----------\n",
        "def build_convnext_tiny(num_classes):\n",
        "    m = models.convnext_tiny(weights=ConvNeXt_Tiny_Weights.DEFAULT)\n",
        "    # Replace final Linear head (classifier[2])\n",
        "    in_f = m.classifier[2].in_features\n",
        "    m.classifier[2] = nn.Linear(in_f, num_classes)\n",
        "    m = m.to(device)\n",
        "    if device.type == \"cuda\":\n",
        "        m = m.to(memory_format=torch.channels_last)\n",
        "    return m\n",
        "\n",
        "def _freeze_all(m):\n",
        "    for p in m.parameters():\n",
        "        p.requires_grad = False\n",
        "\n",
        "def _enable_classifier_head(m):\n",
        "    # Only the final Linear (classifier[2]) is trainable in head warm-up\n",
        "    for p in m.classifier[2].parameters():\n",
        "        p.requires_grad = True\n",
        "\n",
        "def _enable_last_stage(m):\n",
        "    # Unfreeze last stage: features[5] (stage4) + classifier head\n",
        "    for p in m.features[5].parameters():\n",
        "        p.requires_grad = True\n",
        "    for p in m.classifier[2].parameters():\n",
        "        p.requires_grad = True\n",
        "\n",
        "def _enable_last_two_stages(m):\n",
        "    # Unfreeze last two stages: features[4] (stage3) and features[5] (stage4) + head\n",
        "    for p in m.features[4].parameters():\n",
        "        p.requires_grad = True\n",
        "    for p in m.features[5].parameters():\n",
        "        p.requires_grad = True\n",
        "    for p in m.classifier[2].parameters():\n",
        "        p.requires_grad = True\n",
        "\n",
        "def set_trainable(m, mode):\n",
        "    # mode: \"head\" | \"layer4\" | \"layer3_4\"\n",
        "    _freeze_all(m)\n",
        "    if mode == \"head\":\n",
        "        _enable_classifier_head(m)\n",
        "    elif mode == \"layer4\":\n",
        "        _enable_last_stage(m)\n",
        "    elif mode == \"layer3_4\":\n",
        "        _enable_last_two_stages(m)\n",
        "\n",
        "def make_optimizer(model):\n",
        "    # Separate LR for head vs backbone (classifier[2] vs the rest trainable)\n",
        "    head_params, bb_params = [], []\n",
        "    for n, p in model.named_parameters():\n",
        "        if not p.requires_grad:\n",
        "            continue\n",
        "        # Anything in classifier.2 is head; everything else trainable is backbone\n",
        "        if n.startswith(\"classifier.2\"):\n",
        "            head_params.append(p)\n",
        "        else:\n",
        "            bb_params.append(p)\n",
        "    return optim.AdamW(\n",
        "        [{'params': head_params, 'lr': LR_HEAD},\n",
        "         {'params': bb_params,   'lr': LR_BB}],\n",
        "        weight_decay=WEIGHT_DECAY\n",
        "    )\n",
        "\n",
        "# ---------- Train/Eval ----------\n",
        "@torch.no_grad()\n",
        "def evaluate(model, loader, criterion):\n",
        "    model.eval()\n",
        "    tot, y_true, y_pred = 0.0, [], []\n",
        "    for xb,yb in loader:\n",
        "        xb = xb.to(device, non_blocking=True); yb = yb.to(device, non_blocking=True)\n",
        "        with torch.autocast(device_type='cuda', enabled=(device.type=='cuda')):\n",
        "            logits = model(xb)\n",
        "            loss = criterion(logits, yb)\n",
        "        tot += loss.item()\n",
        "        y_pred += logits.argmax(1).cpu().tolist()\n",
        "        y_true += yb.cpu().tolist()\n",
        "    loss = tot / max(1, len(loader))\n",
        "    acc  = accuracy_score(y_true, y_pred)\n",
        "    f1m  = f1_score(y_true, y_pred, average=\"macro\", zero_division=0)\n",
        "    return loss, acc, f1m\n",
        "\n",
        "def train_one_epoch(model, loader, optimizer, criterion):\n",
        "    model.train()\n",
        "    tot, y_true, y_pred = 0.0, [], []\n",
        "    for xb,yb in loader:\n",
        "        xb = xb.to(device, non_blocking=True); yb = yb.to(device, non_blocking=True)\n",
        "        optimizer.zero_grad(set_to_none=True)\n",
        "        with torch.autocast(device_type='cuda', enabled=(device.type=='cuda')):\n",
        "            logits = model(xb)\n",
        "            loss = criterion(logits, yb)\n",
        "        loss.backward(); optimizer.step()\n",
        "        tot += loss.item()\n",
        "        with torch.no_grad():\n",
        "            y_pred += logits.argmax(1).cpu().tolist()\n",
        "            y_true += yb.cpu().tolist()\n",
        "    loss = tot / max(1, len(loader))\n",
        "    acc  = accuracy_score(y_true, y_pred)\n",
        "    f1m  = f1_score(y_true, y_pred, average=\"macro\", zero_division=0)\n",
        "    return loss, acc, f1m\n",
        "\n",
        "def make_loader(ds, shuffle):\n",
        "    return DataLoader(\n",
        "        ds,\n",
        "        batch_size=(BATCH_GPU if device.type=='cuda' else BATCH_CPU),\n",
        "        shuffle=shuffle, num_workers=NUM_WORKERS,\n",
        "        pin_memory=(device.type=='cuda'),\n",
        "        persistent_workers=(NUM_WORKERS>0 and device.type=='cuda')\n",
        "    )\n",
        "\n",
        "def run_fold(fold, tr_idx, va_idx):\n",
        "    ds_tr = AlbDataset(base_full, tr_idx, transform=train_aug)\n",
        "    ds_va = AlbDataset(base_full, va_idx, transform=val_aug)\n",
        "    dl_tr = make_loader(ds_tr, shuffle=True)\n",
        "    dl_va = make_loader(ds_va, shuffle=False)\n",
        "\n",
        "    model = build_convnext_tiny(num_classes)\n",
        "    set_trainable(model, \"head\")\n",
        "    optimizer = make_optimizer(model)\n",
        "    scheduler = optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max=EPOCHS)\n",
        "    criterion = nn.CrossEntropyLoss()\n",
        "\n",
        "    best = {\"f1\": -1, \"ep\": -1, \"state\": None, \"val_acc\": 0.0}\n",
        "    wait = 0\n",
        "\n",
        "    for ep in range(1, EPOCHS+1):\n",
        "        # gradual unfreeze\n",
        "        if ep == UNFREEZE_LAYER4_EPOCH:\n",
        "            set_trainable(model, \"layer4\"); optimizer = make_optimizer(model)\n",
        "        if ep == UNFREEZE_L34_EPOCH:\n",
        "            set_trainable(model, \"layer3_4\"); optimizer = make_optimizer(model)\n",
        "\n",
        "        t0 = time.time()\n",
        "        tr_loss, tr_acc, tr_f1 = train_one_epoch(model, dl_tr, optimizer, criterion)\n",
        "        va_loss, va_acc, va_f1 = evaluate(model, dl_va, criterion)\n",
        "        scheduler.step()\n",
        "\n",
        "        print(f\"[Fold {fold}] Ep {ep:02d}/{EPOCHS} | \"\n",
        "              f\"Train loss {tr_loss:.4f} acc {tr_acc*100:5.2f}% f1M {tr_f1:.4f} | \"\n",
        "              f\"Val loss {va_loss:.4f} acc {va_acc*100:5.2f}% f1M {va_f1:.4f} | {time.time()-t0:.1f}s\")\n",
        "\n",
        "        if va_f1 > best[\"f1\"]:\n",
        "            best.update({\"f1\": va_f1, \"ep\": ep, \"state\": {k:v.cpu() for k,v in model.state_dict().items()}, \"val_acc\": va_acc})\n",
        "            wait = 0\n",
        "        else:\n",
        "            wait += 1\n",
        "            if wait >= PATIENCE:\n",
        "                print(f\"[Fold {fold}] Early stop @ {ep} (best @ {best['ep']}, F1M={best['f1']:.4f})\")\n",
        "                break\n",
        "\n",
        "    ckpt = f\"convnext_tiny_alb_smart_fold{fold}.pt\"\n",
        "    if best[\"state\"] is not None:\n",
        "        torch.save(best[\"state\"], ckpt)\n",
        "\n",
        "    # Final val from best\n",
        "    model.load_state_dict(torch.load(ckpt, map_location=device))\n",
        "    va_loss, va_acc, va_f1 = evaluate(model, dl_va, criterion)\n",
        "    print(f\"[Fold {fold}] Best Val â†’ acc {va_acc*100:5.2f}% | f1M {va_f1:.4f} (epoch {best['ep']})\")\n",
        "    return {\"fold\": fold, \"val_acc\": va_acc, \"val_f1\": va_f1, \"epoch\": best[\"ep\"], \"ckpt\": ckpt}\n",
        "\n",
        "# ---------- K-Fold ----------\n",
        "skf = StratifiedKFold(n_splits=N_SPLITS, shuffle=True, random_state=SEED)\n",
        "fold_summaries = []\n",
        "for fold, (tr_idx, va_idx) in enumerate(skf.split(np.arange(len(all_targets)), all_targets), start=1):\n",
        "    print(f\"\\n===== K-Fold {fold}/{N_SPLITS} ===== (train {len(tr_idx)} | val {len(va_idx)})\")\n",
        "    fs = run_fold(fold, tr_idx, va_idx)\n",
        "    fold_summaries.append(fs)\n",
        "\n",
        "cv_accs = [fs[\"val_acc\"] for fs in fold_summaries]\n",
        "cv_f1s  = [fs[\"val_f1\"] for fs in fold_summaries]\n",
        "print(\"\\n===== CV Summary (best epoch per fold) =====\")\n",
        "print(\"Val Accs:\", [f\"{a*100:0.2f}%\" for a in cv_accs], f\"| MeanÂ±Std = {np.mean(cv_accs)*100:0.2f}% Â± {np.std(cv_accs)*100:0.2f}%\")\n",
        "print(\"Val F1M :\", [f\"{f:0.4f}\" for f in cv_f1s],        f\"| MeanÂ±Std = {np.mean(cv_f1s):0.4f} Â± {np.std(cv_f1s):0.4f}\")\n",
        "\n",
        "# ---------- Full-train on train/, evaluate on test/ ----------\n",
        "if os.path.isdir(test_dir):\n",
        "    full_base  = datasets.ImageFolder(root=train_dir, transform=None)\n",
        "    test_base  = datasets.ImageFolder(root=test_dir, transform=None)\n",
        "    full_ds    = AlbDataset(full_base, transform=train_aug)\n",
        "    test_ds    = AlbDataset(test_base, transform=val_aug)\n",
        "\n",
        "    full_loader = DataLoader(full_ds, batch_size=(BATCH_GPU if device.type=='cuda' else BATCH_CPU),\n",
        "                             shuffle=True, num_workers=NUM_WORKERS, pin_memory=(device.type=='cuda'),\n",
        "                             persistent_workers=(NUM_WORKERS>0 and device.type=='cuda'))\n",
        "    test_loader = DataLoader(test_ds, batch_size=(BATCH_GPU if device.type=='cuda' else BATCH_CPU),\n",
        "                             shuffle=False, num_workers=NUM_WORKERS, pin_memory=(device.type=='cuda'),\n",
        "                             persistent_workers=(NUM_WORKERS>0 and device.type=='cuda'))\n",
        "\n",
        "    model = build_convnext_tiny(num_classes)\n",
        "    set_trainable(model, \"head\")\n",
        "    optimizer = make_optimizer(model)\n",
        "    scheduler = optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max=EPOCHS)\n",
        "    criterion = nn.CrossEntropyLoss()\n",
        "\n",
        "    best_loss, wait = 1e9, 0\n",
        "    best_full_path = \"convnext_tiny_alb_smart_fulltrain.pt\"\n",
        "    for ep in range(1, EPOCHS+1):\n",
        "        if ep == UNFREEZE_LAYER4_EPOCH:\n",
        "            set_trainable(model, \"layer4\"); optimizer = make_optimizer(model)\n",
        "        if ep == UNFREEZE_L34_EPOCH:\n",
        "            set_trainable(model, \"layer3_4\"); optimizer = make_optimizer(model)\n",
        "\n",
        "        t0 = time.time()\n",
        "        tr_loss, tr_acc, tr_f1 = train_one_epoch(model, full_loader, optimizer, criterion)\n",
        "        scheduler.step()\n",
        "        print(f\"[FULL] Ep {ep:02d}/{EPOCHS} | loss {tr_loss:.4f} acc {tr_acc*100:5.2f}% f1M {tr_f1:.4f} | {time.time()-t0:.1f}s\")\n",
        "\n",
        "        if tr_loss < best_loss:\n",
        "            best_loss, wait = tr_loss, 0; torch.save(model.state_dict(), best_full_path)\n",
        "        else:\n",
        "            wait += 1\n",
        "            if wait >= PATIENCE:\n",
        "                print(f\"[FULL] â†’ Early stop @ {ep}\")\n",
        "                break\n",
        "\n",
        "    # Evaluate best\n",
        "    model.load_state_dict(torch.load(best_full_path, map_location=device))\n",
        "    test_loss, test_acc, test_f1 = evaluate(model, test_loader, criterion)\n",
        "    print(f\"\\nTEST â†’ loss {test_loss:.4f} | acc {test_acc*100:5.2f}% | f1M {test_f1:.4f}\")\n",
        "\n",
        "    @torch.no_grad()\n",
        "    def per_class_report():\n",
        "        model.eval()\n",
        "        y_true, y_pred = [], []\n",
        "        for xb, yb in test_loader:\n",
        "            xb = xb.to(device, non_blocking=True)\n",
        "            with torch.autocast(device_type='cuda', enabled=(device.type=='cuda')):\n",
        "                logits = model(xb)\n",
        "            y_pred.extend(logits.argmax(1).cpu().tolist())\n",
        "            y_true.extend(yb.numpy().tolist())\n",
        "        print(\"\\nPer-class report:\\n\",\n",
        "              classification_report(y_true, y_pred, target_names=test_base.classes, digits=4, zero_division=0))\n",
        "    per_class_report()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BO37_q4zymsM",
        "outputId": "c0269c10-6ca6-4b73-9a3e-eb668e9a1c30"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Device: cuda\n",
            "Albumentations version: 2.0.8\n",
            "[WARN] RandomResizedCrop not usable in this Albumentations build. Using Resize+RandomCrop fallback. Error: 1 validation error for InitSchema\n",
            "size\n",
            "  Field required [type=missing, input_value={'scale': (0.85, 1.0), 'r...: None, 'strict': False}, input_type=dict]\n",
            "    For further information visit https://errors.pydantic.dev/2.11/v/missing\n",
            "Train images: 891 | Classes (27): ['Actinophrys', 'Amoeba', 'Arcella', 'Aspidisca', 'Ceratium', 'Codosiga', 'Colpoda', 'Epistylis', 'Euglena', 'Euglypha', 'Gonyaulax', 'Gymnodinium', 'Hydra', 'Keratella_quadrala', 'Noctiluca', 'Paramecium', 'Phacus', 'Rod_bacteria', 'Rotifera', 'Siprostomum', 'Spherical_bacteria', 'Spiral_bacteria', 'Stentor', 'Stylonychia', 'Synchaeta', 'Vorticella', 'Yeast']\n",
            "\n",
            "===== K-Fold 1/5 ===== (train 712 | val 179)\n",
            "Downloading: \"https://download.pytorch.org/models/convnext_tiny-983f1562.pth\" to /root/.cache/torch/hub/checkpoints/convnext_tiny-983f1562.pth\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 109M/109M [00:00<00:00, 181MB/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[Fold 1] Ep 01/60 | Train loss 3.0623 acc 13.20% f1M 0.0553 | Val loss 2.6922 acc 20.11% f1M 0.0655 | 41.9s\n",
            "[Fold 1] Ep 02/60 | Train loss 2.4791 acc 29.21% f1M 0.1579 | Val loss 2.3313 acc 31.84% f1M 0.1963 | 7.4s\n",
            "[Fold 1] Ep 03/60 | Train loss 2.1156 acc 46.63% f1M 0.3375 | Val loss 2.0573 acc 45.25% f1M 0.3836 | 7.5s\n",
            "[Fold 1] Ep 04/60 | Train loss 1.8307 acc 54.78% f1M 0.4327 | Val loss 1.8565 acc 50.28% f1M 0.4131 | 7.5s\n",
            "[Fold 1] Ep 05/60 | Train loss 1.6514 acc 61.38% f1M 0.5735 | Val loss 1.7111 acc 58.10% f1M 0.5426 | 7.3s\n",
            "[Fold 1] Ep 06/60 | Train loss 1.4635 acc 67.28% f1M 0.6127 | Val loss 1.5957 acc 60.89% f1M 0.5797 | 8.6s\n",
            "[Fold 1] Ep 07/60 | Train loss 0.9542 acc 77.95% f1M 0.7920 | Val loss 0.9460 acc 73.74% f1M 0.7619 | 17.9s\n",
            "[Fold 1] Ep 08/60 | Train loss 0.4655 acc 89.89% f1M 0.9062 | Val loss 0.7965 acc 76.54% f1M 0.8217 | 8.1s\n",
            "[Fold 1] Ep 09/60 | Train loss 0.2614 acc 94.66% f1M 0.9568 | Val loss 0.7262 acc 79.33% f1M 0.8340 | 7.8s\n",
            "[Fold 1] Ep 10/60 | Train loss 0.1758 acc 96.91% f1M 0.9748 | Val loss 0.7405 acc 79.33% f1M 0.8299 | 9.0s\n",
            "[Fold 1] Ep 11/60 | Train loss 0.1437 acc 97.75% f1M 0.9788 | Val loss 0.7516 acc 77.65% f1M 0.8236 | 7.3s\n",
            "[Fold 1] Ep 12/60 | Train loss 0.1104 acc 98.17% f1M 0.9879 | Val loss 0.8576 acc 77.09% f1M 0.8055 | 9.0s\n",
            "[Fold 1] Ep 13/60 | Train loss 0.1288 acc 97.75% f1M 0.9806 | Val loss 0.7310 acc 78.77% f1M 0.8220 | 7.3s\n",
            "[Fold 1] Ep 14/60 | Train loss 0.0894 acc 98.60% f1M 0.9876 | Val loss 0.7438 acc 79.33% f1M 0.8391 | 8.6s\n",
            "[Fold 1] Ep 15/60 | Train loss 0.0589 acc 99.02% f1M 0.9928 | Val loss 0.8046 acc 78.21% f1M 0.8204 | 8.3s\n",
            "[Fold 1] Ep 16/60 | Train loss 0.0534 acc 99.30% f1M 0.9945 | Val loss 0.8284 acc 76.54% f1M 0.7966 | 7.3s\n",
            "[Fold 1] Ep 17/60 | Train loss 0.0403 acc 99.44% f1M 0.9959 | Val loss 0.8102 acc 78.77% f1M 0.8230 | 8.8s\n",
            "[Fold 1] Ep 18/60 | Train loss 0.0693 acc 98.46% f1M 0.9898 | Val loss 0.8801 acc 78.21% f1M 0.8226 | 7.5s\n",
            "[Fold 1] Ep 19/60 | Train loss 0.0606 acc 98.31% f1M 0.9902 | Val loss 0.8199 acc 77.65% f1M 0.8110 | 8.8s\n",
            "[Fold 1] Ep 20/60 | Train loss 0.0471 acc 98.46% f1M 0.9911 | Val loss 0.8199 acc 79.89% f1M 0.8390 | 8.0s\n",
            "[Fold 1] Ep 21/60 | Train loss 0.0289 acc 99.16% f1M 0.9941 | Val loss 0.8466 acc 78.21% f1M 0.8252 | 7.9s\n",
            "[Fold 1] Ep 22/60 | Train loss 0.0294 acc 99.30% f1M 0.9961 | Val loss 0.9361 acc 77.09% f1M 0.8179 | 8.6s\n",
            "[Fold 1] Ep 23/60 | Train loss 0.0344 acc 98.88% f1M 0.9932 | Val loss 1.0178 acc 76.54% f1M 0.8168 | 7.4s\n",
            "[Fold 1] Ep 24/60 | Train loss 0.0267 acc 99.30% f1M 0.9970 | Val loss 0.9794 acc 75.98% f1M 0.8018 | 8.8s\n",
            "[Fold 1] Ep 25/60 | Train loss 0.0165 acc 99.58% f1M 0.9966 | Val loss 0.9965 acc 78.21% f1M 0.8102 | 7.5s\n",
            "[Fold 1] Ep 26/60 | Train loss 0.0269 acc 99.16% f1M 0.9965 | Val loss 0.9869 acc 77.65% f1M 0.8190 | 8.7s\n",
            "[Fold 1] Ep 27/60 | Train loss 0.0268 acc 99.16% f1M 0.9957 | Val loss 0.9901 acc 76.54% f1M 0.7937 | 8.0s\n",
            "[Fold 1] Ep 28/60 | Train loss 0.0368 acc 98.88% f1M 0.9916 | Val loss 1.0555 acc 74.30% f1M 0.7944 | 7.6s\n",
            "[Fold 1] Ep 29/60 | Train loss 0.0417 acc 98.46% f1M 0.9880 | Val loss 1.0536 acc 75.98% f1M 0.7949 | 8.6s\n",
            "[Fold 1] Ep 30/60 | Train loss 0.0302 acc 99.16% f1M 0.9928 | Val loss 0.9706 acc 78.21% f1M 0.8130 | 7.4s\n",
            "[Fold 1] Ep 31/60 | Train loss 0.0464 acc 97.89% f1M 0.9817 | Val loss 1.1807 acc 75.42% f1M 0.7847 | 8.9s\n",
            "[Fold 1] Ep 32/60 | Train loss 0.0700 acc 97.47% f1M 0.9738 | Val loss 0.9968 acc 78.21% f1M 0.8335 | 7.5s\n",
            "[Fold 1] Ep 33/60 | Train loss 0.0430 acc 98.88% f1M 0.9921 | Val loss 1.0229 acc 79.33% f1M 0.8315 | 8.7s\n",
            "[Fold 1] Ep 34/60 | Train loss 0.0293 acc 98.88% f1M 0.9905 | Val loss 1.0388 acc 77.09% f1M 0.8150 | 8.0s\n",
            "[Fold 1] Early stop @ 34 (best @ 14, F1M=0.8391)\n",
            "[Fold 1] Best Val â†’ acc 79.33% | f1M 0.8391 (epoch 14)\n",
            "\n",
            "===== K-Fold 2/5 ===== (train 713 | val 178)\n",
            "[Fold 2] Ep 01/60 | Train loss 3.0579 acc 14.87% f1M 0.0635 | Val loss 2.6990 acc 24.16% f1M 0.0839 | 27.4s\n",
            "[Fold 2] Ep 02/60 | Train loss 2.5261 acc 30.43% f1M 0.1630 | Val loss 2.3256 acc 37.64% f1M 0.2230 | 7.1s\n",
            "[Fold 2] Ep 03/60 | Train loss 2.1443 acc 46.84% f1M 0.3592 | Val loss 2.0148 acc 50.56% f1M 0.3782 | 8.4s\n",
            "[Fold 2] Ep 04/60 | Train loss 1.8507 acc 59.47% f1M 0.5218 | Val loss 1.7993 acc 54.49% f1M 0.4339 | 7.0s\n",
            "[Fold 2] Ep 05/60 | Train loss 1.6571 acc 63.25% f1M 0.5859 | Val loss 1.6485 acc 58.43% f1M 0.4722 | 8.1s\n",
            "[Fold 2] Ep 06/60 | Train loss 1.4973 acc 67.32% f1M 0.6271 | Val loss 1.5315 acc 60.67% f1M 0.5083 | 7.5s\n",
            "[Fold 2] Ep 07/60 | Train loss 0.9536 acc 77.28% f1M 0.7798 | Val loss 0.7519 acc 81.46% f1M 0.7962 | 11.9s\n",
            "[Fold 2] Ep 08/60 | Train loss 0.4733 acc 89.06% f1M 0.9046 | Val loss 0.6347 acc 84.83% f1M 0.8215 | 8.9s\n",
            "[Fold 2] Ep 09/60 | Train loss 0.2860 acc 93.97% f1M 0.9556 | Val loss 0.6221 acc 83.15% f1M 0.8316 | 8.3s\n",
            "[Fold 2] Ep 10/60 | Train loss 0.2012 acc 95.65% f1M 0.9641 | Val loss 0.6111 acc 83.15% f1M 0.8371 | 7.8s\n",
            "[Fold 2] Ep 11/60 | Train loss 0.1181 acc 98.18% f1M 0.9872 | Val loss 0.6396 acc 84.27% f1M 0.8574 | 8.7s\n",
            "[Fold 2] Ep 12/60 | Train loss 0.0977 acc 98.74% f1M 0.9917 | Val loss 0.5877 acc 83.15% f1M 0.8500 | 7.3s\n",
            "[Fold 2] Ep 13/60 | Train loss 0.0643 acc 99.02% f1M 0.9919 | Val loss 0.5746 acc 86.52% f1M 0.8648 | 8.8s\n",
            "[Fold 2] Ep 14/60 | Train loss 0.0581 acc 99.16% f1M 0.9965 | Val loss 0.5760 acc 85.96% f1M 0.8690 | 7.6s\n",
            "[Fold 2] Ep 15/60 | Train loss 0.0427 acc 99.44% f1M 0.9967 | Val loss 0.5867 acc 84.27% f1M 0.8554 | 8.7s\n",
            "[Fold 2] Ep 16/60 | Train loss 0.0423 acc 98.88% f1M 0.9912 | Val loss 0.5814 acc 85.96% f1M 0.8716 | 8.8s\n",
            "[Fold 2] Ep 17/60 | Train loss 0.0311 acc 99.58% f1M 0.9980 | Val loss 0.5968 acc 84.83% f1M 0.8637 | 7.4s\n",
            "[Fold 2] Ep 18/60 | Train loss 0.0672 acc 98.04% f1M 0.9859 | Val loss 0.6307 acc 83.71% f1M 0.8485 | 8.7s\n",
            "[Fold 2] Ep 19/60 | Train loss 0.0440 acc 99.16% f1M 0.9948 | Val loss 0.6462 acc 83.15% f1M 0.8007 | 7.3s\n",
            "[Fold 2] Ep 20/60 | Train loss 0.0366 acc 99.16% f1M 0.9945 | Val loss 0.7561 acc 82.02% f1M 0.8312 | 8.8s\n",
            "[Fold 2] Ep 21/60 | Train loss 0.0335 acc 99.30% f1M 0.9956 | Val loss 0.7579 acc 82.58% f1M 0.8390 | 7.7s\n",
            "[Fold 2] Ep 22/60 | Train loss 0.0404 acc 99.02% f1M 0.9932 | Val loss 0.7304 acc 83.71% f1M 0.8529 | 8.0s\n",
            "[Fold 2] Ep 23/60 | Train loss 0.0390 acc 98.74% f1M 0.9906 | Val loss 0.8065 acc 83.15% f1M 0.8407 | 8.5s\n",
            "[Fold 2] Ep 24/60 | Train loss 0.0435 acc 98.74% f1M 0.9921 | Val loss 0.7185 acc 84.27% f1M 0.8638 | 7.5s\n",
            "[Fold 2] Ep 25/60 | Train loss 0.0320 acc 98.74% f1M 0.9901 | Val loss 0.7795 acc 83.15% f1M 0.8354 | 8.7s\n",
            "[Fold 2] Ep 26/60 | Train loss 0.0185 acc 99.72% f1M 0.9975 | Val loss 0.8165 acc 84.27% f1M 0.8420 | 7.4s\n",
            "[Fold 2] Ep 27/60 | Train loss 0.0243 acc 99.16% f1M 0.9949 | Val loss 0.7892 acc 83.71% f1M 0.8502 | 8.9s\n",
            "[Fold 2] Ep 28/60 | Train loss 0.0295 acc 99.30% f1M 0.9927 | Val loss 1.0060 acc 77.53% f1M 0.7757 | 8.2s\n",
            "[Fold 2] Ep 29/60 | Train loss 0.0333 acc 98.60% f1M 0.9930 | Val loss 0.7985 acc 82.58% f1M 0.8213 | 7.6s\n",
            "[Fold 2] Ep 30/60 | Train loss 0.0211 acc 99.30% f1M 0.9965 | Val loss 0.9302 acc 80.34% f1M 0.8163 | 8.6s\n",
            "[Fold 2] Ep 31/60 | Train loss 0.0234 acc 99.30% f1M 0.9949 | Val loss 0.8783 acc 81.46% f1M 0.7804 | 7.2s\n",
            "[Fold 2] Ep 32/60 | Train loss 0.0198 acc 99.30% f1M 0.9948 | Val loss 0.9264 acc 79.78% f1M 0.7768 | 8.9s\n",
            "[Fold 2] Ep 33/60 | Train loss 0.0274 acc 99.02% f1M 0.9931 | Val loss 0.8978 acc 80.90% f1M 0.7888 | 7.6s\n",
            "[Fold 2] Ep 34/60 | Train loss 0.0191 acc 99.44% f1M 0.9976 | Val loss 0.9450 acc 81.46% f1M 0.8168 | 8.4s\n",
            "[Fold 2] Ep 35/60 | Train loss 0.0133 acc 99.58% f1M 0.9979 | Val loss 0.9415 acc 80.34% f1M 0.7919 | 8.1s\n",
            "[Fold 2] Ep 36/60 | Train loss 0.0167 acc 99.44% f1M 0.9965 | Val loss 0.8791 acc 83.15% f1M 0.8472 | 7.8s\n",
            "[Fold 2] Early stop @ 36 (best @ 16, F1M=0.8716)\n",
            "[Fold 2] Best Val â†’ acc 85.96% | f1M 0.8716 (epoch 16)\n",
            "\n",
            "===== K-Fold 3/5 ===== (train 713 | val 178)\n",
            "[Fold 3] Ep 01/60 | Train loss 3.0462 acc 15.29% f1M 0.0550 | Val loss 2.7429 acc 22.47% f1M 0.0883 | 9.1s\n",
            "[Fold 3] Ep 02/60 | Train loss 2.5274 acc 27.63% f1M 0.1570 | Val loss 2.4015 acc 32.02% f1M 0.1792 | 6.7s\n",
            "[Fold 3] Ep 03/60 | Train loss 2.1331 acc 45.02% f1M 0.3590 | Val loss 2.1106 acc 48.31% f1M 0.3762 | 8.2s\n",
            "[Fold 3] Ep 04/60 | Train loss 1.8018 acc 60.03% f1M 0.5184 | Val loss 1.9058 acc 53.93% f1M 0.4399 | 7.2s\n",
            "[Fold 3] Ep 05/60 | Train loss 1.6293 acc 63.11% f1M 0.5763 | Val loss 1.7513 acc 53.93% f1M 0.4378 | 7.6s\n",
            "[Fold 3] Ep 06/60 | Train loss 1.4343 acc 66.62% f1M 0.6400 | Val loss 1.6471 acc 54.49% f1M 0.4608 | 7.2s\n",
            "[Fold 3] Ep 07/60 | Train loss 0.8818 acc 76.30% f1M 0.7697 | Val loss 0.9773 acc 70.79% f1M 0.7124 | 8.1s\n",
            "[Fold 3] Ep 08/60 | Train loss 0.4334 acc 90.18% f1M 0.9187 | Val loss 0.8510 acc 78.65% f1M 0.7924 | 8.5s\n",
            "[Fold 3] Ep 09/60 | Train loss 0.2804 acc 95.23% f1M 0.9649 | Val loss 0.8012 acc 78.09% f1M 0.7727 | 7.3s\n",
            "[Fold 3] Ep 10/60 | Train loss 0.1719 acc 97.62% f1M 0.9790 | Val loss 0.7801 acc 80.34% f1M 0.8284 | 8.7s\n",
            "[Fold 3] Ep 11/60 | Train loss 0.1158 acc 97.76% f1M 0.9853 | Val loss 0.7638 acc 78.09% f1M 0.7612 | 7.1s\n",
            "[Fold 3] Ep 12/60 | Train loss 0.0818 acc 98.88% f1M 0.9934 | Val loss 0.8107 acc 77.53% f1M 0.7854 | 8.3s\n",
            "[Fold 3] Ep 13/60 | Train loss 0.0621 acc 99.44% f1M 0.9965 | Val loss 0.8369 acc 80.34% f1M 0.8310 | 7.4s\n",
            "[Fold 3] Ep 14/60 | Train loss 0.0569 acc 98.60% f1M 0.9906 | Val loss 0.8257 acc 79.78% f1M 0.8218 | 7.7s\n",
            "[Fold 3] Ep 15/60 | Train loss 0.0537 acc 98.88% f1M 0.9920 | Val loss 0.8084 acc 81.46% f1M 0.8477 | 8.1s\n",
            "[Fold 3] Ep 16/60 | Train loss 0.0540 acc 98.74% f1M 0.9921 | Val loss 0.8660 acc 80.34% f1M 0.8278 | 6.9s\n",
            "[Fold 3] Ep 17/60 | Train loss 0.0310 acc 99.58% f1M 0.9977 | Val loss 0.9583 acc 77.53% f1M 0.7962 | 8.0s\n",
            "[Fold 3] Ep 18/60 | Train loss 0.0617 acc 99.16% f1M 0.9951 | Val loss 0.8891 acc 79.78% f1M 0.8354 | 7.1s\n",
            "[Fold 3] Ep 19/60 | Train loss 0.0529 acc 98.88% f1M 0.9907 | Val loss 1.0267 acc 75.28% f1M 0.7852 | 8.5s\n",
            "[Fold 3] Ep 20/60 | Train loss 0.0793 acc 98.46% f1M 0.9898 | Val loss 0.8627 acc 76.97% f1M 0.7874 | 7.0s\n",
            "[Fold 3] Ep 21/60 | Train loss 0.0580 acc 98.46% f1M 0.9909 | Val loss 1.1987 acc 74.16% f1M 0.8007 | 8.2s\n",
            "[Fold 3] Ep 22/60 | Train loss 0.0601 acc 98.46% f1M 0.9895 | Val loss 1.0328 acc 78.65% f1M 0.8218 | 7.0s\n",
            "[Fold 3] Ep 23/60 | Train loss 0.0361 acc 99.02% f1M 0.9888 | Val loss 1.0586 acc 76.40% f1M 0.7974 | 8.4s\n",
            "[Fold 3] Ep 24/60 | Train loss 0.0348 acc 98.88% f1M 0.9923 | Val loss 1.1370 acc 76.40% f1M 0.7929 | 7.5s\n",
            "[Fold 3] Ep 25/60 | Train loss 0.0262 acc 99.16% f1M 0.9937 | Val loss 1.1350 acc 76.97% f1M 0.7984 | 7.7s\n",
            "[Fold 3] Ep 26/60 | Train loss 0.0244 acc 98.74% f1M 0.9912 | Val loss 1.0486 acc 76.40% f1M 0.7888 | 8.1s\n",
            "[Fold 3] Ep 27/60 | Train loss 0.0212 acc 99.58% f1M 0.9962 | Val loss 1.1318 acc 77.53% f1M 0.8054 | 7.1s\n",
            "[Fold 3] Ep 28/60 | Train loss 0.0164 acc 99.44% f1M 0.9973 | Val loss 1.0959 acc 78.09% f1M 0.8057 | 8.6s\n",
            "[Fold 3] Ep 29/60 | Train loss 0.0121 acc 99.72% f1M 0.9986 | Val loss 1.1014 acc 76.97% f1M 0.7929 | 7.1s\n",
            "[Fold 3] Ep 30/60 | Train loss 0.0104 acc 99.58% f1M 0.9982 | Val loss 1.1271 acc 76.97% f1M 0.7978 | 8.5s\n",
            "[Fold 3] Ep 31/60 | Train loss 0.0110 acc 99.58% f1M 0.9965 | Val loss 1.1505 acc 76.97% f1M 0.7987 | 7.1s\n",
            "[Fold 3] Ep 32/60 | Train loss 0.0154 acc 99.44% f1M 0.9973 | Val loss 1.1075 acc 76.40% f1M 0.7992 | 8.4s\n",
            "[Fold 3] Ep 33/60 | Train loss 0.0197 acc 99.30% f1M 0.9969 | Val loss 1.1802 acc 78.09% f1M 0.8076 | 7.7s\n",
            "[Fold 3] Ep 34/60 | Train loss 0.0174 acc 99.58% f1M 0.9982 | Val loss 1.1542 acc 79.78% f1M 0.8194 | 7.4s\n",
            "[Fold 3] Ep 35/60 | Train loss 0.0497 acc 98.88% f1M 0.9935 | Val loss 1.1790 acc 76.97% f1M 0.8023 | 8.1s\n",
            "[Fold 3] Early stop @ 35 (best @ 15, F1M=0.8477)\n",
            "[Fold 3] Best Val â†’ acc 81.46% | f1M 0.8477 (epoch 15)\n",
            "\n",
            "===== K-Fold 4/5 ===== (train 713 | val 178)\n",
            "[Fold 4] Ep 01/60 | Train loss 3.0799 acc 14.73% f1M 0.0539 | Val loss 2.6746 acc 19.66% f1M 0.0745 | 7.2s\n",
            "[Fold 4] Ep 02/60 | Train loss 2.4567 acc 33.10% f1M 0.1915 | Val loss 2.3022 acc 37.08% f1M 0.2361 | 7.9s\n",
            "[Fold 4] Ep 03/60 | Train loss 2.1414 acc 47.55% f1M 0.3536 | Val loss 2.0055 acc 46.63% f1M 0.3725 | 7.2s\n",
            "[Fold 4] Ep 04/60 | Train loss 1.8197 acc 57.22% f1M 0.4871 | Val loss 1.7953 acc 53.93% f1M 0.4565 | 7.3s\n",
            "[Fold 4] Ep 05/60 | Train loss 1.6229 acc 62.69% f1M 0.5622 | Val loss 1.6354 acc 58.43% f1M 0.5136 | 7.2s\n",
            "[Fold 4] Ep 06/60 | Train loss 1.4606 acc 69.28% f1M 0.6479 | Val loss 1.5075 acc 58.99% f1M 0.5157 | 7.0s\n",
            "[Fold 4] Ep 07/60 | Train loss 0.8975 acc 78.12% f1M 0.7765 | Val loss 0.7356 acc 78.65% f1M 0.8015 | 8.3s\n",
            "[Fold 4] Ep 08/60 | Train loss 0.4487 acc 89.34% f1M 0.9044 | Val loss 0.6138 acc 82.58% f1M 0.8211 | 7.1s\n",
            "[Fold 4] Ep 09/60 | Train loss 0.2857 acc 93.69% f1M 0.9395 | Val loss 0.5751 acc 83.15% f1M 0.8357 | 8.6s\n",
            "[Fold 4] Ep 10/60 | Train loss 0.1878 acc 97.19% f1M 0.9746 | Val loss 0.5636 acc 80.34% f1M 0.8141 | 7.2s\n",
            "[Fold 4] Ep 11/60 | Train loss 0.1590 acc 97.34% f1M 0.9784 | Val loss 0.5898 acc 82.58% f1M 0.8366 | 8.6s\n",
            "[Fold 4] Ep 12/60 | Train loss 0.1130 acc 97.76% f1M 0.9841 | Val loss 0.6024 acc 82.58% f1M 0.8239 | 7.4s\n",
            "[Fold 4] Ep 13/60 | Train loss 0.0994 acc 97.90% f1M 0.9880 | Val loss 0.7290 acc 77.53% f1M 0.7965 | 7.9s\n",
            "[Fold 4] Ep 14/60 | Train loss 0.0600 acc 99.02% f1M 0.9938 | Val loss 0.6312 acc 82.02% f1M 0.8404 | 7.8s\n",
            "[Fold 4] Ep 15/60 | Train loss 0.0526 acc 99.30% f1M 0.9954 | Val loss 0.6289 acc 83.15% f1M 0.8509 | 7.2s\n",
            "[Fold 4] Ep 16/60 | Train loss 0.0439 acc 99.16% f1M 0.9946 | Val loss 0.6560 acc 79.78% f1M 0.8172 | 8.2s\n",
            "[Fold 4] Ep 17/60 | Train loss 0.0474 acc 98.74% f1M 0.9918 | Val loss 0.6223 acc 82.58% f1M 0.8330 | 7.0s\n",
            "[Fold 4] Ep 18/60 | Train loss 0.0518 acc 98.60% f1M 0.9877 | Val loss 0.6869 acc 79.21% f1M 0.7940 | 8.6s\n",
            "[Fold 4] Ep 19/60 | Train loss 0.0531 acc 98.74% f1M 0.9883 | Val loss 0.7719 acc 80.90% f1M 0.8212 | 7.1s\n",
            "[Fold 4] Ep 20/60 | Train loss 0.0481 acc 99.02% f1M 0.9912 | Val loss 0.7104 acc 81.46% f1M 0.8314 | 8.5s\n",
            "[Fold 4] Ep 21/60 | Train loss 0.0444 acc 98.46% f1M 0.9849 | Val loss 0.7486 acc 80.90% f1M 0.8161 | 7.4s\n",
            "[Fold 4] Ep 22/60 | Train loss 0.0326 acc 98.88% f1M 0.9922 | Val loss 0.7213 acc 82.58% f1M 0.8329 | 7.7s\n",
            "[Fold 4] Ep 23/60 | Train loss 0.0259 acc 99.02% f1M 0.9929 | Val loss 0.8252 acc 78.65% f1M 0.8042 | 8.2s\n",
            "[Fold 4] Ep 24/60 | Train loss 0.0335 acc 98.88% f1M 0.9913 | Val loss 0.8138 acc 82.02% f1M 0.8292 | 7.1s\n",
            "[Fold 4] Ep 25/60 | Train loss 0.0424 acc 98.60% f1M 0.9898 | Val loss 0.8068 acc 80.34% f1M 0.8180 | 8.6s\n",
            "[Fold 4] Ep 26/60 | Train loss 0.0558 acc 98.04% f1M 0.9867 | Val loss 0.7507 acc 81.46% f1M 0.8270 | 7.1s\n",
            "[Fold 4] Ep 27/60 | Train loss 0.0386 acc 98.74% f1M 0.9889 | Val loss 0.7793 acc 79.21% f1M 0.8227 | 8.5s\n",
            "[Fold 4] Ep 28/60 | Train loss 0.0459 acc 98.60% f1M 0.9915 | Val loss 0.7435 acc 83.15% f1M 0.8519 | 7.1s\n",
            "[Fold 4] Ep 29/60 | Train loss 0.0273 acc 99.58% f1M 0.9975 | Val loss 0.8058 acc 80.34% f1M 0.8220 | 8.4s\n",
            "[Fold 4] Ep 30/60 | Train loss 0.0274 acc 99.44% f1M 0.9968 | Val loss 0.8059 acc 78.65% f1M 0.8081 | 7.5s\n",
            "[Fold 4] Ep 31/60 | Train loss 0.0274 acc 99.16% f1M 0.9939 | Val loss 0.8608 acc 81.46% f1M 0.8252 | 7.7s\n",
            "[Fold 4] Ep 32/60 | Train loss 0.0251 acc 99.16% f1M 0.9935 | Val loss 0.9069 acc 78.65% f1M 0.7987 | 8.1s\n",
            "[Fold 4] Ep 33/60 | Train loss 0.0528 acc 99.16% f1M 0.9924 | Val loss 0.8234 acc 82.02% f1M 0.8383 | 7.1s\n",
            "[Fold 4] Ep 34/60 | Train loss 0.0275 acc 99.02% f1M 0.9941 | Val loss 0.8835 acc 80.34% f1M 0.8093 | 8.3s\n",
            "[Fold 4] Ep 35/60 | Train loss 0.0266 acc 99.30% f1M 0.9937 | Val loss 0.8481 acc 80.90% f1M 0.8135 | 7.2s\n",
            "[Fold 4] Ep 36/60 | Train loss 0.0240 acc 99.16% f1M 0.9946 | Val loss 0.8528 acc 79.78% f1M 0.8117 | 8.5s\n",
            "[Fold 4] Ep 37/60 | Train loss 0.0163 acc 99.44% f1M 0.9949 | Val loss 0.8564 acc 79.21% f1M 0.8143 | 7.0s\n",
            "[Fold 4] Ep 38/60 | Train loss 0.0156 acc 99.58% f1M 0.9960 | Val loss 0.8813 acc 79.78% f1M 0.8191 | 8.3s\n",
            "[Fold 4] Ep 39/60 | Train loss 0.0204 acc 99.02% f1M 0.9913 | Val loss 1.0320 acc 78.09% f1M 0.7858 | 7.8s\n",
            "[Fold 4] Ep 40/60 | Train loss 0.0195 acc 99.16% f1M 0.9910 | Val loss 0.9330 acc 79.78% f1M 0.8133 | 7.3s\n",
            "[Fold 4] Ep 41/60 | Train loss 0.0156 acc 99.30% f1M 0.9946 | Val loss 0.8558 acc 80.34% f1M 0.8285 | 8.2s\n",
            "[Fold 4] Ep 42/60 | Train loss 0.0199 acc 99.30% f1M 0.9955 | Val loss 0.7808 acc 82.02% f1M 0.8347 | 7.0s\n",
            "[Fold 4] Ep 43/60 | Train loss 0.0152 acc 99.58% f1M 0.9974 | Val loss 0.8275 acc 82.58% f1M 0.8242 | 8.5s\n",
            "[Fold 4] Ep 44/60 | Train loss 0.0265 acc 99.30% f1M 0.9950 | Val loss 0.8332 acc 79.78% f1M 0.8217 | 7.0s\n",
            "[Fold 4] Ep 45/60 | Train loss 0.0350 acc 98.88% f1M 0.9890 | Val loss 0.8812 acc 82.58% f1M 0.8235 | 8.4s\n",
            "[Fold 4] Ep 46/60 | Train loss 0.0285 acc 98.88% f1M 0.9908 | Val loss 0.8486 acc 80.34% f1M 0.8147 | 7.5s\n",
            "[Fold 4] Ep 47/60 | Train loss 0.0738 acc 98.60% f1M 0.9868 | Val loss 1.0158 acc 76.97% f1M 0.7679 | 8.0s\n",
            "[Fold 4] Ep 48/60 | Train loss 0.0766 acc 98.04% f1M 0.9835 | Val loss 0.8047 acc 79.21% f1M 0.8075 | 8.2s\n",
            "[Fold 4] Early stop @ 48 (best @ 28, F1M=0.8519)\n",
            "[Fold 4] Best Val â†’ acc 83.15% | f1M 0.8519 (epoch 28)\n",
            "\n",
            "===== K-Fold 5/5 ===== (train 713 | val 178)\n",
            "[Fold 5] Ep 01/60 | Train loss 3.0172 acc 17.25% f1M 0.0818 | Val loss 2.6800 acc 24.16% f1M 0.0921 | 7.4s\n",
            "[Fold 5] Ep 02/60 | Train loss 2.4785 acc 30.43% f1M 0.1477 | Val loss 2.3371 acc 32.02% f1M 0.1984 | 8.4s\n",
            "[Fold 5] Ep 03/60 | Train loss 2.1060 acc 45.72% f1M 0.3407 | Val loss 2.0502 acc 42.70% f1M 0.3251 | 6.6s\n",
            "[Fold 5] Ep 04/60 | Train loss 1.8246 acc 53.44% f1M 0.4325 | Val loss 1.8447 acc 52.25% f1M 0.4462 | 8.0s\n",
            "[Fold 5] Ep 05/60 | Train loss 1.6942 acc 59.89% f1M 0.5354 | Val loss 1.6818 acc 59.55% f1M 0.5290 | 7.0s\n",
            "[Fold 5] Ep 06/60 | Train loss 1.4874 acc 66.76% f1M 0.6214 | Val loss 1.5697 acc 62.92% f1M 0.5776 | 7.5s\n",
            "[Fold 5] Ep 07/60 | Train loss 0.9240 acc 76.44% f1M 0.7540 | Val loss 0.8578 acc 78.09% f1M 0.7754 | 8.1s\n",
            "[Fold 5] Ep 08/60 | Train loss 0.4886 acc 88.78% f1M 0.8838 | Val loss 0.7170 acc 82.02% f1M 0.8382 | 7.3s\n",
            "[Fold 5] Ep 09/60 | Train loss 0.3135 acc 94.53% f1M 0.9478 | Val loss 0.7225 acc 82.58% f1M 0.8424 | 8.6s\n",
            "[Fold 5] Ep 10/60 | Train loss 0.2090 acc 94.67% f1M 0.9471 | Val loss 0.6924 acc 83.71% f1M 0.8629 | 7.2s\n",
            "[Fold 5] Ep 11/60 | Train loss 0.1727 acc 96.91% f1M 0.9781 | Val loss 0.6841 acc 81.46% f1M 0.8324 | 8.5s\n",
            "[Fold 5] Ep 12/60 | Train loss 0.1077 acc 98.46% f1M 0.9888 | Val loss 0.7731 acc 83.15% f1M 0.8611 | 7.0s\n",
            "[Fold 5] Ep 13/60 | Train loss 0.0772 acc 98.32% f1M 0.9902 | Val loss 0.8017 acc 82.58% f1M 0.8496 | 8.3s\n",
            "[Fold 5] Ep 14/60 | Train loss 0.1117 acc 98.04% f1M 0.9883 | Val loss 0.7337 acc 83.15% f1M 0.8483 | 7.7s\n",
            "[Fold 5] Ep 15/60 | Train loss 0.0704 acc 98.46% f1M 0.9889 | Val loss 0.7234 acc 84.83% f1M 0.8696 | 7.6s\n",
            "[Fold 5] Ep 16/60 | Train loss 0.1069 acc 98.32% f1M 0.9870 | Val loss 0.7140 acc 85.96% f1M 0.8883 | 8.3s\n",
            "[Fold 5] Ep 17/60 | Train loss 0.0605 acc 98.88% f1M 0.9929 | Val loss 0.7299 acc 85.39% f1M 0.8847 | 7.1s\n",
            "[Fold 5] Ep 18/60 | Train loss 0.0686 acc 98.18% f1M 0.9888 | Val loss 0.9002 acc 83.71% f1M 0.8588 | 8.5s\n",
            "[Fold 5] Ep 19/60 | Train loss 0.0442 acc 98.88% f1M 0.9924 | Val loss 0.9569 acc 80.90% f1M 0.8545 | 7.1s\n",
            "[Fold 5] Ep 20/60 | Train loss 0.0477 acc 98.74% f1M 0.9923 | Val loss 0.9945 acc 79.21% f1M 0.8327 | 8.5s\n",
            "[Fold 5] Ep 21/60 | Train loss 0.0341 acc 99.44% f1M 0.9971 | Val loss 0.8299 acc 80.90% f1M 0.8524 | 7.3s\n",
            "[Fold 5] Ep 22/60 | Train loss 0.0333 acc 98.74% f1M 0.9916 | Val loss 0.8311 acc 82.58% f1M 0.8705 | 8.1s\n",
            "[Fold 5] Ep 23/60 | Train loss 0.0605 acc 98.46% f1M 0.9889 | Val loss 0.9617 acc 78.65% f1M 0.8275 | 8.1s\n",
            "[Fold 5] Ep 24/60 | Train loss 0.0678 acc 97.62% f1M 0.9835 | Val loss 0.9732 acc 80.90% f1M 0.8231 | 7.2s\n",
            "[Fold 5] Ep 25/60 | Train loss 0.0545 acc 98.18% f1M 0.9869 | Val loss 0.8781 acc 80.34% f1M 0.8439 | 8.5s\n",
            "[Fold 5] Ep 26/60 | Train loss 0.0307 acc 99.02% f1M 0.9928 | Val loss 0.8725 acc 82.58% f1M 0.8575 | 7.1s\n",
            "[Fold 5] Ep 27/60 | Train loss 0.0218 acc 99.16% f1M 0.9956 | Val loss 0.9234 acc 82.02% f1M 0.8465 | 8.5s\n",
            "[Fold 5] Ep 28/60 | Train loss 0.0194 acc 99.44% f1M 0.9969 | Val loss 0.8746 acc 83.15% f1M 0.8528 | 7.4s\n",
            "[Fold 5] Ep 29/60 | Train loss 0.0187 acc 99.30% f1M 0.9943 | Val loss 0.8728 acc 79.78% f1M 0.8217 | 8.6s\n",
            "[Fold 5] Ep 30/60 | Train loss 0.0155 acc 99.44% f1M 0.9967 | Val loss 0.9142 acc 82.02% f1M 0.8622 | 7.7s\n",
            "[Fold 5] Ep 31/60 | Train loss 0.0172 acc 99.44% f1M 0.9970 | Val loss 0.9205 acc 83.15% f1M 0.8431 | 7.6s\n",
            "[Fold 5] Ep 32/60 | Train loss 0.0199 acc 99.44% f1M 0.9972 | Val loss 1.1603 acc 78.65% f1M 0.8167 | 8.2s\n",
            "[Fold 5] Ep 33/60 | Train loss 0.0276 acc 99.30% f1M 0.9964 | Val loss 0.9583 acc 79.78% f1M 0.8277 | 7.1s\n",
            "[Fold 5] Ep 34/60 | Train loss 0.0411 acc 98.32% f1M 0.9898 | Val loss 1.0820 acc 80.90% f1M 0.8308 | 8.5s\n",
            "[Fold 5] Ep 35/60 | Train loss 0.0330 acc 98.60% f1M 0.9911 | Val loss 1.1418 acc 78.65% f1M 0.8165 | 7.3s\n",
            "[Fold 5] Ep 36/60 | Train loss 0.0330 acc 98.74% f1M 0.9913 | Val loss 1.0927 acc 78.09% f1M 0.8065 | 8.5s\n",
            "[Fold 5] Early stop @ 36 (best @ 16, F1M=0.8883)\n",
            "[Fold 5] Best Val â†’ acc 85.96% | f1M 0.8883 (epoch 16)\n",
            "\n",
            "===== CV Summary (best epoch per fold) =====\n",
            "Val Accs: ['79.33%', '85.96%', '81.46%', '83.15%', '85.96%'] | MeanÂ±Std = 83.17% Â± 2.58%\n",
            "Val F1M : ['0.8391', '0.8716', '0.8477', '0.8519', '0.8883'] | MeanÂ±Std = 0.8597 Â± 0.0178\n",
            "[FULL] Ep 01/60 | loss 2.9538 acc 14.93% f1M 0.0740 | 22.3s\n",
            "[FULL] Ep 02/60 | loss 2.3706 acc 35.02% f1M 0.2375 | 7.1s\n",
            "[FULL] Ep 03/60 | loss 1.9671 acc 53.20% f1M 0.4226 | 9.1s\n",
            "[FULL] Ep 04/60 | loss 1.6727 acc 64.42% f1M 0.5875 | 9.2s\n",
            "[FULL] Ep 05/60 | loss 1.4624 acc 67.79% f1M 0.6384 | 7.4s\n",
            "[FULL] Ep 06/60 | loss 1.3202 acc 73.40% f1M 0.7142 | 9.7s\n",
            "[FULL] Ep 07/60 | loss 0.8446 acc 79.01% f1M 0.7950 | 14.7s\n",
            "[FULL] Ep 08/60 | loss 0.3817 acc 90.91% f1M 0.9179 | 9.5s\n",
            "[FULL] Ep 09/60 | loss 0.2239 acc 95.40% f1M 0.9607 | 8.2s\n",
            "[FULL] Ep 10/60 | loss 0.1449 acc 96.30% f1M 0.9715 | 9.5s\n",
            "[FULL] Ep 11/60 | loss 0.0976 acc 98.54% f1M 0.9894 | 9.5s\n",
            "[FULL] Ep 12/60 | loss 0.0796 acc 98.32% f1M 0.9889 | 8.1s\n",
            "[FULL] Ep 13/60 | loss 0.0603 acc 98.77% f1M 0.9919 | 9.3s\n",
            "[FULL] Ep 14/60 | loss 0.0441 acc 99.10% f1M 0.9944 | 8.8s\n",
            "[FULL] Ep 15/60 | loss 0.0456 acc 99.33% f1M 0.9966 | 7.9s\n",
            "[FULL] Ep 16/60 | loss 0.0372 acc 98.99% f1M 0.9946 | 9.0s\n",
            "[FULL] Ep 17/60 | loss 0.0384 acc 98.99% f1M 0.9941 | 8.3s\n",
            "[FULL] Ep 18/60 | loss 0.0707 acc 98.32% f1M 0.9897 | 8.3s\n",
            "[FULL] Ep 19/60 | loss 0.0576 acc 98.65% f1M 0.9904 | 9.0s\n",
            "[FULL] Ep 20/60 | loss 0.0520 acc 98.54% f1M 0.9897 | 7.2s\n",
            "[FULL] Ep 21/60 | loss 0.0481 acc 98.32% f1M 0.9885 | 8.9s\n",
            "[FULL] Ep 22/60 | loss 0.0416 acc 99.21% f1M 0.9957 | 7.5s\n",
            "[FULL] Ep 23/60 | loss 0.0377 acc 98.99% f1M 0.9930 | 8.3s\n",
            "[FULL] Ep 24/60 | loss 0.0346 acc 98.99% f1M 0.9931 | 8.8s\n",
            "[FULL] Ep 25/60 | loss 0.0444 acc 98.65% f1M 0.9914 | 7.8s\n",
            "[FULL] Ep 26/60 | loss 0.0317 acc 99.33% f1M 0.9966 | 8.8s\n",
            "[FULL] Ep 27/60 | loss 0.0341 acc 98.77% f1M 0.9927 | 8.4s\n",
            "[FULL] Ep 28/60 | loss 0.0286 acc 98.99% f1M 0.9941 | 7.8s\n",
            "[FULL] Ep 29/60 | loss 0.0273 acc 98.99% f1M 0.9924 | 8.9s\n",
            "[FULL] Ep 30/60 | loss 0.0327 acc 98.77% f1M 0.9912 | 7.8s\n",
            "[FULL] Ep 31/60 | loss 0.0367 acc 98.88% f1M 0.9919 | 9.0s\n",
            "[FULL] Ep 32/60 | loss 0.0387 acc 98.88% f1M 0.9898 | 8.2s\n",
            "[FULL] Ep 33/60 | loss 0.0290 acc 99.10% f1M 0.9944 | 7.5s\n",
            "[FULL] Ep 34/60 | loss 0.0343 acc 98.54% f1M 0.9903 | 8.7s\n",
            "[FULL] Ep 35/60 | loss 0.0257 acc 98.99% f1M 0.9929 | 7.1s\n",
            "[FULL] Ep 36/60 | loss 0.0242 acc 98.99% f1M 0.9923 | 9.2s\n",
            "[FULL] Ep 37/60 | loss 0.0210 acc 98.99% f1M 0.9935 | 9.2s\n",
            "[FULL] Ep 38/60 | loss 0.0243 acc 98.99% f1M 0.9939 | 7.8s\n",
            "[FULL] Ep 39/60 | loss 0.0347 acc 99.10% f1M 0.9941 | 8.7s\n",
            "[FULL] Ep 40/60 | loss 0.0254 acc 99.10% f1M 0.9948 | 7.5s\n",
            "[FULL] Ep 41/60 | loss 0.0252 acc 99.10% f1M 0.9939 | 8.5s\n",
            "[FULL] Ep 42/60 | loss 0.0245 acc 99.21% f1M 0.9952 | 8.5s\n",
            "[FULL] Ep 43/60 | loss 0.0379 acc 98.88% f1M 0.9909 | 7.4s\n",
            "[FULL] Ep 44/60 | loss 0.0236 acc 99.33% f1M 0.9964 | 9.0s\n",
            "[FULL] Ep 45/60 | loss 0.0217 acc 99.10% f1M 0.9943 | 7.5s\n",
            "[FULL] Ep 46/60 | loss 0.0243 acc 99.33% f1M 0.9943 | 9.0s\n",
            "[FULL] Ep 47/60 | loss 0.0152 acc 99.55% f1M 0.9965 | 8.7s\n",
            "[FULL] Ep 48/60 | loss 0.0125 acc 99.55% f1M 0.9974 | 8.3s\n",
            "[FULL] Ep 49/60 | loss 0.0144 acc 99.33% f1M 0.9951 | 9.4s\n",
            "[FULL] Ep 50/60 | loss 0.0159 acc 99.33% f1M 0.9951 | 8.4s\n",
            "[FULL] Ep 51/60 | loss 0.0224 acc 99.33% f1M 0.9958 | 7.9s\n",
            "[FULL] Ep 52/60 | loss 0.0158 acc 99.44% f1M 0.9970 | 9.0s\n",
            "[FULL] Ep 53/60 | loss 0.0192 acc 99.10% f1M 0.9918 | 7.6s\n",
            "[FULL] Ep 54/60 | loss 0.0143 acc 99.44% f1M 0.9959 | 8.9s\n",
            "[FULL] Ep 55/60 | loss 0.0173 acc 99.33% f1M 0.9965 | 8.8s\n",
            "[FULL] Ep 56/60 | loss 0.0216 acc 99.33% f1M 0.9957 | 7.4s\n",
            "[FULL] Ep 57/60 | loss 0.0295 acc 99.10% f1M 0.9944 | 9.0s\n",
            "[FULL] Ep 58/60 | loss 0.0298 acc 98.99% f1M 0.9936 | 7.7s\n",
            "[FULL] Ep 59/60 | loss 0.0344 acc 98.88% f1M 0.9931 | 8.9s\n",
            "[FULL] Ep 60/60 | loss 0.0212 acc 99.10% f1M 0.9943 | 8.9s\n",
            "\n",
            "TEST â†’ loss 1.0940 | acc 80.91% | f1M 0.8335\n",
            "\n",
            "Per-class report:\n",
            "                     precision    recall  f1-score   support\n",
            "\n",
            "       Actinophrys     1.0000    1.0000    1.0000         5\n",
            "            Amoeba     0.5000    0.4000    0.4444        10\n",
            "           Arcella     1.0000    0.7500    0.8571         4\n",
            "         Aspidisca     0.7500    0.7500    0.7500         4\n",
            "          Ceratium     1.0000    1.0000    1.0000         6\n",
            "          Codosiga     1.0000    0.7500    0.8571         4\n",
            "           Colpoda     0.6667    0.4000    0.5000         5\n",
            "         Epistylis     0.8333    1.0000    0.9091         5\n",
            "           Euglena     0.9167    0.8462    0.8800        26\n",
            "          Euglypha     0.6667    1.0000    0.8000         2\n",
            "         Gonyaulax     0.8000    0.8000    0.8000         5\n",
            "       Gymnodinium     0.8000    1.0000    0.8889         4\n",
            "             Hydra     0.8750    0.7778    0.8235         9\n",
            "Keratella_quadrala     0.8750    1.0000    0.9333         7\n",
            "         Noctiluca     0.8750    1.0000    0.9333         7\n",
            "        Paramecium     0.9259    0.8621    0.8929        29\n",
            "            Phacus     1.0000    1.0000    1.0000         3\n",
            "      Rod_bacteria     0.5714    0.5714    0.5714        14\n",
            "          Rotifera     1.0000    1.0000    1.0000         6\n",
            "       Siprostomum     1.0000    1.0000    1.0000         6\n",
            "Spherical_bacteria     0.6667    0.8571    0.7500        14\n",
            "   Spiral_bacteria     0.3529    0.5000    0.4138        12\n",
            "           Stentor     1.0000    1.0000    1.0000         7\n",
            "       Stylonychia     1.0000    1.0000    1.0000         4\n",
            "         Synchaeta     1.0000    1.0000    1.0000         8\n",
            "        Vorticella     1.0000    0.6667    0.8000         3\n",
            "             Yeast     0.7778    0.6364    0.7000        11\n",
            "\n",
            "          accuracy                         0.8091       220\n",
            "         macro avg     0.8464    0.8358    0.8335       220\n",
            "      weighted avg     0.8226    0.8091    0.8105       220\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# ===================== Swin-Tiny + K-Fold + Albumentations \"Smart Aug\" (no MixUp/CutMix) =====================\n",
        "import os, time, random, numpy as np, torch\n",
        "from torch import nn, optim\n",
        "from torch.utils.data import DataLoader, Dataset\n",
        "from torchvision import datasets, models\n",
        "from torchvision.models import Swin_T_Weights\n",
        "from sklearn.metrics import accuracy_score, f1_score, classification_report\n",
        "from sklearn.model_selection import StratifiedKFold\n",
        "from PIL import Image\n",
        "from torchvision.transforms.functional import to_tensor\n",
        "\n",
        "# Albumentations\n",
        "import albumentations as A\n",
        "from albumentations.pytorch import ToTensorV2\n",
        "import cv2  # for border_mode constants\n",
        "\n",
        "# ---------- Paths ----------\n",
        "train_dir = \"/content/drive/MyDrive/Bacteria Paper/datasets/Processed/train_320\"\n",
        "test_dir  = \"/content/drive/MyDrive/Bacteria Paper/datasets/Processed/test_320\"\n",
        "\n",
        "# ---------- Your dataset mean/std (0â€“1 scale; RGB) ----------\n",
        "DATA_MEAN = [0.54200659, 0.57473042, 0.55953813]   # <-- keep your computed values\n",
        "DATA_STD  = [0.2644171,  0.25311802, 0.26218295]\n",
        "\n",
        "# ---------- Config ----------\n",
        "SEED = 42\n",
        "IMG_SIZE = 320              # Swin-T default pretrain is 224, but 320 works fine in torchvision\n",
        "N_SPLITS = 5\n",
        "EPOCHS = 60\n",
        "PATIENCE = 15\n",
        "\n",
        "HEAD_WARMUP_EPOCHS   = 6     # train only classifier head initially\n",
        "UNFREEZE_LAYER4_EPOCH = 7    # unfreeze last stage (features[-1])\n",
        "UNFREEZE_L34_EPOCH    = 18   # unfreeze last two stages (features[-2] + features[-1])\n",
        "\n",
        "LR_HEAD = 1e-3\n",
        "LR_BB   = 3e-4\n",
        "WEIGHT_DECAY = 5e-5\n",
        "BATCH_GPU, BATCH_CPU = 64, 16\n",
        "NUM_WORKERS = min(8, os.cpu_count() or 2)\n",
        "\n",
        "# Smart aug knobs\n",
        "ROT_LIMIT = 20\n",
        "HFLIP_P   = 0.5\n",
        "VFLIP_P   = 0.2\n",
        "CLAHE_P   = 0.2\n",
        "GBLUR_P   = 0.25\n",
        "MBLUR_P   = 0.15\n",
        "GAMMA_P   = 0.3\n",
        "HSV_P     = 0.3\n",
        "RRC_SCALE = (0.85, 1.0)\n",
        "RRC_RATIO = (0.9, 1.1)\n",
        "\n",
        "# ---------- Seed & Device ----------\n",
        "def set_seed(s):\n",
        "    random.seed(s); np.random.seed(s); torch.manual_seed(s); torch.cuda.manual_seed_all(s)\n",
        "set_seed(SEED)\n",
        "torch.backends.cudnn.benchmark = True\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "print(\"Device:\", device)\n",
        "\n",
        "# ---------- Albumentations pipelines (version-proof) ----------\n",
        "print(\"Albumentations version:\", A.__version__)  # for visibility\n",
        "\n",
        "_first_train = []\n",
        "try:\n",
        "    # Validate signature once; if it errors we will fallback\n",
        "    _ = A.RandomResizedCrop(height=IMG_SIZE, width=IMG_SIZE, scale=RRC_SCALE, ratio=RRC_RATIO, p=1.0)\n",
        "    _first_train.append(\n",
        "        A.RandomResizedCrop(height=IMG_SIZE, width=IMG_SIZE, scale=RRC_SCALE, ratio=RRC_RATIO, p=1.0)\n",
        "    )\n",
        "except Exception as e:\n",
        "    print(\"[WARN] RandomResizedCrop not usable in this Albumentations build. \"\n",
        "          \"Using Resize+RandomCrop fallback. Error:\", e)\n",
        "    _first_train += [\n",
        "        A.Resize(height=int(IMG_SIZE * 1.15), width=int(IMG_SIZE * 1.15)),\n",
        "        A.RandomCrop(height=IMG_SIZE, width=IMG_SIZE),\n",
        "    ]\n",
        "\n",
        "train_aug = A.Compose(\n",
        "    _first_train + [\n",
        "        A.HorizontalFlip(p=HFLIP_P),\n",
        "        A.VerticalFlip(p=VFLIP_P),\n",
        "        A.Rotate(limit=ROT_LIMIT, p=0.5, border_mode=cv2.BORDER_REFLECT_101),\n",
        "        A.RandomBrightnessContrast(p=0.5),\n",
        "        A.RandomGamma(p=GAMMA_P),\n",
        "        A.HueSaturationValue(p=HSV_P),\n",
        "        A.GaussianBlur(blur_limit=(3, 5), p=GBLUR_P),\n",
        "        A.MotionBlur(blur_limit=5, p=MBLUR_P),\n",
        "        A.CLAHE(p=CLAHE_P),\n",
        "        A.Normalize(mean=DATA_MEAN, std=DATA_STD),\n",
        "        ToTensorV2(),\n",
        "    ]\n",
        ")\n",
        "\n",
        "val_aug = A.Compose([\n",
        "    A.Resize(height=int(IMG_SIZE * 1.15), width=int(IMG_SIZE * 1.15)),\n",
        "    A.CenterCrop(height=IMG_SIZE, width=IMG_SIZE),\n",
        "    A.Normalize(mean=DATA_MEAN, std=DATA_STD),\n",
        "    ToTensorV2(),\n",
        "])\n",
        "\n",
        "# ---------- Dataset wrapper using Albumentations ----------\n",
        "class AlbDataset(Dataset):\n",
        "    def __init__(self, base, indices=None, transform=None):\n",
        "        self.base = base\n",
        "        self.samples = base.samples\n",
        "        self.indices = list(indices) if indices is not None else list(range(len(self.samples)))\n",
        "        self.targets = np.array([self.base.targets[i] for i in self.indices], dtype=np.int64)\n",
        "        self.transform = transform\n",
        "    def __len__(self): return len(self.indices)\n",
        "    def __getitem__(self, i):\n",
        "        idx = self.indices[i]\n",
        "        path, label = self.samples[idx]\n",
        "        img = np.array(Image.open(path).convert(\"RGB\"))\n",
        "        if self.transform:\n",
        "            img = self.transform(image=img)[\"image\"]\n",
        "        else:\n",
        "            img = to_tensor(Image.fromarray(img)).float()\n",
        "        return img, label\n",
        "\n",
        "# ---------- Data ----------\n",
        "base_full = datasets.ImageFolder(root=train_dir, transform=None)\n",
        "class_names = base_full.classes\n",
        "num_classes = len(class_names)\n",
        "all_targets = np.array(base_full.targets, dtype=np.int64)\n",
        "print(f\"Train images: {len(base_full)} | Classes ({num_classes}): {class_names}\")\n",
        "\n",
        "# ---------- Model / Optim (Swin-Tiny) ----------\n",
        "def build_swin_tiny(num_classes):\n",
        "    m = models.swin_t(weights=Swin_T_Weights.DEFAULT)\n",
        "    # Replace final Linear head (m.head)\n",
        "    in_f = m.head.in_features\n",
        "    m.head = nn.Linear(in_f, num_classes)\n",
        "    m = m.to(device)\n",
        "    if device.type == \"cuda\":\n",
        "        m = m.to(memory_format=torch.channels_last)\n",
        "    return m\n",
        "\n",
        "def _freeze_all(m):\n",
        "    for p in m.parameters():\n",
        "        p.requires_grad = False\n",
        "\n",
        "def _enable_classifier_head(m):\n",
        "    # Only the final Linear head is trainable in head warm-up\n",
        "    for p in m.head.parameters():\n",
        "        p.requires_grad = True\n",
        "    # (Keeping norm frozen to match your \"same features\" policy)\n",
        "\n",
        "def _enable_last_stage(m):\n",
        "    # Unfreeze last stage: features[-1] + head\n",
        "    for p in m.features[-1].parameters():\n",
        "        p.requires_grad = True\n",
        "    for p in m.head.parameters():\n",
        "        p.requires_grad = True\n",
        "\n",
        "def _enable_last_two_stages(m):\n",
        "    # Unfreeze last two stages: features[-2] and features[-1] + head\n",
        "    for p in m.features[-2].parameters():\n",
        "        p.requires_grad = True\n",
        "    for p in m.features[-1].parameters():\n",
        "        p.requires_grad = True\n",
        "    for p in m.head.parameters():\n",
        "        p.requires_grad = True\n",
        "\n",
        "def set_trainable(m, mode):\n",
        "    # mode: \"head\" | \"layer4\" | \"layer3_4\"\n",
        "    _freeze_all(m)\n",
        "    if mode == \"head\":\n",
        "        _enable_classifier_head(m)\n",
        "    elif mode == \"layer4\":\n",
        "        _enable_last_stage(m)\n",
        "    elif mode == \"layer3_4\":\n",
        "        _enable_last_two_stages(m)\n",
        "\n",
        "def make_optimizer(model):\n",
        "    # Separate LR for head vs backbone (head vs the rest trainable)\n",
        "    head_params, bb_params = [], []\n",
        "    for n, p in model.named_parameters():\n",
        "        if not p.requires_grad:\n",
        "            continue\n",
        "        if n.startswith(\"head.\"):\n",
        "            head_params.append(p)\n",
        "        else:\n",
        "            bb_params.append(p)\n",
        "    return optim.AdamW(\n",
        "        [{'params': head_params, 'lr': LR_HEAD},\n",
        "         {'params': bb_params,   'lr': LR_BB}],\n",
        "        weight_decay=WEIGHT_DECAY\n",
        "    )\n",
        "\n",
        "# ---------- Train/Eval ----------\n",
        "@torch.no_grad()\n",
        "def evaluate(model, loader, criterion):\n",
        "    model.eval()\n",
        "    tot, y_true, y_pred = 0.0, [], []\n",
        "    for xb,yb in loader:\n",
        "        xb = xb.to(device, non_blocking=True); yb = yb.to(device, non_blocking=True)\n",
        "        with torch.autocast(device_type='cuda', enabled=(device.type=='cuda')):\n",
        "            logits = model(xb)\n",
        "            loss = criterion(logits, yb)\n",
        "        tot += loss.item()\n",
        "        y_pred += logits.argmax(1).cpu().tolist()\n",
        "        y_true += yb.cpu().tolist()\n",
        "    loss = tot / max(1, len(loader))\n",
        "    acc  = accuracy_score(y_true, y_pred)\n",
        "    f1m  = f1_score(y_true, y_pred, average=\"macro\", zero_division=0)\n",
        "    return loss, acc, f1m\n",
        "\n",
        "def train_one_epoch(model, loader, optimizer, criterion):\n",
        "    model.train()\n",
        "    tot, y_true, y_pred = 0.0, [], []\n",
        "    for xb,yb in loader:\n",
        "        xb = xb.to(device, non_blocking=True); yb = yb.to(device, non_blocking=True)\n",
        "        optimizer.zero_grad(set_to_none=True)\n",
        "        with torch.autocast(device_type='cuda', enabled=(device.type=='cuda')):\n",
        "            logits = model(xb)\n",
        "            loss = criterion(logits, yb)\n",
        "        loss.backward(); optimizer.step()\n",
        "        tot += loss.item()\n",
        "        with torch.no_grad():\n",
        "            y_pred += logits.argmax(1).cpu().tolist()\n",
        "            y_true += yb.cpu().tolist()\n",
        "    loss = tot / max(1, len(loader))\n",
        "    acc  = accuracy_score(y_true, y_pred)\n",
        "    f1m  = f1_score(y_true, y_pred, average=\"macro\", zero_division=0)\n",
        "    return loss, acc, f1m\n",
        "\n",
        "def make_loader(ds, shuffle):\n",
        "    return DataLoader(\n",
        "        ds,\n",
        "        batch_size=(BATCH_GPU if device.type=='cuda' else BATCH_CPU),\n",
        "        shuffle=shuffle, num_workers=NUM_WORKERS,\n",
        "        pin_memory=(device.type=='cuda'),\n",
        "        persistent_workers=(NUM_WORKERS>0 and device.type=='cuda')\n",
        "    )\n",
        "\n",
        "def run_fold(fold, tr_idx, va_idx):\n",
        "    ds_tr = AlbDataset(base_full, tr_idx, transform=train_aug)\n",
        "    ds_va = AlbDataset(base_full, va_idx, transform=val_aug)\n",
        "    dl_tr = make_loader(ds_tr, shuffle=True)\n",
        "    dl_va = make_loader(ds_va, shuffle=False)\n",
        "\n",
        "    model = build_swin_tiny(num_classes)\n",
        "    set_trainable(model, \"head\")\n",
        "    optimizer = make_optimizer(model)\n",
        "    scheduler = optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max=EPOCHS)\n",
        "    criterion = nn.CrossEntropyLoss()\n",
        "\n",
        "    best = {\"f1\": -1, \"ep\": -1, \"state\": None, \"val_acc\": 0.0}\n",
        "    wait = 0\n",
        "\n",
        "    for ep in range(1, EPOCHS+1):\n",
        "        # gradual unfreeze\n",
        "        if ep == UNFREEZE_LAYER4_EPOCH:\n",
        "            set_trainable(model, \"layer4\"); optimizer = make_optimizer(model)\n",
        "        if ep == UNFREEZE_L34_EPOCH:\n",
        "            set_trainable(model, \"layer3_4\"); optimizer = make_optimizer(model)\n",
        "\n",
        "        t0 = time.time()\n",
        "        tr_loss, tr_acc, tr_f1 = train_one_epoch(model, dl_tr, optimizer, criterion)\n",
        "        va_loss, va_acc, va_f1 = evaluate(model, dl_va, criterion)\n",
        "        scheduler.step()\n",
        "\n",
        "        print(f\"[Fold {fold}] Ep {ep:02d}/{EPOCHS} | \"\n",
        "              f\"Train loss {tr_loss:.4f} acc {tr_acc*100:5.2f}% f1M {tr_f1:.4f} | \"\n",
        "              f\"Val loss {va_loss:.4f} acc {va_acc*100:5.2f}% f1M {va_f1:.4f} | {time.time()-t0:.1f}s\")\n",
        "\n",
        "        if va_f1 > best[\"f1\"]:\n",
        "            best.update({\"f1\": va_f1, \"ep\": ep, \"state\": {k:v.cpu() for k,v in model.state_dict().items()}, \"val_acc\": va_acc})\n",
        "            wait = 0\n",
        "        else:\n",
        "            wait += 1\n",
        "            if wait >= PATIENCE:\n",
        "                print(f\"[Fold {fold}] Early stop @ {ep} (best @ {best['ep']}, F1M={best['f1']:.4f})\")\n",
        "                break\n",
        "\n",
        "    ckpt = f\"swin_tiny_alb_smart_fold{fold}.pt\"\n",
        "    if best[\"state\"] is not None:\n",
        "        torch.save(best[\"state\"], ckpt)\n",
        "\n",
        "    # Final val from best\n",
        "    model.load_state_dict(torch.load(ckpt, map_location=device))\n",
        "    va_loss, va_acc, va_f1 = evaluate(model, dl_va, criterion)\n",
        "    print(f\"[Fold {fold}] Best Val â†’ acc {va_acc*100:5.2f}% | f1M {va_f1:.4f} (epoch {best['ep']})\")\n",
        "    return {\"fold\": fold, \"val_acc\": va_acc, \"val_f1\": va_f1, \"epoch\": best[\"ep\"], \"ckpt\": ckpt}\n",
        "\n",
        "# ---------- K-Fold ----------\n",
        "skf = StratifiedKFold(n_splits=N_SPLITS, shuffle=True, random_state=SEED)\n",
        "fold_summaries = []\n",
        "for fold, (tr_idx, va_idx) in enumerate(skf.split(np.arange(len(all_targets)), all_targets), start=1):\n",
        "    print(f\"\\n===== K-Fold {fold}/{N_SPLITS} ===== (train {len(tr_idx)} | val {len(va_idx)})\")\n",
        "    fs = run_fold(fold, tr_idx, va_idx)\n",
        "    fold_summaries.append(fs)\n",
        "\n",
        "cv_accs = [fs[\"val_acc\"] for fs in fold_summaries]\n",
        "cv_f1s  = [fs[\"val_f1\"] for fs in fold_summaries]\n",
        "print(\"\\n===== CV Summary (best epoch per fold) =====\")\n",
        "print(\"Val Accs:\", [f\"{a*100:0.2f}%\" for a in cv_accs], f\"| MeanÂ±Std = {np.mean(cv_accs)*100:0.2f}% Â± {np.std(cv_accs)*100:0.2f}%\")\n",
        "print(\"Val F1M :\", [f\"{f:0.4f}\" for f in cv_f1s],        f\"| MeanÂ±Std = {np.mean(cv_f1s):0.4f} Â± {np.std(cv_f1s):0.4f}\")\n",
        "\n",
        "# ---------- Full-train on train/, evaluate on test/ ----------\n",
        "if os.path.isdir(test_dir):\n",
        "    full_base  = datasets.ImageFolder(root=train_dir, transform=None)\n",
        "    test_base  = datasets.ImageFolder(root=test_dir, transform=None)\n",
        "    full_ds    = AlbDataset(full_base, transform=train_aug)\n",
        "    test_ds    = AlbDataset(test_base, transform=val_aug)\n",
        "\n",
        "    full_loader = DataLoader(full_ds, batch_size=(BATCH_GPU if device.type=='cuda' else BATCH_CPU),\n",
        "                             shuffle=True, num_workers=NUM_WORKERS, pin_memory=(device.type=='cuda'),\n",
        "                             persistent_workers=(NUM_WORKERS>0 and device.type=='cuda'))\n",
        "    test_loader = DataLoader(test_ds, batch_size=(BATCH_GPU if device.type=='cuda' else BATCH_CPU),\n",
        "                             shuffle=False, num_workers=NUM_WORKERS, pin_memory=(device.type=='cuda'),\n",
        "                             persistent_workers=(NUM_WORKERS>0 and device.type=='cuda'))\n",
        "\n",
        "    model = build_swin_tiny(num_classes)\n",
        "    set_trainable(model, \"head\")\n",
        "    optimizer = make_optimizer(model)\n",
        "    scheduler = optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max=EPOCHS)\n",
        "    criterion = nn.CrossEntropyLoss()\n",
        "\n",
        "    best_loss, wait = 1e9, 0\n",
        "    best_full_path = \"swin_tiny_alb_smart_fulltrain.pt\"\n",
        "    for ep in range(1, EPOCHS+1):\n",
        "        if ep == UNFREEZE_LAYER4_EPOCH:\n",
        "            set_trainable(model, \"layer4\"); optimizer = make_optimizer(model)\n",
        "        if ep == UNFREEZE_L34_EPOCH:\n",
        "            set_trainable(model, \"layer3_4\"); optimizer = make_optimizer(model)\n",
        "\n",
        "        t0 = time.time()\n",
        "        tr_loss, tr_acc, tr_f1 = train_one_epoch(model, full_loader, optimizer, criterion)\n",
        "        scheduler.step()\n",
        "        print(f\"[FULL] Ep {ep:02d}/{EPOCHS} | loss {tr_loss:.4f} acc {tr_acc*100:5.2f}% f1M {tr_f1:.4f} | {time.time()-t0:.1f}s\")\n",
        "\n",
        "        if tr_loss < best_loss:\n",
        "            best_loss, wait = tr_loss, 0; torch.save(model.state_dict(), best_full_path)\n",
        "        else:\n",
        "            wait += 1\n",
        "            if wait >= PATIENCE:\n",
        "                print(f\"[FULL] â†’ Early stop @ {ep}\")\n",
        "                break\n",
        "\n",
        "    # Evaluate best\n",
        "    model.load_state_dict(torch.load(best_full_path, map_location=device))\n",
        "    test_loss, test_acc, test_f1 = evaluate(model, test_loader, criterion)\n",
        "    print(f\"\\nTEST â†’ loss {test_loss:.4f} | acc {test_acc*100:5.2f}% | f1M {test_f1:.4f}\")\n",
        "\n",
        "    @torch.no_grad()\n",
        "    def per_class_report():\n",
        "        model.eval()\n",
        "        y_true, y_pred = [], []\n",
        "        for xb, yb in test_loader:\n",
        "            xb = xb.to(device, non_blocking=True)\n",
        "            with torch.autocast(device_type='cuda', enabled=(device.type=='cuda')):\n",
        "                logits = model(xb)\n",
        "            y_pred.extend(logits.argmax(1).cpu().tolist())\n",
        "            y_true.extend(yb.numpy().tolist())\n",
        "        print(\"\\nPer-class report:\\n\",\n",
        "              classification_report(y_true, y_pred, target_names=test_base.classes, digits=4, zero_division=0))\n",
        "    per_class_report()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "8e8d5721-7a58-4ee4-cf02-55d05b5471d9",
        "id": "H4X-SDab70FZ"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Device: cuda\n",
            "Albumentations version: 2.0.8\n",
            "[WARN] RandomResizedCrop not usable in this Albumentations build. Using Resize+RandomCrop fallback. Error: 1 validation error for InitSchema\n",
            "size\n",
            "  Field required [type=missing, input_value={'scale': (0.85, 1.0), 'r...: None, 'strict': False}, input_type=dict]\n",
            "    For further information visit https://errors.pydantic.dev/2.11/v/missing\n",
            "Train images: 891 | Classes (27): ['Actinophrys', 'Amoeba', 'Arcella', 'Aspidisca', 'Ceratium', 'Codosiga', 'Colpoda', 'Epistylis', 'Euglena', 'Euglypha', 'Gonyaulax', 'Gymnodinium', 'Hydra', 'Keratella_quadrala', 'Noctiluca', 'Paramecium', 'Phacus', 'Rod_bacteria', 'Rotifera', 'Siprostomum', 'Spherical_bacteria', 'Spiral_bacteria', 'Stentor', 'Stylonychia', 'Synchaeta', 'Vorticella', 'Yeast']\n",
            "\n",
            "===== K-Fold 1/5 ===== (train 712 | val 179)\n",
            "Downloading: \"https://download.pytorch.org/models/swin_t-704ceda3.pth\" to /root/.cache/torch/hub/checkpoints/swin_t-704ceda3.pth\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 108M/108M [00:00<00:00, 188MB/s] \n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[Fold 1] Ep 01/60 | Train loss 3.0237 acc 16.71% f1M 0.0695 | Val loss 2.6664 acc 25.14% f1M 0.1080 | 337.6s\n",
            "[Fold 1] Ep 02/60 | Train loss 2.5278 acc 30.06% f1M 0.1568 | Val loss 2.3421 acc 30.17% f1M 0.1568 | 7.4s\n",
            "[Fold 1] Ep 03/60 | Train loss 2.1756 acc 42.28% f1M 0.2898 | Val loss 2.0848 acc 42.46% f1M 0.3217 | 8.6s\n",
            "[Fold 1] Ep 04/60 | Train loss 1.9583 acc 54.07% f1M 0.4416 | Val loss 1.8860 acc 53.07% f1M 0.4549 | 8.1s\n",
            "[Fold 1] Ep 05/60 | Train loss 1.7953 acc 59.97% f1M 0.5415 | Val loss 1.7489 acc 57.54% f1M 0.5200 | 7.5s\n",
            "[Fold 1] Ep 06/60 | Train loss 1.6633 acc 62.92% f1M 0.5888 | Val loss 1.6387 acc 61.45% f1M 0.5564 | 8.7s\n",
            "[Fold 1] Ep 07/60 | Train loss 1.0674 acc 72.47% f1M 0.7195 | Val loss 1.0836 acc 69.27% f1M 0.7409 | 7.7s\n",
            "[Fold 1] Ep 08/60 | Train loss 0.6223 acc 81.88% f1M 0.8224 | Val loss 1.0021 acc 74.86% f1M 0.7902 | 9.1s\n",
            "[Fold 1] Ep 09/60 | Train loss 0.4522 acc 89.04% f1M 0.8791 | Val loss 0.9708 acc 78.21% f1M 0.8378 | 8.3s\n",
            "[Fold 1] Ep 10/60 | Train loss 0.3713 acc 89.47% f1M 0.9075 | Val loss 0.9974 acc 78.77% f1M 0.8227 | 7.6s\n",
            "[Fold 1] Ep 11/60 | Train loss 0.2339 acc 93.54% f1M 0.9403 | Val loss 1.0546 acc 75.98% f1M 0.8199 | 9.0s\n",
            "[Fold 1] Ep 12/60 | Train loss 0.2469 acc 92.84% f1M 0.9372 | Val loss 1.1544 acc 72.63% f1M 0.7616 | 7.5s\n",
            "[Fold 1] Ep 13/60 | Train loss 0.2640 acc 90.87% f1M 0.9265 | Val loss 1.0029 acc 77.09% f1M 0.8224 | 9.0s\n",
            "[Fold 1] Ep 14/60 | Train loss 0.1707 acc 94.80% f1M 0.9619 | Val loss 1.0408 acc 74.86% f1M 0.7921 | 8.3s\n",
            "[Fold 1] Ep 15/60 | Train loss 0.1564 acc 96.49% f1M 0.9713 | Val loss 1.0904 acc 78.77% f1M 0.8183 | 7.9s\n",
            "[Fold 1] Ep 16/60 | Train loss 0.1184 acc 97.33% f1M 0.9775 | Val loss 1.1295 acc 76.54% f1M 0.8021 | 8.8s\n",
            "[Fold 1] Ep 17/60 | Train loss 0.0883 acc 97.75% f1M 0.9817 | Val loss 1.1871 acc 73.74% f1M 0.7769 | 7.7s\n",
            "[Fold 1] Ep 18/60 | Train loss 0.1636 acc 95.22% f1M 0.9594 | Val loss 1.3286 acc 72.63% f1M 0.7646 | 8.9s\n",
            "[Fold 1] Ep 19/60 | Train loss 0.1122 acc 96.49% f1M 0.9725 | Val loss 1.3626 acc 72.63% f1M 0.7602 | 8.8s\n",
            "[Fold 1] Ep 20/60 | Train loss 0.1339 acc 95.93% f1M 0.9562 | Val loss 1.3326 acc 72.63% f1M 0.7717 | 7.7s\n",
            "[Fold 1] Ep 21/60 | Train loss 0.1110 acc 96.49% f1M 0.9699 | Val loss 1.2502 acc 72.63% f1M 0.7691 | 8.9s\n",
            "[Fold 1] Ep 22/60 | Train loss 0.1173 acc 96.35% f1M 0.9638 | Val loss 1.2956 acc 73.18% f1M 0.7718 | 7.8s\n",
            "[Fold 1] Ep 23/60 | Train loss 0.0996 acc 96.77% f1M 0.9717 | Val loss 1.2136 acc 77.09% f1M 0.8085 | 8.6s\n",
            "[Fold 1] Ep 24/60 | Train loss 0.0896 acc 97.19% f1M 0.9754 | Val loss 1.3371 acc 74.86% f1M 0.8062 | 8.7s\n",
            "[Fold 1] Early stop @ 24 (best @ 9, F1M=0.8378)\n",
            "[Fold 1] Best Val â†’ acc 78.21% | f1M 0.8378 (epoch 9)\n",
            "\n",
            "===== K-Fold 2/5 ===== (train 713 | val 178)\n",
            "[Fold 2] Ep 01/60 | Train loss 3.0730 acc 13.18% f1M 0.0480 | Val loss 2.7044 acc 26.40% f1M 0.0976 | 8.4s\n",
            "[Fold 2] Ep 02/60 | Train loss 2.5866 acc 26.09% f1M 0.1102 | Val loss 2.3684 acc 35.96% f1M 0.1825 | 7.7s\n",
            "[Fold 2] Ep 03/60 | Train loss 2.2538 acc 39.27% f1M 0.2698 | Val loss 2.0976 acc 50.00% f1M 0.3544 | 8.7s\n",
            "[Fold 2] Ep 04/60 | Train loss 1.9936 acc 50.21% f1M 0.4221 | Val loss 1.9035 acc 51.12% f1M 0.3645 | 7.2s\n",
            "[Fold 2] Ep 05/60 | Train loss 1.7934 acc 56.80% f1M 0.4841 | Val loss 1.7567 acc 56.74% f1M 0.4817 | 8.7s\n",
            "[Fold 2] Ep 06/60 | Train loss 1.6060 acc 62.41% f1M 0.5794 | Val loss 1.6439 acc 57.87% f1M 0.4838 | 7.2s\n",
            "[Fold 2] Ep 07/60 | Train loss 1.1142 acc 71.67% f1M 0.7061 | Val loss 1.0049 acc 73.03% f1M 0.7470 | 8.5s\n",
            "[Fold 2] Ep 08/60 | Train loss 0.6768 acc 81.21% f1M 0.8333 | Val loss 0.7886 acc 76.97% f1M 0.7696 | 8.4s\n",
            "[Fold 2] Ep 09/60 | Train loss 0.4902 acc 88.64% f1M 0.8931 | Val loss 0.7717 acc 80.34% f1M 0.8123 | 7.4s\n",
            "[Fold 2] Ep 10/60 | Train loss 0.3214 acc 91.30% f1M 0.9149 | Val loss 0.8377 acc 79.21% f1M 0.7897 | 8.8s\n",
            "[Fold 2] Ep 11/60 | Train loss 0.2780 acc 92.43% f1M 0.9449 | Val loss 0.8873 acc 76.40% f1M 0.7780 | 7.4s\n",
            "[Fold 2] Ep 12/60 | Train loss 0.2046 acc 94.81% f1M 0.9525 | Val loss 0.8212 acc 80.34% f1M 0.8212 | 8.5s\n",
            "[Fold 2] Ep 13/60 | Train loss 0.1789 acc 94.81% f1M 0.9531 | Val loss 0.8856 acc 77.53% f1M 0.7660 | 8.2s\n",
            "[Fold 2] Ep 14/60 | Train loss 0.1163 acc 96.77% f1M 0.9762 | Val loss 0.8383 acc 80.90% f1M 0.7917 | 7.6s\n",
            "[Fold 2] Ep 15/60 | Train loss 0.1302 acc 96.07% f1M 0.9668 | Val loss 0.8489 acc 78.65% f1M 0.7581 | 10.2s\n",
            "[Fold 2] Ep 16/60 | Train loss 0.0963 acc 97.90% f1M 0.9867 | Val loss 0.9157 acc 78.65% f1M 0.7598 | 7.2s\n",
            "[Fold 2] Ep 17/60 | Train loss 0.1004 acc 97.34% f1M 0.9714 | Val loss 0.9868 acc 79.78% f1M 0.7905 | 8.9s\n",
            "[Fold 2] Ep 18/60 | Train loss 0.1430 acc 95.37% f1M 0.9623 | Val loss 1.0552 acc 80.34% f1M 0.7736 | 8.0s\n",
            "[Fold 2] Ep 19/60 | Train loss 0.1430 acc 96.35% f1M 0.9647 | Val loss 1.0514 acc 80.34% f1M 0.8214 | 7.6s\n",
            "[Fold 2] Ep 20/60 | Train loss 0.1496 acc 96.91% f1M 0.9708 | Val loss 0.9558 acc 79.78% f1M 0.8036 | 8.8s\n",
            "[Fold 2] Ep 21/60 | Train loss 0.2070 acc 95.51% f1M 0.9588 | Val loss 1.1466 acc 77.53% f1M 0.7833 | 7.6s\n",
            "[Fold 2] Ep 22/60 | Train loss 0.1223 acc 96.07% f1M 0.9712 | Val loss 1.1125 acc 78.09% f1M 0.7764 | 8.9s\n",
            "[Fold 2] Ep 23/60 | Train loss 0.0992 acc 96.77% f1M 0.9713 | Val loss 1.2579 acc 74.72% f1M 0.7321 | 8.2s\n",
            "[Fold 2] Ep 24/60 | Train loss 0.0674 acc 97.76% f1M 0.9843 | Val loss 1.0984 acc 78.09% f1M 0.7625 | 7.4s\n",
            "[Fold 2] Ep 25/60 | Train loss 0.0942 acc 97.62% f1M 0.9825 | Val loss 1.1066 acc 77.53% f1M 0.7639 | 8.8s\n",
            "[Fold 2] Ep 26/60 | Train loss 0.0788 acc 97.62% f1M 0.9827 | Val loss 1.0208 acc 79.21% f1M 0.8051 | 7.5s\n",
            "[Fold 2] Ep 27/60 | Train loss 0.0761 acc 97.90% f1M 0.9805 | Val loss 1.0675 acc 79.21% f1M 0.8011 | 8.8s\n",
            "[Fold 2] Ep 28/60 | Train loss 0.0876 acc 97.34% f1M 0.9727 | Val loss 1.2953 acc 78.09% f1M 0.7604 | 7.9s\n",
            "[Fold 2] Ep 29/60 | Train loss 0.0879 acc 96.63% f1M 0.9738 | Val loss 1.2110 acc 77.53% f1M 0.7814 | 7.8s\n",
            "[Fold 2] Ep 30/60 | Train loss 0.0680 acc 96.91% f1M 0.9780 | Val loss 1.0472 acc 83.15% f1M 0.8394 | 8.3s\n",
            "[Fold 2] Ep 31/60 | Train loss 0.0530 acc 98.88% f1M 0.9920 | Val loss 1.1141 acc 81.46% f1M 0.8265 | 7.5s\n",
            "[Fold 2] Ep 32/60 | Train loss 0.0956 acc 98.04% f1M 0.9859 | Val loss 1.1111 acc 83.15% f1M 0.8339 | 8.8s\n",
            "[Fold 2] Ep 33/60 | Train loss 0.0925 acc 97.34% f1M 0.9787 | Val loss 1.1906 acc 81.46% f1M 0.8157 | 7.4s\n",
            "[Fold 2] Ep 34/60 | Train loss 0.0881 acc 97.62% f1M 0.9841 | Val loss 1.0242 acc 78.65% f1M 0.7864 | 8.4s\n",
            "[Fold 2] Ep 35/60 | Train loss 0.0873 acc 96.63% f1M 0.9769 | Val loss 1.1494 acc 76.97% f1M 0.7583 | 8.5s\n",
            "[Fold 2] Ep 36/60 | Train loss 0.0625 acc 97.76% f1M 0.9787 | Val loss 1.2762 acc 78.09% f1M 0.7760 | 7.6s\n",
            "[Fold 2] Ep 37/60 | Train loss 0.0681 acc 97.62% f1M 0.9807 | Val loss 1.1113 acc 76.97% f1M 0.7666 | 8.8s\n",
            "[Fold 2] Ep 38/60 | Train loss 0.0953 acc 97.05% f1M 0.9740 | Val loss 1.0091 acc 79.78% f1M 0.8153 | 7.6s\n",
            "[Fold 2] Ep 39/60 | Train loss 0.0848 acc 97.05% f1M 0.9726 | Val loss 0.8991 acc 82.02% f1M 0.8276 | 8.7s\n",
            "[Fold 2] Ep 40/60 | Train loss 0.0772 acc 98.18% f1M 0.9772 | Val loss 0.9097 acc 80.90% f1M 0.8021 | 8.4s\n",
            "[Fold 2] Ep 41/60 | Train loss 0.0830 acc 97.48% f1M 0.9792 | Val loss 1.0942 acc 78.09% f1M 0.7581 | 7.5s\n",
            "[Fold 2] Ep 42/60 | Train loss 0.1031 acc 97.19% f1M 0.9712 | Val loss 1.0336 acc 78.09% f1M 0.7879 | 9.1s\n",
            "[Fold 2] Ep 43/60 | Train loss 0.0543 acc 98.46% f1M 0.9902 | Val loss 1.0438 acc 79.78% f1M 0.8122 | 7.7s\n",
            "[Fold 2] Ep 44/60 | Train loss 0.0568 acc 98.60% f1M 0.9882 | Val loss 1.1941 acc 78.65% f1M 0.7987 | 8.7s\n",
            "[Fold 2] Ep 45/60 | Train loss 0.0510 acc 98.60% f1M 0.9896 | Val loss 1.1976 acc 81.46% f1M 0.8173 | 8.4s\n",
            "[Fold 2] Early stop @ 45 (best @ 30, F1M=0.8394)\n",
            "[Fold 2] Best Val â†’ acc 83.15% | f1M 0.8394 (epoch 30)\n",
            "\n",
            "===== K-Fold 3/5 ===== (train 713 | val 178)\n",
            "[Fold 3] Ep 01/60 | Train loss 3.0310 acc 16.83% f1M 0.0529 | Val loss 2.7201 acc 26.40% f1M 0.1050 | 8.4s\n",
            "[Fold 3] Ep 02/60 | Train loss 2.5510 acc 29.59% f1M 0.1257 | Val loss 2.4054 acc 31.46% f1M 0.1495 | 8.4s\n",
            "[Fold 3] Ep 03/60 | Train loss 2.1985 acc 42.64% f1M 0.3076 | Val loss 2.1494 acc 41.01% f1M 0.2807 | 8.5s\n",
            "[Fold 3] Ep 04/60 | Train loss 1.9574 acc 52.88% f1M 0.4575 | Val loss 1.9733 acc 48.88% f1M 0.3805 | 7.2s\n",
            "[Fold 3] Ep 05/60 | Train loss 1.7981 acc 58.49% f1M 0.5232 | Val loss 1.8474 acc 51.69% f1M 0.4183 | 8.5s\n",
            "[Fold 3] Ep 06/60 | Train loss 1.5687 acc 62.97% f1M 0.5890 | Val loss 1.7520 acc 53.37% f1M 0.4556 | 7.3s\n",
            "[Fold 3] Ep 07/60 | Train loss 1.0703 acc 69.14% f1M 0.7038 | Val loss 1.0379 acc 70.79% f1M 0.7137 | 8.9s\n",
            "[Fold 3] Ep 08/60 | Train loss 0.6256 acc 82.19% f1M 0.8176 | Val loss 1.0089 acc 66.29% f1M 0.6331 | 8.3s\n",
            "[Fold 3] Ep 09/60 | Train loss 0.4179 acc 89.20% f1M 0.9024 | Val loss 0.8756 acc 74.72% f1M 0.7214 | 9.0s\n",
            "[Fold 3] Ep 10/60 | Train loss 0.2792 acc 92.85% f1M 0.9365 | Val loss 0.9560 acc 72.47% f1M 0.7159 | 9.0s\n",
            "[Fold 3] Ep 11/60 | Train loss 0.2912 acc 92.71% f1M 0.9443 | Val loss 0.9965 acc 73.60% f1M 0.7263 | 7.5s\n",
            "[Fold 3] Ep 12/60 | Train loss 0.2222 acc 93.55% f1M 0.9521 | Val loss 1.0503 acc 74.16% f1M 0.7243 | 8.3s\n",
            "[Fold 3] Ep 13/60 | Train loss 0.2022 acc 93.55% f1M 0.9450 | Val loss 1.0791 acc 74.72% f1M 0.7398 | 8.5s\n",
            "[Fold 3] Ep 14/60 | Train loss 0.1570 acc 96.21% f1M 0.9700 | Val loss 1.0984 acc 74.72% f1M 0.7454 | 7.4s\n",
            "[Fold 3] Ep 15/60 | Train loss 0.1304 acc 96.07% f1M 0.9689 | Val loss 1.1223 acc 72.47% f1M 0.7133 | 8.9s\n",
            "[Fold 3] Ep 16/60 | Train loss 0.1271 acc 96.35% f1M 0.9630 | Val loss 1.1955 acc 73.03% f1M 0.7400 | 7.7s\n",
            "[Fold 3] Ep 17/60 | Train loss 0.1159 acc 96.35% f1M 0.9646 | Val loss 1.1556 acc 74.72% f1M 0.7297 | 8.4s\n",
            "[Fold 3] Ep 18/60 | Train loss 0.1162 acc 96.63% f1M 0.9684 | Val loss 1.3804 acc 69.66% f1M 0.6863 | 8.5s\n",
            "[Fold 3] Ep 19/60 | Train loss 0.1465 acc 95.23% f1M 0.9638 | Val loss 1.0757 acc 75.84% f1M 0.7548 | 7.6s\n",
            "[Fold 3] Ep 20/60 | Train loss 0.1064 acc 96.49% f1M 0.9715 | Val loss 1.1331 acc 77.53% f1M 0.7785 | 9.1s\n",
            "[Fold 3] Ep 21/60 | Train loss 0.1175 acc 97.34% f1M 0.9738 | Val loss 1.3441 acc 75.84% f1M 0.7919 | 7.8s\n",
            "[Fold 3] Ep 22/60 | Train loss 0.1439 acc 96.91% f1M 0.9739 | Val loss 1.2245 acc 75.84% f1M 0.7609 | 8.4s\n",
            "[Fold 3] Ep 23/60 | Train loss 0.1450 acc 95.93% f1M 0.9538 | Val loss 1.3354 acc 75.28% f1M 0.7262 | 8.6s\n",
            "[Fold 3] Ep 24/60 | Train loss 0.1126 acc 96.63% f1M 0.9694 | Val loss 1.3177 acc 73.03% f1M 0.6954 | 7.6s\n",
            "[Fold 3] Ep 25/60 | Train loss 0.0821 acc 98.18% f1M 0.9804 | Val loss 1.2497 acc 77.53% f1M 0.7849 | 9.0s\n",
            "[Fold 3] Ep 26/60 | Train loss 0.0931 acc 97.05% f1M 0.9717 | Val loss 1.4052 acc 77.53% f1M 0.7784 | 8.0s\n",
            "[Fold 3] Ep 27/60 | Train loss 0.0654 acc 98.18% f1M 0.9873 | Val loss 1.3909 acc 73.60% f1M 0.7554 | 8.1s\n",
            "[Fold 3] Ep 28/60 | Train loss 0.0638 acc 98.60% f1M 0.9872 | Val loss 1.4105 acc 72.47% f1M 0.7385 | 8.6s\n",
            "[Fold 3] Ep 29/60 | Train loss 0.0668 acc 97.62% f1M 0.9787 | Val loss 1.5413 acc 73.60% f1M 0.7341 | 7.6s\n",
            "[Fold 3] Ep 30/60 | Train loss 0.1004 acc 97.05% f1M 0.9750 | Val loss 1.3430 acc 76.40% f1M 0.7440 | 8.8s\n",
            "[Fold 3] Ep 31/60 | Train loss 0.0777 acc 98.04% f1M 0.9878 | Val loss 1.6617 acc 70.79% f1M 0.6961 | 7.9s\n",
            "[Fold 3] Ep 32/60 | Train loss 0.0755 acc 97.62% f1M 0.9835 | Val loss 1.3507 acc 73.60% f1M 0.7215 | 8.3s\n",
            "[Fold 3] Ep 33/60 | Train loss 0.0723 acc 97.90% f1M 0.9850 | Val loss 1.3997 acc 71.91% f1M 0.7036 | 8.5s\n",
            "[Fold 3] Ep 34/60 | Train loss 0.0469 acc 98.60% f1M 0.9914 | Val loss 1.5780 acc 73.60% f1M 0.7170 | 7.7s\n",
            "[Fold 3] Ep 35/60 | Train loss 0.0862 acc 97.90% f1M 0.9861 | Val loss 1.4945 acc 74.16% f1M 0.7258 | 8.9s\n",
            "[Fold 3] Ep 36/60 | Train loss 0.0759 acc 96.77% f1M 0.9820 | Val loss 1.4689 acc 76.40% f1M 0.7635 | 7.4s\n",
            "[Fold 3] Early stop @ 36 (best @ 21, F1M=0.7919)\n",
            "[Fold 3] Best Val â†’ acc 75.84% | f1M 0.7919 (epoch 21)\n",
            "\n",
            "===== K-Fold 4/5 ===== (train 713 | val 178)\n",
            "[Fold 4] Ep 01/60 | Train loss 3.0278 acc 15.01% f1M 0.0740 | Val loss 2.7269 acc 21.35% f1M 0.0856 | 7.9s\n",
            "[Fold 4] Ep 02/60 | Train loss 2.5881 acc 25.53% f1M 0.1361 | Val loss 2.3995 acc 29.21% f1M 0.1675 | 8.9s\n",
            "[Fold 4] Ep 03/60 | Train loss 2.2028 acc 39.27% f1M 0.2827 | Val loss 2.1127 acc 41.01% f1M 0.3189 | 7.3s\n",
            "[Fold 4] Ep 04/60 | Train loss 2.0042 acc 51.05% f1M 0.4096 | Val loss 1.9090 acc 47.19% f1M 0.3900 | 8.3s\n",
            "[Fold 4] Ep 05/60 | Train loss 1.7697 acc 57.08% f1M 0.5124 | Val loss 1.7500 acc 51.69% f1M 0.4588 | 7.8s\n",
            "[Fold 4] Ep 06/60 | Train loss 1.6461 acc 61.01% f1M 0.5648 | Val loss 1.6244 acc 56.18% f1M 0.4942 | 7.4s\n",
            "[Fold 4] Ep 07/60 | Train loss 1.0480 acc 68.58% f1M 0.6739 | Val loss 0.8199 acc 75.84% f1M 0.7628 | 8.7s\n",
            "[Fold 4] Ep 08/60 | Train loss 0.6317 acc 81.91% f1M 0.8345 | Val loss 0.6941 acc 76.97% f1M 0.7800 | 7.4s\n",
            "[Fold 4] Ep 09/60 | Train loss 0.4369 acc 88.78% f1M 0.8948 | Val loss 0.7185 acc 78.65% f1M 0.8102 | 8.9s\n",
            "[Fold 4] Ep 10/60 | Train loss 0.3182 acc 91.30% f1M 0.9162 | Val loss 0.6725 acc 79.21% f1M 0.8036 | 8.1s\n",
            "[Fold 4] Ep 11/60 | Train loss 0.2646 acc 92.43% f1M 0.9294 | Val loss 0.7548 acc 79.78% f1M 0.8202 | 8.0s\n",
            "[Fold 4] Ep 12/60 | Train loss 0.1850 acc 94.53% f1M 0.9436 | Val loss 0.7297 acc 79.78% f1M 0.8330 | 9.3s\n",
            "[Fold 4] Ep 13/60 | Train loss 0.1930 acc 96.21% f1M 0.9749 | Val loss 0.8031 acc 79.21% f1M 0.8236 | 7.6s\n",
            "[Fold 4] Ep 14/60 | Train loss 0.1493 acc 95.65% f1M 0.9638 | Val loss 0.8067 acc 78.65% f1M 0.8228 | 8.8s\n",
            "[Fold 4] Ep 15/60 | Train loss 0.1342 acc 97.05% f1M 0.9753 | Val loss 0.8359 acc 77.53% f1M 0.7984 | 8.1s\n",
            "[Fold 4] Ep 16/60 | Train loss 0.1327 acc 96.63% f1M 0.9702 | Val loss 0.8411 acc 79.78% f1M 0.8338 | 7.7s\n",
            "[Fold 4] Ep 17/60 | Train loss 0.1080 acc 97.48% f1M 0.9786 | Val loss 0.9221 acc 74.72% f1M 0.7810 | 9.0s\n",
            "[Fold 4] Ep 18/60 | Train loss 0.1695 acc 97.05% f1M 0.9707 | Val loss 0.9514 acc 75.84% f1M 0.8004 | 7.6s\n",
            "[Fold 4] Ep 19/60 | Train loss 0.1207 acc 96.63% f1M 0.9739 | Val loss 0.9130 acc 79.21% f1M 0.8144 | 8.9s\n",
            "[Fold 4] Ep 20/60 | Train loss 0.2145 acc 95.93% f1M 0.9635 | Val loss 0.8341 acc 79.21% f1M 0.8134 | 8.0s\n",
            "[Fold 4] Ep 21/60 | Train loss 0.1414 acc 96.21% f1M 0.9650 | Val loss 0.9404 acc 76.97% f1M 0.7863 | 8.2s\n",
            "[Fold 4] Ep 22/60 | Train loss 0.1480 acc 95.09% f1M 0.9632 | Val loss 0.9766 acc 79.21% f1M 0.8049 | 9.1s\n",
            "[Fold 4] Ep 23/60 | Train loss 0.1287 acc 96.63% f1M 0.9744 | Val loss 0.8358 acc 78.09% f1M 0.7985 | 7.8s\n",
            "[Fold 4] Ep 24/60 | Train loss 0.1160 acc 96.77% f1M 0.9731 | Val loss 0.8799 acc 78.09% f1M 0.8059 | 9.0s\n",
            "[Fold 4] Ep 25/60 | Train loss 0.0740 acc 98.18% f1M 0.9840 | Val loss 0.8381 acc 79.78% f1M 0.8185 | 8.2s\n",
            "[Fold 4] Ep 26/60 | Train loss 0.0824 acc 97.76% f1M 0.9818 | Val loss 0.8835 acc 78.09% f1M 0.8048 | 8.0s\n",
            "[Fold 4] Ep 27/60 | Train loss 0.0615 acc 97.76% f1M 0.9754 | Val loss 0.9390 acc 79.21% f1M 0.8100 | 8.7s\n",
            "[Fold 4] Ep 28/60 | Train loss 0.0883 acc 98.04% f1M 0.9832 | Val loss 0.8129 acc 78.65% f1M 0.8224 | 7.8s\n",
            "[Fold 4] Ep 29/60 | Train loss 0.0720 acc 97.90% f1M 0.9832 | Val loss 0.9065 acc 79.78% f1M 0.8350 | 9.2s\n",
            "[Fold 4] Ep 30/60 | Train loss 0.0620 acc 98.04% f1M 0.9826 | Val loss 0.9157 acc 79.78% f1M 0.8375 | 8.3s\n",
            "[Fold 4] Ep 31/60 | Train loss 0.0578 acc 98.32% f1M 0.9863 | Val loss 0.9267 acc 78.65% f1M 0.8135 | 7.8s\n",
            "[Fold 4] Ep 32/60 | Train loss 0.0678 acc 97.90% f1M 0.9848 | Val loss 0.9836 acc 76.97% f1M 0.8016 | 8.9s\n",
            "[Fold 4] Ep 33/60 | Train loss 0.0621 acc 97.62% f1M 0.9834 | Val loss 0.9433 acc 78.65% f1M 0.8142 | 7.7s\n",
            "[Fold 4] Ep 34/60 | Train loss 0.0594 acc 98.32% f1M 0.9884 | Val loss 0.8596 acc 78.09% f1M 0.8122 | 8.8s\n",
            "[Fold 4] Ep 35/60 | Train loss 0.0592 acc 97.62% f1M 0.9820 | Val loss 0.9640 acc 79.21% f1M 0.8185 | 8.6s\n",
            "[Fold 4] Ep 36/60 | Train loss 0.0576 acc 98.18% f1M 0.9870 | Val loss 0.8527 acc 80.34% f1M 0.8173 | 7.5s\n",
            "[Fold 4] Ep 37/60 | Train loss 0.0537 acc 98.60% f1M 0.9906 | Val loss 0.8504 acc 78.65% f1M 0.8160 | 8.7s\n",
            "[Fold 4] Ep 38/60 | Train loss 0.0621 acc 97.90% f1M 0.9823 | Val loss 0.8176 acc 78.09% f1M 0.7986 | 7.7s\n",
            "[Fold 4] Ep 39/60 | Train loss 0.0570 acc 98.60% f1M 0.9858 | Val loss 0.8444 acc 80.34% f1M 0.8285 | 9.0s\n",
            "[Fold 4] Ep 40/60 | Train loss 0.0542 acc 98.18% f1M 0.9814 | Val loss 0.8187 acc 80.90% f1M 0.8197 | 8.2s\n",
            "[Fold 4] Ep 41/60 | Train loss 0.0431 acc 98.60% f1M 0.9887 | Val loss 0.8934 acc 79.21% f1M 0.8039 | 7.6s\n",
            "[Fold 4] Ep 42/60 | Train loss 0.0608 acc 98.04% f1M 0.9826 | Val loss 0.8934 acc 81.46% f1M 0.8089 | 8.8s\n",
            "[Fold 4] Ep 43/60 | Train loss 0.0403 acc 98.88% f1M 0.9901 | Val loss 1.0356 acc 78.09% f1M 0.8009 | 7.7s\n",
            "[Fold 4] Ep 44/60 | Train loss 0.0689 acc 97.05% f1M 0.9770 | Val loss 1.1675 acc 76.97% f1M 0.7857 | 8.9s\n",
            "[Fold 4] Ep 45/60 | Train loss 0.0723 acc 97.90% f1M 0.9816 | Val loss 1.0511 acc 78.65% f1M 0.8031 | 8.1s\n",
            "[Fold 4] Early stop @ 45 (best @ 30, F1M=0.8375)\n",
            "[Fold 4] Best Val â†’ acc 79.78% | f1M 0.8375 (epoch 30)\n",
            "\n",
            "===== K-Fold 5/5 ===== (train 713 | val 178)\n",
            "[Fold 5] Ep 01/60 | Train loss 3.0013 acc 16.27% f1M 0.0662 | Val loss 2.7024 acc 21.35% f1M 0.0973 | 8.0s\n",
            "[Fold 5] Ep 02/60 | Train loss 2.5776 acc 28.89% f1M 0.1545 | Val loss 2.3853 acc 30.34% f1M 0.1749 | 8.7s\n",
            "[Fold 5] Ep 03/60 | Train loss 2.1799 acc 42.08% f1M 0.3018 | Val loss 2.1356 acc 43.82% f1M 0.3278 | 7.8s\n",
            "[Fold 5] Ep 04/60 | Train loss 1.9689 acc 52.73% f1M 0.4126 | Val loss 1.9572 acc 46.63% f1M 0.3909 | 7.4s\n",
            "[Fold 5] Ep 05/60 | Train loss 1.7809 acc 59.19% f1M 0.5141 | Val loss 1.8235 acc 55.06% f1M 0.5144 | 8.6s\n",
            "[Fold 5] Ep 06/60 | Train loss 1.5789 acc 63.11% f1M 0.5743 | Val loss 1.7146 acc 60.11% f1M 0.5849 | 7.3s\n",
            "[Fold 5] Ep 07/60 | Train loss 1.1112 acc 69.57% f1M 0.6955 | Val loss 1.0393 acc 71.91% f1M 0.7276 | 10.6s\n",
            "[Fold 5] Ep 08/60 | Train loss 0.6053 acc 84.85% f1M 0.8545 | Val loss 0.9637 acc 78.09% f1M 0.7999 | 7.7s\n",
            "[Fold 5] Ep 09/60 | Train loss 0.4610 acc 84.57% f1M 0.8522 | Val loss 0.9907 acc 76.97% f1M 0.7736 | 8.5s\n",
            "[Fold 5] Ep 10/60 | Train loss 0.3027 acc 91.87% f1M 0.9278 | Val loss 0.9555 acc 79.21% f1M 0.8124 | 8.8s\n",
            "[Fold 5] Ep 11/60 | Train loss 0.2187 acc 94.53% f1M 0.9599 | Val loss 1.0394 acc 79.78% f1M 0.8347 | 7.4s\n",
            "[Fold 5] Ep 12/60 | Train loss 0.1836 acc 95.09% f1M 0.9592 | Val loss 0.9473 acc 80.34% f1M 0.8044 | 8.9s\n",
            "[Fold 5] Ep 13/60 | Train loss 0.1807 acc 94.95% f1M 0.9593 | Val loss 1.1374 acc 79.21% f1M 0.8263 | 7.9s\n",
            "[Fold 5] Ep 14/60 | Train loss 0.1433 acc 95.93% f1M 0.9627 | Val loss 1.0254 acc 82.58% f1M 0.8644 | 8.2s\n",
            "[Fold 5] Ep 15/60 | Train loss 0.2000 acc 95.23% f1M 0.9552 | Val loss 1.0615 acc 81.46% f1M 0.8460 | 8.6s\n",
            "[Fold 5] Ep 16/60 | Train loss 0.1380 acc 96.49% f1M 0.9689 | Val loss 1.1062 acc 77.53% f1M 0.7998 | 7.2s\n",
            "[Fold 5] Ep 17/60 | Train loss 0.1632 acc 95.65% f1M 0.9676 | Val loss 1.1200 acc 80.90% f1M 0.8390 | 8.8s\n",
            "[Fold 5] Ep 18/60 | Train loss 0.1940 acc 92.71% f1M 0.9315 | Val loss 1.1405 acc 79.21% f1M 0.8339 | 7.6s\n",
            "[Fold 5] Ep 19/60 | Train loss 0.1399 acc 95.93% f1M 0.9646 | Val loss 1.0629 acc 76.40% f1M 0.7812 | 8.6s\n",
            "[Fold 5] Ep 20/60 | Train loss 0.1131 acc 96.91% f1M 0.9702 | Val loss 1.1613 acc 78.65% f1M 0.8240 | 8.4s\n",
            "[Fold 5] Ep 21/60 | Train loss 0.0786 acc 98.18% f1M 0.9842 | Val loss 1.3031 acc 78.65% f1M 0.8187 | 7.5s\n",
            "[Fold 5] Ep 22/60 | Train loss 0.1011 acc 97.34% f1M 0.9742 | Val loss 1.0782 acc 80.90% f1M 0.8477 | 8.8s\n",
            "[Fold 5] Ep 23/60 | Train loss 0.0671 acc 97.34% f1M 0.9788 | Val loss 1.4136 acc 78.09% f1M 0.8235 | 7.6s\n",
            "[Fold 5] Ep 24/60 | Train loss 0.1185 acc 96.63% f1M 0.9740 | Val loss 1.3578 acc 76.97% f1M 0.8095 | 9.2s\n",
            "[Fold 5] Ep 25/60 | Train loss 0.1017 acc 97.90% f1M 0.9818 | Val loss 1.2201 acc 79.78% f1M 0.8357 | 9.0s\n",
            "[Fold 5] Ep 26/60 | Train loss 0.1460 acc 97.34% f1M 0.9797 | Val loss 0.9730 acc 83.15% f1M 0.8645 | 7.8s\n",
            "[Fold 5] Ep 27/60 | Train loss 0.0999 acc 96.91% f1M 0.9756 | Val loss 1.0343 acc 82.58% f1M 0.8664 | 9.1s\n",
            "[Fold 5] Ep 28/60 | Train loss 0.0920 acc 97.76% f1M 0.9834 | Val loss 1.0690 acc 80.90% f1M 0.8469 | 8.3s\n",
            "[Fold 5] Ep 29/60 | Train loss 0.0814 acc 97.90% f1M 0.9845 | Val loss 1.0284 acc 83.15% f1M 0.8676 | 8.1s\n",
            "[Fold 5] Ep 30/60 | Train loss 0.0860 acc 97.19% f1M 0.9736 | Val loss 1.2441 acc 78.09% f1M 0.8188 | 8.9s\n",
            "[Fold 5] Ep 31/60 | Train loss 0.1323 acc 96.49% f1M 0.9725 | Val loss 1.3718 acc 78.65% f1M 0.8343 | 7.8s\n",
            "[Fold 5] Ep 32/60 | Train loss 0.0596 acc 98.04% f1M 0.9825 | Val loss 1.1381 acc 80.34% f1M 0.8357 | 9.1s\n",
            "[Fold 5] Ep 33/60 | Train loss 0.0635 acc 98.04% f1M 0.9874 | Val loss 1.1660 acc 80.34% f1M 0.8456 | 8.4s\n",
            "[Fold 5] Ep 34/60 | Train loss 0.0505 acc 98.74% f1M 0.9916 | Val loss 1.1109 acc 82.58% f1M 0.8610 | 7.8s\n",
            "[Fold 5] Ep 35/60 | Train loss 0.0745 acc 97.48% f1M 0.9823 | Val loss 1.3062 acc 82.02% f1M 0.8649 | 9.2s\n",
            "[Fold 5] Ep 36/60 | Train loss 0.0482 acc 98.60% f1M 0.9919 | Val loss 1.5450 acc 78.65% f1M 0.8365 | 7.9s\n",
            "[Fold 5] Ep 37/60 | Train loss 0.0944 acc 96.77% f1M 0.9763 | Val loss 1.3678 acc 77.53% f1M 0.8215 | 8.9s\n",
            "[Fold 5] Ep 38/60 | Train loss 0.0950 acc 97.34% f1M 0.9778 | Val loss 1.2560 acc 78.65% f1M 0.8189 | 8.4s\n",
            "[Fold 5] Ep 39/60 | Train loss 0.0892 acc 97.19% f1M 0.9703 | Val loss 1.2929 acc 77.53% f1M 0.8091 | 7.7s\n",
            "[Fold 5] Ep 40/60 | Train loss 0.0613 acc 98.04% f1M 0.9859 | Val loss 1.3935 acc 79.78% f1M 0.8191 | 8.8s\n",
            "[Fold 5] Ep 41/60 | Train loss 0.0541 acc 98.32% f1M 0.9868 | Val loss 1.4240 acc 78.09% f1M 0.8130 | 7.6s\n",
            "[Fold 5] Ep 42/60 | Train loss 0.0545 acc 98.32% f1M 0.9860 | Val loss 1.1994 acc 79.21% f1M 0.8382 | 8.9s\n",
            "[Fold 5] Ep 43/60 | Train loss 0.0357 acc 98.88% f1M 0.9888 | Val loss 1.2886 acc 80.90% f1M 0.8390 | 8.6s\n",
            "[Fold 5] Ep 44/60 | Train loss 0.0344 acc 99.02% f1M 0.9923 | Val loss 1.3458 acc 80.34% f1M 0.8272 | 7.6s\n",
            "[Fold 5] Early stop @ 44 (best @ 29, F1M=0.8676)\n",
            "[Fold 5] Best Val â†’ acc 83.15% | f1M 0.8676 (epoch 29)\n",
            "\n",
            "===== CV Summary (best epoch per fold) =====\n",
            "Val Accs: ['78.21%', '83.15%', '75.84%', '79.78%', '83.15%'] | MeanÂ±Std = 80.02% Â± 2.84%\n",
            "Val F1M : ['0.8378', '0.8394', '0.7919', '0.8375', '0.8676'] | MeanÂ±Std = 0.8348 Â± 0.0243\n",
            "[FULL] Ep 01/60 | loss 3.0354 acc 13.58% f1M 0.0438 | 8.1s\n",
            "[FULL] Ep 02/60 | loss 2.4834 acc 30.42% f1M 0.1717 | 9.6s\n",
            "[FULL] Ep 03/60 | loss 2.1185 acc 47.81% f1M 0.3879 | 9.0s\n",
            "[FULL] Ep 04/60 | loss 1.8276 acc 58.02% f1M 0.4947 | 7.7s\n",
            "[FULL] Ep 05/60 | loss 1.6215 acc 62.96% f1M 0.5748 | 8.7s\n",
            "[FULL] Ep 06/60 | loss 1.4828 acc 67.00% f1M 0.6315 | 9.5s\n",
            "[FULL] Ep 07/60 | loss 1.0403 acc 71.04% f1M 0.7162 | 8.1s\n",
            "[FULL] Ep 08/60 | loss 0.5951 acc 83.05% f1M 0.8325 | 9.1s\n",
            "[FULL] Ep 09/60 | loss 0.3949 acc 88.66% f1M 0.9019 | 9.0s\n",
            "[FULL] Ep 10/60 | loss 0.3010 acc 92.03% f1M 0.9304 | 8.1s\n",
            "[FULL] Ep 11/60 | loss 0.2480 acc 94.16% f1M 0.9471 | 9.2s\n",
            "[FULL] Ep 12/60 | loss 0.1923 acc 94.39% f1M 0.9550 | 9.1s\n",
            "[FULL] Ep 13/60 | loss 0.1861 acc 94.73% f1M 0.9484 | 8.0s\n",
            "[FULL] Ep 14/60 | loss 0.1348 acc 96.41% f1M 0.9741 | 9.2s\n",
            "[FULL] Ep 15/60 | loss 0.1359 acc 96.30% f1M 0.9698 | 9.5s\n",
            "[FULL] Ep 16/60 | loss 0.0977 acc 97.53% f1M 0.9779 | 7.7s\n",
            "[FULL] Ep 17/60 | loss 0.1120 acc 96.75% f1M 0.9703 | 9.5s\n",
            "[FULL] Ep 18/60 | loss 0.1611 acc 95.06% f1M 0.9505 | 7.7s\n",
            "[FULL] Ep 19/60 | loss 0.1402 acc 95.85% f1M 0.9636 | 8.6s\n",
            "[FULL] Ep 20/60 | loss 0.1201 acc 96.97% f1M 0.9719 | 9.2s\n",
            "[FULL] Ep 21/60 | loss 0.1440 acc 95.40% f1M 0.9584 | 7.7s\n",
            "[FULL] Ep 22/60 | loss 0.0944 acc 97.53% f1M 0.9764 | 9.0s\n",
            "[FULL] Ep 23/60 | loss 0.1071 acc 96.97% f1M 0.9725 | 9.0s\n",
            "[FULL] Ep 24/60 | loss 0.0804 acc 98.20% f1M 0.9833 | 7.8s\n",
            "[FULL] Ep 25/60 | loss 0.1050 acc 96.75% f1M 0.9717 | 9.5s\n",
            "[FULL] Ep 26/60 | loss 0.0795 acc 97.42% f1M 0.9762 | 7.8s\n",
            "[FULL] Ep 27/60 | loss 0.0768 acc 97.42% f1M 0.9741 | 8.6s\n",
            "[FULL] Ep 28/60 | loss 0.0905 acc 97.64% f1M 0.9750 | 9.2s\n",
            "[FULL] Ep 29/60 | loss 0.0822 acc 97.64% f1M 0.9830 | 7.8s\n",
            "[FULL] Ep 30/60 | loss 0.0679 acc 97.87% f1M 0.9785 | 8.7s\n",
            "[FULL] Ep 31/60 | loss 0.0355 acc 98.65% f1M 0.9923 | 9.2s\n",
            "[FULL] Ep 32/60 | loss 0.0812 acc 97.64% f1M 0.9774 | 8.0s\n",
            "[FULL] Ep 33/60 | loss 0.0634 acc 98.54% f1M 0.9891 | 8.9s\n",
            "[FULL] Ep 34/60 | loss 0.0731 acc 97.53% f1M 0.9786 | 8.3s\n",
            "[FULL] Ep 35/60 | loss 0.0652 acc 98.09% f1M 0.9824 | 8.1s\n",
            "[FULL] Ep 36/60 | loss 0.0646 acc 98.20% f1M 0.9849 | 9.2s\n",
            "[FULL] Ep 37/60 | loss 0.0746 acc 97.31% f1M 0.9754 | 7.7s\n",
            "[FULL] Ep 38/60 | loss 0.0417 acc 98.88% f1M 0.9923 | 9.2s\n",
            "[FULL] Ep 39/60 | loss 0.0565 acc 98.32% f1M 0.9837 | 8.4s\n",
            "[FULL] Ep 40/60 | loss 0.0771 acc 97.87% f1M 0.9802 | 8.4s\n",
            "[FULL] Ep 41/60 | loss 0.0774 acc 97.87% f1M 0.9820 | 9.0s\n",
            "[FULL] Ep 42/60 | loss 0.0511 acc 98.09% f1M 0.9779 | 7.6s\n",
            "[FULL] Ep 43/60 | loss 0.0610 acc 98.09% f1M 0.9791 | 8.6s\n",
            "[FULL] Ep 44/60 | loss 0.0856 acc 97.87% f1M 0.9839 | 9.1s\n",
            "[FULL] Ep 45/60 | loss 0.0509 acc 98.88% f1M 0.9902 | 7.7s\n",
            "[FULL] Ep 46/60 | loss 0.0495 acc 98.54% f1M 0.9906 | 8.9s\n",
            "[FULL] â†’ Early stop @ 46\n",
            "\n",
            "TEST â†’ loss 1.3045 | acc 77.27% | f1M 0.7970\n",
            "\n",
            "Per-class report:\n",
            "                     precision    recall  f1-score   support\n",
            "\n",
            "       Actinophrys     1.0000    1.0000    1.0000         5\n",
            "            Amoeba     0.8000    0.4000    0.5333        10\n",
            "           Arcella     1.0000    0.5000    0.6667         4\n",
            "         Aspidisca     1.0000    0.7500    0.8571         4\n",
            "          Ceratium     1.0000    1.0000    1.0000         6\n",
            "          Codosiga     1.0000    0.5000    0.6667         4\n",
            "           Colpoda     0.6000    0.6000    0.6000         5\n",
            "         Epistylis     0.8333    1.0000    0.9091         5\n",
            "           Euglena     0.7778    0.8077    0.7925        26\n",
            "          Euglypha     0.6667    1.0000    0.8000         2\n",
            "         Gonyaulax     1.0000    1.0000    1.0000         5\n",
            "       Gymnodinium     0.7500    0.7500    0.7500         4\n",
            "             Hydra     0.7778    0.7778    0.7778         9\n",
            "Keratella_quadrala     1.0000    1.0000    1.0000         7\n",
            "         Noctiluca     1.0000    1.0000    1.0000         7\n",
            "        Paramecium     0.8621    0.8621    0.8621        29\n",
            "            Phacus     1.0000    1.0000    1.0000         3\n",
            "      Rod_bacteria     0.4348    0.7143    0.5405        14\n",
            "          Rotifera     0.8571    1.0000    0.9231         6\n",
            "       Siprostomum     0.7500    1.0000    0.8571         6\n",
            "Spherical_bacteria     0.7000    0.5000    0.5833        14\n",
            "   Spiral_bacteria     0.4286    0.2500    0.3158        12\n",
            "           Stentor     0.7778    1.0000    0.8750         7\n",
            "       Stylonychia     0.6667    1.0000    0.8000         4\n",
            "         Synchaeta     1.0000    1.0000    1.0000         8\n",
            "        Vorticella     1.0000    0.6667    0.8000         3\n",
            "             Yeast     0.5833    0.6364    0.6087        11\n",
            "\n",
            "          accuracy                         0.7727       220\n",
            "         macro avg     0.8247    0.8043    0.7970       220\n",
            "      weighted avg     0.7860    0.7727    0.7658       220\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "NBLFvRvy74UR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ===================== EfficientNet-B4 + K-Fold + Albumentations \"Smart Aug\" (no MixUp/CutMix) =====================\n",
        "import os, time, random, numpy as np, torch\n",
        "from torch import nn, optim\n",
        "from torch.utils.data import DataLoader, Dataset\n",
        "from torchvision import datasets, models\n",
        "from torchvision.models import EfficientNet_B4_Weights\n",
        "from sklearn.metrics import accuracy_score, f1_score, classification_report\n",
        "from sklearn.model_selection import StratifiedKFold\n",
        "from PIL import Image\n",
        "from torchvision.transforms.functional import to_tensor\n",
        "\n",
        "# Albumentations\n",
        "import albumentations as A\n",
        "from albumentations.pytorch import ToTensorV2\n",
        "import cv2  # for border_mode constants\n",
        "\n",
        "# ---------- Paths ----------\n",
        "train_dir = \"/content/drive/MyDrive/Bacteria Paper/datasets/Processed/train_320\"\n",
        "test_dir  = \"/content/drive/MyDrive/Bacteria Paper/datasets/Processed/test_320\"\n",
        "\n",
        "# ---------- Your dataset mean/std (0â€“1 scale; RGB) ----------\n",
        "DATA_MEAN = [0.54200659, 0.57473042, 0.55953813]   # <-- keep your computed values\n",
        "DATA_STD  = [0.2644171,  0.25311802, 0.26218295]\n",
        "\n",
        "# ---------- Config ----------\n",
        "SEED = 42\n",
        "IMG_SIZE = 320              # EfficientNet-B4 default is 380, but 320 works fine\n",
        "N_SPLITS = 5\n",
        "EPOCHS = 100\n",
        "PATIENCE = 50\n",
        "\n",
        "HEAD_WARMUP_EPOCHS   = 6     # train only classifier head initially\n",
        "UNFREEZE_LAYER4_EPOCH = 7    # unfreeze \"last stage\"\n",
        "UNFREEZE_L34_EPOCH    = 18   # unfreeze \"last two stages\"\n",
        "\n",
        "LR_HEAD = 1e-3\n",
        "LR_BB   = 3e-4\n",
        "WEIGHT_DECAY = 5e-5\n",
        "BATCH_GPU, BATCH_CPU = 64, 16\n",
        "NUM_WORKERS = min(8, os.cpu_count() or 2)\n",
        "\n",
        "# Smart aug knobs\n",
        "ROT_LIMIT = 20\n",
        "HFLIP_P   = 0.5\n",
        "VFLIP_P   = 0.2\n",
        "CLAHE_P   = 0.2\n",
        "GBLUR_P   = 0.25\n",
        "MBLUR_P   = 0.15\n",
        "GAMMA_P   = 0.3\n",
        "HSV_P     = 0.3\n",
        "RRC_SCALE = (0.85, 1.0)\n",
        "RRC_RATIO = (0.9, 1.1)\n",
        "\n",
        "# ---------- Seed & Device ----------\n",
        "def set_seed(s):\n",
        "    random.seed(s); np.random.seed(s); torch.manual_seed(s); torch.cuda.manual_seed_all(s)\n",
        "set_seed(SEED)\n",
        "torch.backends.cudnn.benchmark = True\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "print(\"Device:\", device)\n",
        "\n",
        "# ---------- Albumentations pipelines (version-proof) ----------\n",
        "print(\"Albumentations version:\", A.__version__)  # for visibility\n",
        "\n",
        "_first_train = []\n",
        "try:\n",
        "    # Validate signature once; if it errors we will fallback\n",
        "    _ = A.RandomResizedCrop(height=IMG_SIZE, width=IMG_SIZE, scale=RRC_SCALE, ratio=RRC_RATIO, p=1.0)\n",
        "    _first_train.append(\n",
        "        A.RandomResizedCrop(height=IMG_SIZE, width=IMG_SIZE, scale=RRC_SCALE, ratio=RRC_RATIO, p=1.0)\n",
        "    )\n",
        "except Exception as e:\n",
        "    print(\"[WARN] RandomResizedCrop not usable in this Albumentations build. \"\n",
        "          \"Using Resize+RandomCrop fallback. Error:\", e)\n",
        "    _first_train += [\n",
        "        A.Resize(height=int(IMG_SIZE * 1.15), width=int(IMG_SIZE * 1.15)),\n",
        "        A.RandomCrop(height=IMG_SIZE, width=IMG_SIZE),\n",
        "    ]\n",
        "\n",
        "train_aug = A.Compose(\n",
        "    _first_train + [\n",
        "        A.HorizontalFlip(p=HFLIP_P),\n",
        "        A.VerticalFlip(p=VFLIP_P),\n",
        "        A.Rotate(limit=ROT_LIMIT, p=0.5, border_mode=cv2.BORDER_REFLECT_101),\n",
        "        A.RandomBrightnessContrast(p=0.5),\n",
        "        A.RandomGamma(p=GAMMA_P),\n",
        "        A.HueSaturationValue(p=HSV_P),\n",
        "        A.GaussianBlur(blur_limit=(3, 5), p=GBLUR_P),\n",
        "        A.MotionBlur(blur_limit=5, p=MBLUR_P),\n",
        "        A.CLAHE(p=CLAHE_P),\n",
        "        A.Normalize(mean=DATA_MEAN, std=DATA_STD),\n",
        "        ToTensorV2(),\n",
        "    ]\n",
        ")\n",
        "\n",
        "val_aug = A.Compose([\n",
        "    A.Resize(height=int(IMG_SIZE * 1.15), width=int(IMG_SIZE * 1.15)),\n",
        "    A.CenterCrop(height=IMG_SIZE, width=IMG_SIZE),\n",
        "    A.Normalize(mean=DATA_MEAN, std=DATA_STD),\n",
        "    ToTensorV2(),\n",
        "])\n",
        "\n",
        "# ---------- Dataset wrapper using Albumentations ----------\n",
        "class AlbDataset(Dataset):\n",
        "    def __init__(self, base, indices=None, transform=None):\n",
        "        self.base = base\n",
        "        self.samples = base.samples\n",
        "        self.indices = list(indices) if indices is not None else list(range(len(self.samples)))\n",
        "        self.targets = np.array([self.base.targets[i] for i in self.indices], dtype=np.int64)\n",
        "        self.transform = transform\n",
        "    def __len__(self): return len(self.indices)\n",
        "    def __getitem__(self, i):\n",
        "        idx = self.indices[i]\n",
        "        path, label = self.samples[idx]\n",
        "        img = np.array(Image.open(path).convert(\"RGB\"))\n",
        "        if self.transform:\n",
        "            img = self.transform(image=img)[\"image\"]\n",
        "        else:\n",
        "            img = to_tensor(Image.fromarray(img)).float()\n",
        "        return img, label\n",
        "\n",
        "# ---------- Data ----------\n",
        "base_full = datasets.ImageFolder(root=train_dir, transform=None)\n",
        "class_names = base_full.classes\n",
        "num_classes = len(class_names)\n",
        "all_targets = np.array(base_full.targets, dtype=np.int64)\n",
        "print(f\"Train images: {len(base_full)} | Classes ({num_classes}): {class_names}\")\n",
        "\n",
        "# ---------- Model / Optim (EfficientNet-B4) ----------\n",
        "def build_efficientnet_b4(num_classes):\n",
        "    m = models.efficientnet_b4(weights=EfficientNet_B4_Weights.DEFAULT)\n",
        "    # Replace final Linear head (classifier[1])\n",
        "    in_f = m.classifier[1].in_features\n",
        "    m.classifier[1] = nn.Linear(in_f, num_classes)\n",
        "    m = m.to(device)\n",
        "    if device.type == \"cuda\":\n",
        "        m = m.to(memory_format=torch.channels_last)\n",
        "    return m\n",
        "\n",
        "def _freeze_all(m):\n",
        "    for p in m.parameters():\n",
        "        p.requires_grad = False\n",
        "\n",
        "def _enable_classifier_head(m):\n",
        "    # Only the final Linear (classifier[1]) is trainable in head warm-up\n",
        "    for p in m.classifier[1].parameters():\n",
        "        p.requires_grad = True\n",
        "\n",
        "def _enable_last_stage(m):\n",
        "    # Unfreeze \"last stage\" â‰ˆ final conv + last MBConv block(s)\n",
        "    L = len(m.features)\n",
        "    idxs = [L-1, L-2]  # last conv + last MBConv block\n",
        "    for i in idxs:\n",
        "        if i >= 0:\n",
        "            for p in m.features[i].parameters():\n",
        "                p.requires_grad = True\n",
        "    for p in m.classifier[1].parameters():\n",
        "        p.requires_grad = True\n",
        "\n",
        "def _enable_last_two_stages(m):\n",
        "    # Unfreeze \"last two stages\" â‰ˆ final conv + last two MBConv blocks\n",
        "    L = len(m.features)\n",
        "    idxs = [L-1, L-2, L-3]\n",
        "    for i in idxs:\n",
        "        if i >= 0:\n",
        "            for p in m.features[i].parameters():\n",
        "                p.requires_grad = True\n",
        "    for p in m.classifier[1].parameters():\n",
        "        p.requires_grad = True\n",
        "\n",
        "def set_trainable(m, mode):\n",
        "    # mode: \"head\" | \"layer4\" | \"layer3_4\"\n",
        "    _freeze_all(m)\n",
        "    if mode == \"head\":\n",
        "        _enable_classifier_head(m)\n",
        "    elif mode == \"layer4\":\n",
        "        _enable_last_stage(m)\n",
        "    elif mode == \"layer3_4\":\n",
        "        _enable_last_two_stages(m)\n",
        "\n",
        "def make_optimizer(model):\n",
        "    # Separate LR for head vs backbone (classifier[1] vs the rest trainable)\n",
        "    head_params, bb_params = [], []\n",
        "    for n, p in model.named_parameters():\n",
        "        if not p.requires_grad:\n",
        "            continue\n",
        "        # Anything in classifier.1 is head; everything else trainable is backbone\n",
        "        if n.startswith(\"classifier.1\"):\n",
        "            head_params.append(p)\n",
        "        else:\n",
        "            bb_params.append(p)\n",
        "    return optim.AdamW(\n",
        "        [{'params': head_params, 'lr': LR_HEAD},\n",
        "         {'params': bb_params,   'lr': LR_BB}],\n",
        "        weight_decay=WEIGHT_DECAY\n",
        "    )\n",
        "\n",
        "# ---------- Train/Eval ----------\n",
        "@torch.no_grad()\n",
        "def evaluate(model, loader, criterion):\n",
        "    model.eval()\n",
        "    tot, y_true, y_pred = 0.0, [], []\n",
        "    for xb,yb in loader:\n",
        "        xb = xb.to(device, non_blocking=True); yb = yb.to(device, non_blocking=True)\n",
        "        with torch.autocast(device_type='cuda', enabled=(device.type=='cuda')):\n",
        "            logits = model(xb)\n",
        "            loss = criterion(logits, yb)\n",
        "        tot += loss.item()\n",
        "        y_pred += logits.argmax(1).cpu().tolist()\n",
        "        y_true += yb.cpu().tolist()\n",
        "    loss = tot / max(1, len(loader))\n",
        "    acc  = accuracy_score(y_true, y_pred)\n",
        "    f1m  = f1_score(y_true, y_pred, average=\"macro\", zero_division=0)\n",
        "    return loss, acc, f1m\n",
        "\n",
        "def train_one_epoch(model, loader, optimizer, criterion):\n",
        "    model.train()\n",
        "    tot, y_true, y_pred = 0.0, [], []\n",
        "    for xb,yb in loader:\n",
        "        xb = xb.to(device, non_blocking=True); yb = yb.to(device, non_blocking=True)\n",
        "        optimizer.zero_grad(set_to_none=True)\n",
        "        with torch.autocast(device_type='cuda', enabled=(device.type=='cuda')):\n",
        "            logits = model(xb)\n",
        "            loss = criterion(logits, yb)\n",
        "        loss.backward(); optimizer.step()\n",
        "        tot += loss.item()\n",
        "        with torch.no_grad():\n",
        "            y_pred += logits.argmax(1).cpu().tolist()\n",
        "            y_true += yb.cpu().tolist()\n",
        "    loss = tot / max(1, len(loader))\n",
        "    acc  = accuracy_score(y_true, y_pred)\n",
        "    f1m  = f1_score(y_true, y_pred, average=\"macro\", zero_division=0)\n",
        "    return loss, acc, f1m\n",
        "\n",
        "def make_loader(ds, shuffle):\n",
        "    return DataLoader(\n",
        "        ds,\n",
        "        batch_size=(BATCH_GPU if device.type=='cuda' else BATCH_CPU),\n",
        "        shuffle=shuffle, num_workers=NUM_WORKERS,\n",
        "        pin_memory=(device.type=='cuda'),\n",
        "        persistent_workers=(NUM_WORKERS>0 and device.type=='cuda')\n",
        "    )\n",
        "\n",
        "def run_fold(fold, tr_idx, va_idx):\n",
        "    ds_tr = AlbDataset(base_full, tr_idx, transform=train_aug)\n",
        "    ds_va = AlbDataset(base_full, va_idx, transform=val_aug)\n",
        "    dl_tr = make_loader(ds_tr, shuffle=True)\n",
        "    dl_va = make_loader(ds_va, shuffle=False)\n",
        "\n",
        "    model = build_efficientnet_b4(num_classes)\n",
        "    set_trainable(model, \"head\")\n",
        "    optimizer = make_optimizer(model)\n",
        "    scheduler = optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max=EPOCHS)\n",
        "    criterion = nn.CrossEntropyLoss()\n",
        "\n",
        "    best = {\"f1\": -1, \"ep\": -1, \"state\": None, \"val_acc\": 0.0}\n",
        "    wait = 0\n",
        "\n",
        "    for ep in range(1, EPOCHS+1):\n",
        "        # gradual unfreeze\n",
        "        if ep == UNFREEZE_LAYER4_EPOCH:\n",
        "            set_trainable(model, \"layer4\"); optimizer = make_optimizer(model)\n",
        "        if ep == UNFREEZE_L34_EPOCH:\n",
        "            set_trainable(model, \"layer3_4\"); optimizer = make_optimizer(model)\n",
        "\n",
        "        t0 = time.time()\n",
        "        tr_loss, tr_acc, tr_f1 = train_one_epoch(model, dl_tr, optimizer, criterion)\n",
        "        va_loss, va_acc, va_f1 = evaluate(model, dl_va, criterion)\n",
        "        scheduler.step()\n",
        "\n",
        "        print(f\"[Fold {fold}] Ep {ep:02d}/{EPOCHS} | \"\n",
        "              f\"Train loss {tr_loss:.4f} acc {tr_acc*100:5.2f}% f1M {tr_f1:.4f} | \"\n",
        "              f\"Val loss {va_loss:.4f} acc {va_acc*100:5.2f}% f1M {va_f1:.4f} | {time.time()-t0:.1f}s\")\n",
        "\n",
        "        if va_f1 > best[\"f1\"]:\n",
        "            best.update({\"f1\": va_f1, \"ep\": ep, \"state\": {k:v.cpu() for k,v in model.state_dict().items()}, \"val_acc\": va_acc})\n",
        "            wait = 0\n",
        "        else:\n",
        "            wait += 1\n",
        "            if wait >= PATIENCE:\n",
        "                print(f\"[Fold {fold}] Early stop @ {ep} (best @ {best['ep']}, F1M={best['f1']:.4f})\")\n",
        "                break\n",
        "\n",
        "    ckpt = f\"efficientnet_b4_alb_smart_fold{fold}.pt\"\n",
        "    if best[\"state\"] is not None:\n",
        "        torch.save(best[\"state\"], ckpt)\n",
        "\n",
        "    # Final val from best\n",
        "    model.load_state_dict(torch.load(ckpt, map_location=device))\n",
        "    va_loss, va_acc, va_f1 = evaluate(model, dl_va, criterion)\n",
        "    print(f\"[Fold {fold}] Best Val â†’ acc {va_acc*100:5.2f}% | f1M {va_f1:.4f} (epoch {best['ep']})\")\n",
        "    return {\"fold\": fold, \"val_acc\": va_acc, \"val_f1\": va_f1, \"epoch\": best[\"ep\"], \"ckpt\": ckpt}\n",
        "\n",
        "# ---------- K-Fold ----------\n",
        "skf = StratifiedKFold(n_splits=N_SPLITS, shuffle=True, random_state=SEED)\n",
        "fold_summaries = []\n",
        "for fold, (tr_idx, va_idx) in enumerate(skf.split(np.arange(len(all_targets)), all_targets), start=1):\n",
        "    print(f\"\\n===== K-Fold {fold}/{N_SPLITS} ===== (train {len(tr_idx)} | val {len(va_idx)})\")\n",
        "    fs = run_fold(fold, tr_idx, va_idx)\n",
        "    fold_summaries.append(fs)\n",
        "\n",
        "cv_accs = [fs[\"val_acc\"] for fs in fold_summaries]\n",
        "cv_f1s  = [fs[\"val_f1\"] for fs in fold_summaries]\n",
        "print(\"\\n===== CV Summary (best epoch per fold) =====\")\n",
        "print(\"Val Accs:\", [f\"{a*100:0.2f}%\" for a in cv_accs], f\"| MeanÂ±Std = {np.mean(cv_accs)*100:0.2f}% Â± {np.std(cv_accs)*100:0.2f}%\")\n",
        "print(\"Val F1M :\", [f\"{f:0.4f}\" for f in cv_f1s],        f\"| MeanÂ±Std = {np.mean(cv_f1s):0.4f} Â± {np.std(cv_f1s):0.4f}\")\n",
        "\n",
        "# ---------- Full-train on train/, evaluate on test/ ----------\n",
        "if os.path.isdir(test_dir):\n",
        "    full_base  = datasets.ImageFolder(root=train_dir, transform=None)\n",
        "    test_base  = datasets.ImageFolder(root=test_dir, transform=None)\n",
        "    full_ds    = AlbDataset(full_base, transform=train_aug)\n",
        "    test_ds    = AlbDataset(test_base, transform=val_aug)\n",
        "\n",
        "    full_loader = DataLoader(full_ds, batch_size=(BATCH_GPU if device.type=='cuda' else BATCH_CPU),\n",
        "                             shuffle=True, num_workers=NUM_WORKERS, pin_memory=(device.type=='cuda'),\n",
        "                             persistent_workers=(NUM_WORKERS>0 and device.type=='cuda'))\n",
        "    test_loader = DataLoader(test_ds, batch_size=(BATCH_GPU if device.type=='cuda' else BATCH_CPU),\n",
        "                             shuffle=False, num_workers=NUM_WORKERS, pin_memory=(device.type=='cuda'),\n",
        "                             persistent_workers=(NUM_WORKERS>0 and device.type=='cuda'))\n",
        "\n",
        "    model = build_efficientnet_b4(num_classes)\n",
        "    set_trainable(model, \"head\")\n",
        "    optimizer = make_optimizer(model)\n",
        "    scheduler = optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max=EPOCHS)\n",
        "    criterion = nn.CrossEntropyLoss()\n",
        "\n",
        "    best_loss, wait = 1e9, 0\n",
        "    best_full_path = \"efficientnet_b4_alb_smart_fulltrain.pt\"\n",
        "    for ep in range(1, EPOCHS+1):\n",
        "        if ep == UNFREEZE_LAYER4_EPOCH:\n",
        "            set_trainable(model, \"layer4\"); optimizer = make_optimizer(model)\n",
        "        if ep == UNFREEZE_L34_EPOCH:\n",
        "            set_trainable(model, \"layer3_4\"); optimizer = make_optimizer(model)\n",
        "\n",
        "        t0 = time.time()\n",
        "        tr_loss, tr_acc, tr_f1 = train_one_epoch(model, full_loader, optimizer, criterion)\n",
        "        scheduler.step()\n",
        "        print(f\"[FULL] Ep {ep:02d}/{EPOCHS} | loss {tr_loss:.4f} acc {tr_acc*100:5.2f}% f1M {tr_f1:.4f} | {time.time()-t0:.1f}s\")\n",
        "\n",
        "        if tr_loss < best_loss:\n",
        "            best_loss, wait = tr_loss, 0; torch.save(model.state_dict(), best_full_path)\n",
        "        else:\n",
        "            wait += 1\n",
        "            if wait >= PATIENCE:\n",
        "                print(f\"[FULL] â†’ Early stop @ {ep}\")\n",
        "                break\n",
        "\n",
        "    # Evaluate best\n",
        "    model.load_state_dict(torch.load(best_full_path, map_location=device))\n",
        "    test_loss, test_acc, test_f1 = evaluate(model, test_loader, criterion)\n",
        "    print(f\"\\nTEST â†’ loss {test_loss:.4f} | acc {test_acc*100:5.2f}% | f1M {test_f1:.4f}\")\n",
        "\n",
        "    @torch.no_grad()\n",
        "    def per_class_report():\n",
        "        model.eval()\n",
        "        y_true, y_pred = [], []\n",
        "        for xb, yb in test_loader:\n",
        "            xb = xb.to(device, non_blocking=True)\n",
        "            with torch.autocast(device_type='cuda', enabled=(device.type=='cuda')):\n",
        "                logits = model(xb)\n",
        "            y_pred.extend(logits.argmax(1).cpu().tolist())\n",
        "            y_true.extend(yb.numpy().tolist())\n",
        "        print(\"\\nPer-class report:\\n\",\n",
        "              classification_report(y_true, y_pred, target_names=test_base.classes, digits=4, zero_division=0))\n",
        "    per_class_report()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "7062e515-d27c-406f-fba1-1801c345a2f3",
        "id": "_8KWM_Be8AN_"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Device: cuda\n",
            "Albumentations version: 2.0.8\n",
            "[WARN] RandomResizedCrop not usable in this Albumentations build. Using Resize+RandomCrop fallback. Error: 1 validation error for InitSchema\n",
            "size\n",
            "  Field required [type=missing, input_value={'scale': (0.85, 1.0), 'r...: None, 'strict': False}, input_type=dict]\n",
            "    For further information visit https://errors.pydantic.dev/2.11/v/missing\n",
            "Train images: 891 | Classes (27): ['Actinophrys', 'Amoeba', 'Arcella', 'Aspidisca', 'Ceratium', 'Codosiga', 'Colpoda', 'Epistylis', 'Euglena', 'Euglypha', 'Gonyaulax', 'Gymnodinium', 'Hydra', 'Keratella_quadrala', 'Noctiluca', 'Paramecium', 'Phacus', 'Rod_bacteria', 'Rotifera', 'Siprostomum', 'Spherical_bacteria', 'Spiral_bacteria', 'Stentor', 'Stylonychia', 'Synchaeta', 'Vorticella', 'Yeast']\n",
            "\n",
            "===== K-Fold 1/5 ===== (train 712 | val 179)\n",
            "Downloading: \"https://download.pytorch.org/models/efficientnet_b4_rwightman-23ab8bcd.pth\" to /root/.cache/torch/hub/checkpoints/efficientnet_b4_rwightman-23ab8bcd.pth\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 74.5M/74.5M [00:00<00:00, 200MB/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[Fold 1] Ep 01/100 | Train loss 3.2252 acc 16.99% f1M 0.1146 | Val loss 3.1242 acc 39.11% f1M 0.2945 | 65.8s\n",
            "[Fold 1] Ep 02/100 | Train loss 3.0474 acc 44.24% f1M 0.3546 | Val loss 2.9613 acc 54.19% f1M 0.4151 | 8.3s\n",
            "[Fold 1] Ep 03/100 | Train loss 2.8697 acc 52.39% f1M 0.4461 | Val loss 2.8112 acc 54.75% f1M 0.4706 | 9.0s\n",
            "[Fold 1] Ep 04/100 | Train loss 2.7577 acc 58.71% f1M 0.5470 | Val loss 2.7020 acc 57.54% f1M 0.4945 | 7.5s\n",
            "[Fold 1] Ep 05/100 | Train loss 2.6143 acc 59.69% f1M 0.5365 | Val loss 2.5904 acc 61.45% f1M 0.5683 | 8.0s\n",
            "[Fold 1] Ep 06/100 | Train loss 2.4896 acc 64.33% f1M 0.5824 | Val loss 2.4787 acc 63.13% f1M 0.5933 | 6.7s\n",
            "[Fold 1] Ep 07/100 | Train loss 2.1860 acc 60.81% f1M 0.5561 | Val loss 1.9646 acc 60.34% f1M 0.5388 | 17.3s\n",
            "[Fold 1] Ep 08/100 | Train loss 1.6956 acc 66.99% f1M 0.6586 | Val loss 1.5988 acc 64.80% f1M 0.6247 | 7.8s\n",
            "[Fold 1] Ep 09/100 | Train loss 1.4058 acc 73.03% f1M 0.7235 | Val loss 1.3260 acc 68.72% f1M 0.6648 | 7.3s\n",
            "[Fold 1] Ep 10/100 | Train loss 1.1080 acc 76.26% f1M 0.7766 | Val loss 1.1542 acc 68.72% f1M 0.6723 | 7.6s\n",
            "[Fold 1] Ep 11/100 | Train loss 0.8643 acc 81.18% f1M 0.8390 | Val loss 1.0345 acc 71.51% f1M 0.7105 | 8.2s\n",
            "[Fold 1] Ep 12/100 | Train loss 0.7416 acc 82.72% f1M 0.8436 | Val loss 0.9457 acc 72.63% f1M 0.7302 | 6.8s\n",
            "[Fold 1] Ep 13/100 | Train loss 0.6103 acc 84.27% f1M 0.8708 | Val loss 0.9079 acc 72.07% f1M 0.7416 | 8.3s\n",
            "[Fold 1] Ep 14/100 | Train loss 0.6081 acc 86.66% f1M 0.8828 | Val loss 0.8759 acc 77.09% f1M 0.7949 | 7.2s\n",
            "[Fold 1] Ep 15/100 | Train loss 0.5438 acc 86.10% f1M 0.8761 | Val loss 0.8896 acc 74.86% f1M 0.7790 | 8.5s\n",
            "[Fold 1] Ep 16/100 | Train loss 0.4557 acc 89.61% f1M 0.9113 | Val loss 0.8719 acc 76.54% f1M 0.7844 | 6.7s\n",
            "[Fold 1] Ep 17/100 | Train loss 0.4587 acc 89.33% f1M 0.9106 | Val loss 0.8643 acc 74.30% f1M 0.7707 | 8.3s\n",
            "[Fold 1] Ep 18/100 | Train loss 0.3897 acc 87.36% f1M 0.8803 | Val loss 0.8083 acc 79.89% f1M 0.8175 | 17.5s\n",
            "[Fold 1] Ep 19/100 | Train loss 0.3754 acc 91.15% f1M 0.9264 | Val loss 0.8036 acc 79.89% f1M 0.8046 | 8.3s\n",
            "[Fold 1] Ep 20/100 | Train loss 0.2641 acc 93.54% f1M 0.9476 | Val loss 0.8073 acc 79.89% f1M 0.8143 | 7.0s\n",
            "[Fold 1] Ep 21/100 | Train loss 0.2385 acc 94.38% f1M 0.9525 | Val loss 0.8586 acc 78.21% f1M 0.8083 | 8.5s\n",
            "[Fold 1] Ep 22/100 | Train loss 0.2486 acc 92.84% f1M 0.9350 | Val loss 0.8376 acc 80.45% f1M 0.8263 | 7.5s\n",
            "[Fold 1] Ep 23/100 | Train loss 0.2152 acc 93.12% f1M 0.9481 | Val loss 0.8506 acc 80.45% f1M 0.8259 | 7.7s\n",
            "[Fold 1] Ep 24/100 | Train loss 0.1642 acc 96.77% f1M 0.9708 | Val loss 0.8694 acc 77.09% f1M 0.8021 | 8.2s\n",
            "[Fold 1] Ep 25/100 | Train loss 0.1520 acc 95.93% f1M 0.9702 | Val loss 0.8666 acc 79.33% f1M 0.8028 | 7.0s\n",
            "[Fold 1] Ep 26/100 | Train loss 0.1887 acc 94.66% f1M 0.9539 | Val loss 0.8416 acc 81.01% f1M 0.8302 | 8.6s\n",
            "[Fold 1] Ep 27/100 | Train loss 0.1990 acc 96.49% f1M 0.9726 | Val loss 0.8762 acc 81.56% f1M 0.8149 | 7.1s\n",
            "[Fold 1] Ep 28/100 | Train loss 0.0824 acc 98.03% f1M 0.9813 | Val loss 0.9201 acc 78.21% f1M 0.8125 | 8.6s\n",
            "[Fold 1] Ep 29/100 | Train loss 0.1294 acc 96.77% f1M 0.9745 | Val loss 0.9111 acc 79.33% f1M 0.8062 | 7.4s\n",
            "[Fold 1] Ep 30/100 | Train loss 0.1341 acc 96.49% f1M 0.9737 | Val loss 0.9136 acc 79.33% f1M 0.8005 | 8.2s\n",
            "[Fold 1] Ep 31/100 | Train loss 0.0894 acc 97.19% f1M 0.9792 | Val loss 0.9205 acc 77.65% f1M 0.7930 | 8.3s\n",
            "[Fold 1] Ep 32/100 | Train loss 0.1619 acc 97.47% f1M 0.9789 | Val loss 0.9247 acc 79.89% f1M 0.8219 | 7.3s\n",
            "[Fold 1] Ep 33/100 | Train loss 0.0830 acc 98.17% f1M 0.9889 | Val loss 0.9423 acc 80.45% f1M 0.8026 | 8.6s\n",
            "[Fold 1] Ep 34/100 | Train loss 0.0977 acc 97.75% f1M 0.9831 | Val loss 0.9527 acc 79.33% f1M 0.8034 | 7.1s\n",
            "[Fold 1] Ep 35/100 | Train loss 0.0795 acc 97.33% f1M 0.9768 | Val loss 0.9796 acc 78.77% f1M 0.8037 | 8.5s\n",
            "[Fold 1] Ep 36/100 | Train loss 0.0971 acc 97.61% f1M 0.9799 | Val loss 0.9645 acc 78.21% f1M 0.7938 | 7.2s\n",
            "[Fold 1] Ep 37/100 | Train loss 0.0477 acc 99.02% f1M 0.9921 | Val loss 0.9953 acc 79.33% f1M 0.8029 | 8.2s\n",
            "[Fold 1] Ep 38/100 | Train loss 0.0678 acc 97.61% f1M 0.9798 | Val loss 1.0180 acc 79.33% f1M 0.8055 | 7.9s\n",
            "[Fold 1] Ep 39/100 | Train loss 0.0903 acc 98.46% f1M 0.9864 | Val loss 1.0310 acc 79.33% f1M 0.8109 | 7.4s\n",
            "[Fold 1] Ep 40/100 | Train loss 0.1035 acc 98.17% f1M 0.9863 | Val loss 1.0277 acc 79.33% f1M 0.8121 | 8.5s\n",
            "[Fold 1] Ep 41/100 | Train loss 0.0594 acc 98.60% f1M 0.9891 | Val loss 0.9806 acc 79.89% f1M 0.8311 | 7.1s\n",
            "[Fold 1] Ep 42/100 | Train loss 0.0558 acc 98.46% f1M 0.9861 | Val loss 0.9855 acc 79.33% f1M 0.8224 | 8.3s\n",
            "[Fold 1] Ep 43/100 | Train loss 0.1095 acc 97.61% f1M 0.9794 | Val loss 0.9631 acc 79.89% f1M 0.8239 | 7.6s\n",
            "[Fold 1] Ep 44/100 | Train loss 0.1107 acc 96.63% f1M 0.9721 | Val loss 0.9734 acc 81.01% f1M 0.8349 | 9.1s\n",
            "[Fold 1] Ep 45/100 | Train loss 0.0780 acc 97.89% f1M 0.9845 | Val loss 0.9336 acc 81.01% f1M 0.8375 | 7.1s\n",
            "[Fold 1] Ep 46/100 | Train loss 0.0858 acc 98.17% f1M 0.9804 | Val loss 0.9395 acc 81.56% f1M 0.8425 | 7.9s\n",
            "[Fold 1] Ep 47/100 | Train loss 0.0891 acc 97.75% f1M 0.9806 | Val loss 0.9384 acc 81.01% f1M 0.8337 | 7.8s\n",
            "[Fold 1] Ep 48/100 | Train loss 0.0782 acc 97.89% f1M 0.9848 | Val loss 0.9534 acc 79.89% f1M 0.8316 | 7.3s\n",
            "[Fold 1] Ep 49/100 | Train loss 0.0669 acc 98.03% f1M 0.9832 | Val loss 0.9033 acc 81.56% f1M 0.8324 | 8.4s\n",
            "[Fold 1] Ep 50/100 | Train loss 0.0439 acc 98.74% f1M 0.9905 | Val loss 0.9228 acc 81.56% f1M 0.8331 | 6.9s\n",
            "[Fold 1] Ep 51/100 | Train loss 0.0522 acc 98.60% f1M 0.9813 | Val loss 0.9307 acc 80.45% f1M 0.8229 | 8.5s\n",
            "[Fold 1] Ep 52/100 | Train loss 0.0506 acc 99.16% f1M 0.9927 | Val loss 0.9262 acc 79.89% f1M 0.8187 | 7.0s\n",
            "[Fold 1] Ep 53/100 | Train loss 0.0366 acc 98.74% f1M 0.9928 | Val loss 0.9675 acc 79.89% f1M 0.8318 | 8.4s\n",
            "[Fold 1] Ep 54/100 | Train loss 0.0878 acc 98.03% f1M 0.9825 | Val loss 0.9737 acc 79.89% f1M 0.8164 | 7.2s\n",
            "[Fold 1] Ep 55/100 | Train loss 0.0526 acc 98.31% f1M 0.9886 | Val loss 0.9963 acc 80.45% f1M 0.8312 | 8.0s\n",
            "[Fold 1] Ep 56/100 | Train loss 0.1342 acc 97.75% f1M 0.9819 | Val loss 0.9433 acc 81.01% f1M 0.8402 | 8.1s\n",
            "[Fold 1] Ep 57/100 | Train loss 0.0438 acc 98.88% f1M 0.9905 | Val loss 0.9585 acc 80.45% f1M 0.8434 | 7.0s\n",
            "[Fold 1] Ep 58/100 | Train loss 0.1356 acc 97.75% f1M 0.9834 | Val loss 0.9428 acc 78.77% f1M 0.8184 | 8.4s\n",
            "[Fold 1] Ep 59/100 | Train loss 0.0843 acc 98.60% f1M 0.9860 | Val loss 1.0427 acc 79.89% f1M 0.8189 | 7.1s\n",
            "[Fold 1] Ep 60/100 | Train loss 0.1022 acc 98.03% f1M 0.9866 | Val loss 0.9540 acc 80.45% f1M 0.8358 | 8.2s\n",
            "[Fold 1] Ep 61/100 | Train loss 0.0631 acc 98.74% f1M 0.9887 | Val loss 0.9307 acc 79.33% f1M 0.8134 | 6.9s\n",
            "[Fold 1] Ep 62/100 | Train loss 0.0698 acc 98.46% f1M 0.9862 | Val loss 0.9020 acc 81.01% f1M 0.8296 | 8.4s\n",
            "[Fold 1] Ep 63/100 | Train loss 0.0525 acc 98.60% f1M 0.9875 | Val loss 0.8808 acc 82.12% f1M 0.8403 | 7.5s\n",
            "[Fold 1] Ep 64/100 | Train loss 0.0733 acc 98.46% f1M 0.9905 | Val loss 0.8942 acc 81.56% f1M 0.8459 | 7.7s\n",
            "[Fold 1] Ep 65/100 | Train loss 0.0515 acc 98.31% f1M 0.9862 | Val loss 0.8833 acc 83.24% f1M 0.8667 | 8.1s\n",
            "[Fold 1] Ep 66/100 | Train loss 0.0531 acc 98.74% f1M 0.9897 | Val loss 0.9076 acc 82.12% f1M 0.8426 | 6.9s\n",
            "[Fold 1] Ep 67/100 | Train loss 0.0589 acc 99.44% f1M 0.9962 | Val loss 0.9389 acc 79.89% f1M 0.8079 | 8.4s\n",
            "[Fold 1] Ep 68/100 | Train loss 0.0969 acc 98.74% f1M 0.9916 | Val loss 0.9359 acc 79.89% f1M 0.8151 | 7.1s\n",
            "[Fold 1] Ep 69/100 | Train loss 0.0341 acc 98.60% f1M 0.9894 | Val loss 0.9354 acc 79.89% f1M 0.8203 | 8.3s\n",
            "[Fold 1] Ep 70/100 | Train loss 0.0867 acc 98.88% f1M 0.9893 | Val loss 0.9120 acc 81.56% f1M 0.8387 | 6.9s\n",
            "[Fold 1] Ep 71/100 | Train loss 0.0359 acc 98.88% f1M 0.9910 | Val loss 0.9358 acc 81.01% f1M 0.8355 | 8.3s\n",
            "[Fold 1] Ep 72/100 | Train loss 0.0634 acc 98.74% f1M 0.9917 | Val loss 0.9343 acc 81.01% f1M 0.8330 | 7.6s\n",
            "[Fold 1] Ep 73/100 | Train loss 0.0478 acc 98.60% f1M 0.9860 | Val loss 0.9179 acc 81.01% f1M 0.8313 | 7.6s\n",
            "[Fold 1] Ep 74/100 | Train loss 0.0628 acc 99.02% f1M 0.9941 | Val loss 0.9248 acc 79.89% f1M 0.8190 | 8.1s\n",
            "[Fold 1] Ep 75/100 | Train loss 0.0544 acc 98.88% f1M 0.9867 | Val loss 0.9386 acc 80.45% f1M 0.8212 | 7.0s\n",
            "[Fold 1] Ep 76/100 | Train loss 0.0613 acc 98.74% f1M 0.9906 | Val loss 0.9097 acc 80.45% f1M 0.8255 | 8.1s\n",
            "[Fold 1] Ep 77/100 | Train loss 0.0293 acc 99.44% f1M 0.9964 | Val loss 0.8999 acc 79.89% f1M 0.8232 | 6.8s\n",
            "[Fold 1] Ep 78/100 | Train loss 0.0352 acc 99.58% f1M 0.9970 | Val loss 0.9047 acc 79.89% f1M 0.8256 | 8.3s\n",
            "[Fold 1] Ep 79/100 | Train loss 0.0539 acc 98.74% f1M 0.9897 | Val loss 0.9383 acc 81.56% f1M 0.8376 | 7.2s\n",
            "[Fold 1] Ep 80/100 | Train loss 0.0397 acc 98.74% f1M 0.9930 | Val loss 0.9956 acc 80.45% f1M 0.8248 | 8.5s\n",
            "[Fold 1] Ep 81/100 | Train loss 0.1392 acc 98.74% f1M 0.9906 | Val loss 0.9584 acc 80.45% f1M 0.8351 | 7.4s\n",
            "[Fold 1] Ep 82/100 | Train loss 0.0388 acc 99.16% f1M 0.9925 | Val loss 0.9657 acc 78.77% f1M 0.8311 | 7.7s\n",
            "[Fold 1] Ep 83/100 | Train loss 0.0450 acc 98.46% f1M 0.9865 | Val loss 0.9294 acc 80.45% f1M 0.8486 | 8.2s\n",
            "[Fold 1] Ep 84/100 | Train loss 0.0421 acc 98.88% f1M 0.9939 | Val loss 0.8998 acc 81.01% f1M 0.8436 | 7.0s\n",
            "[Fold 1] Ep 85/100 | Train loss 0.0367 acc 99.44% f1M 0.9951 | Val loss 0.8998 acc 81.01% f1M 0.8335 | 8.4s\n",
            "[Fold 1] Ep 86/100 | Train loss 0.0549 acc 99.02% f1M 0.9924 | Val loss 0.9199 acc 80.45% f1M 0.8382 | 6.9s\n",
            "[Fold 1] Ep 87/100 | Train loss 0.0362 acc 99.16% f1M 0.9933 | Val loss 0.9241 acc 80.45% f1M 0.8243 | 8.6s\n",
            "[Fold 1] Ep 88/100 | Train loss 0.0455 acc 98.46% f1M 0.9877 | Val loss 0.9631 acc 81.01% f1M 0.8368 | 8.2s\n",
            "[Fold 1] Ep 89/100 | Train loss 0.0355 acc 98.88% f1M 0.9901 | Val loss 0.9803 acc 81.56% f1M 0.8344 | 8.4s\n",
            "[Fold 1] Ep 90/100 | Train loss 0.0385 acc 98.60% f1M 0.9860 | Val loss 0.9343 acc 81.56% f1M 0.8470 | 7.8s\n",
            "[Fold 1] Ep 91/100 | Train loss 0.0310 acc 99.16% f1M 0.9965 | Val loss 0.9366 acc 83.24% f1M 0.8558 | 7.6s\n",
            "[Fold 1] Ep 92/100 | Train loss 0.0214 acc 99.30% f1M 0.9954 | Val loss 0.9804 acc 81.01% f1M 0.8340 | 8.5s\n",
            "[Fold 1] Ep 93/100 | Train loss 0.0281 acc 99.16% f1M 0.9938 | Val loss 0.9776 acc 81.01% f1M 0.8375 | 7.1s\n",
            "[Fold 1] Ep 94/100 | Train loss 0.0350 acc 99.30% f1M 0.9954 | Val loss 0.9852 acc 81.56% f1M 0.8412 | 8.4s\n",
            "[Fold 1] Ep 95/100 | Train loss 0.0587 acc 98.60% f1M 0.9891 | Val loss 1.0293 acc 79.89% f1M 0.8261 | 7.2s\n",
            "[Fold 1] Ep 96/100 | Train loss 0.0399 acc 99.16% f1M 0.9956 | Val loss 1.0019 acc 81.56% f1M 0.8421 | 8.5s\n",
            "[Fold 1] Ep 97/100 | Train loss 0.0365 acc 98.88% f1M 0.9874 | Val loss 1.0137 acc 81.01% f1M 0.8347 | 7.4s\n",
            "[Fold 1] Ep 98/100 | Train loss 0.0275 acc 99.16% f1M 0.9930 | Val loss 0.9914 acc 81.01% f1M 0.8350 | 7.9s\n",
            "[Fold 1] Ep 99/100 | Train loss 0.0188 acc 99.30% f1M 0.9960 | Val loss 0.9642 acc 80.45% f1M 0.8278 | 8.0s\n",
            "[Fold 1] Ep 100/100 | Train loss 0.0480 acc 98.74% f1M 0.9910 | Val loss 0.9695 acc 81.01% f1M 0.8309 | 7.0s\n",
            "[Fold 1] Best Val â†’ acc 83.24% | f1M 0.8667 (epoch 65)\n",
            "\n",
            "===== K-Fold 2/5 ===== (train 713 | val 178)\n",
            "[Fold 2] Ep 01/100 | Train loss 3.2317 acc 16.55% f1M 0.1163 | Val loss 3.1273 acc 38.20% f1M 0.2754 | 40.3s\n",
            "[Fold 2] Ep 02/100 | Train loss 3.0348 acc 45.44% f1M 0.3929 | Val loss 2.9600 acc 46.07% f1M 0.3251 | 8.2s\n",
            "[Fold 2] Ep 03/100 | Train loss 2.8705 acc 53.16% f1M 0.4642 | Val loss 2.8286 acc 51.69% f1M 0.3779 | 6.6s\n",
            "[Fold 2] Ep 04/100 | Train loss 2.7283 acc 58.63% f1M 0.5230 | Val loss 2.7058 acc 53.93% f1M 0.4177 | 8.1s\n",
            "[Fold 2] Ep 05/100 | Train loss 2.5746 acc 62.27% f1M 0.5897 | Val loss 2.5921 acc 56.74% f1M 0.4441 | 6.9s\n",
            "[Fold 2] Ep 06/100 | Train loss 2.4639 acc 62.55% f1M 0.5713 | Val loss 2.4920 acc 56.18% f1M 0.4472 | 7.5s\n",
            "[Fold 2] Ep 07/100 | Train loss 2.1583 acc 61.01% f1M 0.5664 | Val loss 1.9892 acc 55.06% f1M 0.4609 | 12.2s\n",
            "[Fold 2] Ep 08/100 | Train loss 1.7485 acc 65.78% f1M 0.6225 | Val loss 1.5940 acc 62.92% f1M 0.5816 | 6.9s\n",
            "[Fold 2] Ep 09/100 | Train loss 1.3465 acc 73.63% f1M 0.7295 | Val loss 1.3291 acc 67.98% f1M 0.6351 | 8.0s\n",
            "[Fold 2] Ep 10/100 | Train loss 1.0848 acc 77.28% f1M 0.7950 | Val loss 1.1353 acc 73.60% f1M 0.7083 | 7.5s\n",
            "[Fold 2] Ep 11/100 | Train loss 0.8977 acc 78.96% f1M 0.8127 | Val loss 1.0218 acc 74.16% f1M 0.7263 | 7.1s\n",
            "[Fold 2] Ep 12/100 | Train loss 0.8320 acc 82.33% f1M 0.8476 | Val loss 0.9191 acc 75.28% f1M 0.7393 | 7.8s\n",
            "[Fold 2] Ep 13/100 | Train loss 0.6540 acc 85.27% f1M 0.8557 | Val loss 0.8328 acc 75.28% f1M 0.7505 | 6.8s\n",
            "[Fold 2] Ep 14/100 | Train loss 0.6758 acc 83.87% f1M 0.8670 | Val loss 0.8059 acc 75.84% f1M 0.7526 | 8.0s\n",
            "[Fold 2] Ep 15/100 | Train loss 0.5557 acc 85.55% f1M 0.8729 | Val loss 0.8187 acc 76.97% f1M 0.7579 | 6.6s\n",
            "[Fold 2] Ep 16/100 | Train loss 0.5022 acc 86.26% f1M 0.8866 | Val loss 0.7534 acc 78.65% f1M 0.7922 | 8.0s\n",
            "[Fold 2] Ep 17/100 | Train loss 0.4326 acc 89.34% f1M 0.9149 | Val loss 0.7559 acc 78.65% f1M 0.7957 | 6.5s\n",
            "[Fold 2] Ep 18/100 | Train loss 0.5049 acc 87.24% f1M 0.8878 | Val loss 0.7903 acc 79.78% f1M 0.7799 | 12.5s\n",
            "[Fold 2] Ep 19/100 | Train loss 0.3689 acc 89.20% f1M 0.9130 | Val loss 0.7639 acc 76.97% f1M 0.7589 | 8.2s\n",
            "[Fold 2] Ep 20/100 | Train loss 0.3000 acc 92.71% f1M 0.9412 | Val loss 0.7356 acc 79.21% f1M 0.8036 | 7.2s\n",
            "[Fold 2] Ep 21/100 | Train loss 0.2840 acc 93.41% f1M 0.9427 | Val loss 0.7770 acc 80.90% f1M 0.8128 | 8.5s\n",
            "[Fold 2] Ep 22/100 | Train loss 0.2756 acc 94.11% f1M 0.9460 | Val loss 0.8326 acc 81.46% f1M 0.8143 | 6.9s\n",
            "[Fold 2] Ep 23/100 | Train loss 0.1962 acc 94.95% f1M 0.9602 | Val loss 0.7975 acc 81.46% f1M 0.8179 | 8.2s\n",
            "[Fold 2] Ep 24/100 | Train loss 0.2110 acc 93.41% f1M 0.9485 | Val loss 0.7576 acc 82.58% f1M 0.8273 | 7.5s\n",
            "[Fold 2] Ep 25/100 | Train loss 0.2383 acc 96.07% f1M 0.9659 | Val loss 0.8063 acc 82.02% f1M 0.8247 | 7.7s\n",
            "[Fold 2] Ep 26/100 | Train loss 0.2093 acc 95.37% f1M 0.9624 | Val loss 0.7826 acc 80.90% f1M 0.8181 | 8.1s\n",
            "[Fold 2] Ep 27/100 | Train loss 0.1737 acc 95.51% f1M 0.9584 | Val loss 0.7839 acc 78.65% f1M 0.7985 | 6.9s\n",
            "[Fold 2] Ep 28/100 | Train loss 0.1515 acc 96.07% f1M 0.9627 | Val loss 0.8072 acc 80.90% f1M 0.8218 | 9.8s\n",
            "[Fold 2] Ep 29/100 | Train loss 0.1316 acc 96.21% f1M 0.9645 | Val loss 0.8078 acc 79.78% f1M 0.8045 | 7.1s\n",
            "[Fold 2] Ep 30/100 | Train loss 0.1634 acc 96.49% f1M 0.9738 | Val loss 0.8185 acc 80.90% f1M 0.8143 | 8.4s\n",
            "[Fold 2] Ep 31/100 | Train loss 0.1415 acc 97.76% f1M 0.9855 | Val loss 0.7926 acc 79.78% f1M 0.7916 | 6.9s\n",
            "[Fold 2] Ep 32/100 | Train loss 0.1571 acc 96.91% f1M 0.9740 | Val loss 0.8101 acc 82.02% f1M 0.8238 | 8.4s\n",
            "[Fold 2] Ep 33/100 | Train loss 0.1155 acc 96.63% f1M 0.9678 | Val loss 0.8592 acc 82.58% f1M 0.8263 | 7.1s\n",
            "[Fold 2] Ep 34/100 | Train loss 0.1085 acc 97.19% f1M 0.9757 | Val loss 0.8171 acc 81.46% f1M 0.8230 | 8.5s\n",
            "[Fold 2] Ep 35/100 | Train loss 0.1164 acc 96.21% f1M 0.9609 | Val loss 0.8452 acc 80.90% f1M 0.8157 | 7.4s\n",
            "[Fold 2] Ep 36/100 | Train loss 0.1313 acc 97.48% f1M 0.9791 | Val loss 0.8474 acc 83.15% f1M 0.8306 | 7.8s\n",
            "[Fold 2] Ep 37/100 | Train loss 0.0705 acc 98.04% f1M 0.9848 | Val loss 0.8223 acc 82.58% f1M 0.8350 | 8.3s\n",
            "[Fold 2] Ep 38/100 | Train loss 0.1327 acc 98.04% f1M 0.9838 | Val loss 0.8067 acc 83.15% f1M 0.8376 | 7.0s\n",
            "[Fold 2] Ep 39/100 | Train loss 0.1011 acc 98.46% f1M 0.9910 | Val loss 0.8690 acc 82.02% f1M 0.8313 | 8.5s\n",
            "[Fold 2] Ep 40/100 | Train loss 0.1178 acc 97.90% f1M 0.9861 | Val loss 0.9014 acc 79.78% f1M 0.7969 | 7.0s\n",
            "[Fold 2] Ep 41/100 | Train loss 0.0875 acc 97.05% f1M 0.9756 | Val loss 0.9037 acc 80.34% f1M 0.8100 | 8.5s\n",
            "[Fold 2] Ep 42/100 | Train loss 0.1097 acc 96.77% f1M 0.9710 | Val loss 0.8278 acc 80.34% f1M 0.8016 | 6.9s\n",
            "[Fold 2] Ep 43/100 | Train loss 0.0766 acc 98.32% f1M 0.9867 | Val loss 0.8246 acc 80.34% f1M 0.8195 | 8.2s\n",
            "[Fold 2] Ep 44/100 | Train loss 0.1090 acc 97.76% f1M 0.9807 | Val loss 0.8359 acc 80.34% f1M 0.8147 | 7.8s\n",
            "[Fold 2] Ep 45/100 | Train loss 0.0687 acc 98.18% f1M 0.9792 | Val loss 0.8705 acc 80.90% f1M 0.8201 | 7.6s\n",
            "[Fold 2] Ep 46/100 | Train loss 0.0545 acc 98.46% f1M 0.9907 | Val loss 0.8770 acc 79.78% f1M 0.8003 | 8.2s\n",
            "[Fold 2] Ep 47/100 | Train loss 0.0813 acc 98.46% f1M 0.9860 | Val loss 0.8612 acc 79.78% f1M 0.7981 | 7.1s\n",
            "[Fold 2] Ep 48/100 | Train loss 0.0861 acc 98.18% f1M 0.9833 | Val loss 0.9274 acc 79.21% f1M 0.8006 | 8.4s\n",
            "[Fold 2] Ep 49/100 | Train loss 0.0945 acc 97.76% f1M 0.9827 | Val loss 0.9030 acc 78.65% f1M 0.7932 | 7.2s\n",
            "[Fold 2] Ep 50/100 | Train loss 0.0500 acc 99.02% f1M 0.9928 | Val loss 0.8633 acc 81.46% f1M 0.8198 | 8.6s\n",
            "[Fold 2] Ep 51/100 | Train loss 0.0487 acc 99.02% f1M 0.9932 | Val loss 0.8381 acc 83.15% f1M 0.8418 | 7.4s\n",
            "[Fold 2] Ep 52/100 | Train loss 0.0385 acc 99.02% f1M 0.9914 | Val loss 0.8901 acc 83.71% f1M 0.8398 | 7.8s\n",
            "[Fold 2] Ep 53/100 | Train loss 0.0470 acc 98.60% f1M 0.9873 | Val loss 0.9021 acc 82.58% f1M 0.8334 | 8.4s\n",
            "[Fold 2] Ep 54/100 | Train loss 0.0461 acc 99.72% f1M 0.9987 | Val loss 0.9269 acc 80.34% f1M 0.8129 | 7.3s\n",
            "[Fold 2] Ep 55/100 | Train loss 0.0554 acc 98.46% f1M 0.9885 | Val loss 0.9579 acc 79.21% f1M 0.8079 | 8.5s\n",
            "[Fold 2] Ep 56/100 | Train loss 0.0509 acc 99.02% f1M 0.9922 | Val loss 0.9334 acc 80.34% f1M 0.8132 | 7.0s\n",
            "[Fold 2] Ep 57/100 | Train loss 0.1287 acc 98.88% f1M 0.9892 | Val loss 0.9007 acc 80.34% f1M 0.8092 | 8.5s\n",
            "[Fold 2] Ep 58/100 | Train loss 0.0426 acc 98.74% f1M 0.9895 | Val loss 1.0345 acc 79.78% f1M 0.8066 | 7.4s\n",
            "[Fold 2] Ep 59/100 | Train loss 0.0428 acc 98.88% f1M 0.9940 | Val loss 0.9455 acc 80.34% f1M 0.8115 | 8.0s\n",
            "[Fold 2] Ep 60/100 | Train loss 0.1148 acc 97.90% f1M 0.9880 | Val loss 0.9355 acc 79.78% f1M 0.8004 | 7.9s\n",
            "[Fold 2] Ep 61/100 | Train loss 0.0464 acc 99.30% f1M 0.9938 | Val loss 0.9198 acc 80.34% f1M 0.8031 | 7.2s\n",
            "[Fold 2] Ep 62/100 | Train loss 0.0326 acc 99.58% f1M 0.9978 | Val loss 0.9376 acc 80.34% f1M 0.8151 | 8.3s\n",
            "[Fold 2] Ep 63/100 | Train loss 0.0559 acc 98.88% f1M 0.9937 | Val loss 0.8905 acc 82.02% f1M 0.8254 | 7.0s\n",
            "[Fold 2] Ep 64/100 | Train loss 0.0380 acc 99.16% f1M 0.9913 | Val loss 0.9174 acc 80.34% f1M 0.8162 | 8.6s\n",
            "[Fold 2] Ep 65/100 | Train loss 0.1130 acc 98.60% f1M 0.9838 | Val loss 0.8886 acc 80.90% f1M 0.8160 | 7.0s\n",
            "[Fold 2] Ep 66/100 | Train loss 0.0590 acc 98.32% f1M 0.9819 | Val loss 0.9595 acc 80.34% f1M 0.8084 | 8.5s\n",
            "[Fold 2] Ep 67/100 | Train loss 0.0497 acc 98.74% f1M 0.9918 | Val loss 0.9745 acc 80.34% f1M 0.8024 | 7.5s\n",
            "[Fold 2] Ep 68/100 | Train loss 0.0457 acc 98.46% f1M 0.9911 | Val loss 0.9559 acc 78.65% f1M 0.7929 | 7.9s\n",
            "[Fold 2] Ep 69/100 | Train loss 0.0486 acc 99.16% f1M 0.9940 | Val loss 0.9565 acc 78.65% f1M 0.8043 | 8.1s\n",
            "[Fold 2] Ep 70/100 | Train loss 0.0643 acc 99.02% f1M 0.9921 | Val loss 0.9624 acc 79.21% f1M 0.7950 | 7.0s\n",
            "[Fold 2] Ep 71/100 | Train loss 0.0613 acc 98.74% f1M 0.9870 | Val loss 0.9341 acc 80.90% f1M 0.8169 | 8.6s\n",
            "[Fold 2] Ep 72/100 | Train loss 0.0498 acc 99.02% f1M 0.9885 | Val loss 0.9436 acc 80.34% f1M 0.8144 | 7.8s\n",
            "[Fold 2] Ep 73/100 | Train loss 0.0333 acc 99.72% f1M 0.9990 | Val loss 0.9546 acc 79.78% f1M 0.8163 | 9.4s\n",
            "[Fold 2] Ep 74/100 | Train loss 0.0355 acc 99.16% f1M 0.9927 | Val loss 0.9648 acc 80.34% f1M 0.8150 | 7.3s\n",
            "[Fold 2] Ep 75/100 | Train loss 0.0276 acc 99.58% f1M 0.9965 | Val loss 0.9302 acc 80.90% f1M 0.8144 | 8.0s\n",
            "[Fold 2] Ep 76/100 | Train loss 0.0362 acc 99.30% f1M 0.9964 | Val loss 0.9328 acc 80.34% f1M 0.8101 | 8.5s\n",
            "[Fold 2] Ep 77/100 | Train loss 0.0676 acc 99.02% f1M 0.9943 | Val loss 0.9322 acc 80.34% f1M 0.8074 | 7.0s\n",
            "[Fold 2] Ep 78/100 | Train loss 0.0335 acc 99.02% f1M 0.9930 | Val loss 0.9433 acc 80.90% f1M 0.8240 | 8.4s\n",
            "[Fold 2] Ep 79/100 | Train loss 0.0359 acc 98.88% f1M 0.9926 | Val loss 0.9204 acc 81.46% f1M 0.8282 | 7.1s\n",
            "[Fold 2] Ep 80/100 | Train loss 0.0376 acc 98.88% f1M 0.9940 | Val loss 0.9397 acc 80.34% f1M 0.8098 | 8.7s\n",
            "[Fold 2] Ep 81/100 | Train loss 0.0276 acc 99.86% f1M 0.9996 | Val loss 0.9262 acc 80.90% f1M 0.8111 | 7.0s\n",
            "[Fold 2] Ep 82/100 | Train loss 0.0267 acc 99.44% f1M 0.9974 | Val loss 0.9157 acc 80.34% f1M 0.8070 | 8.5s\n",
            "[Fold 2] Ep 83/100 | Train loss 0.0298 acc 99.44% f1M 0.9962 | Val loss 0.9632 acc 79.78% f1M 0.8051 | 7.7s\n",
            "[Fold 2] Ep 84/100 | Train loss 0.0320 acc 99.44% f1M 0.9965 | Val loss 0.9342 acc 79.21% f1M 0.7992 | 7.7s\n",
            "[Fold 2] Ep 85/100 | Train loss 0.0317 acc 99.44% f1M 0.9942 | Val loss 0.9388 acc 79.78% f1M 0.8058 | 8.3s\n",
            "[Fold 2] Ep 86/100 | Train loss 0.0461 acc 98.74% f1M 0.9902 | Val loss 0.9386 acc 80.34% f1M 0.8226 | 7.0s\n",
            "[Fold 2] Ep 87/100 | Train loss 0.0415 acc 99.16% f1M 0.9930 | Val loss 0.9534 acc 80.90% f1M 0.8083 | 8.5s\n",
            "[Fold 2] Ep 88/100 | Train loss 0.0529 acc 99.44% f1M 0.9964 | Val loss 0.9359 acc 79.78% f1M 0.7907 | 7.0s\n",
            "[Fold 2] Ep 89/100 | Train loss 0.0397 acc 99.44% f1M 0.9947 | Val loss 0.9109 acc 81.46% f1M 0.7960 | 8.5s\n",
            "[Fold 2] Ep 90/100 | Train loss 0.0325 acc 99.30% f1M 0.9952 | Val loss 0.9429 acc 81.46% f1M 0.8147 | 7.2s\n",
            "[Fold 2] Ep 91/100 | Train loss 0.0250 acc 99.30% f1M 0.9961 | Val loss 0.9453 acc 82.02% f1M 0.8194 | 8.1s\n",
            "[Fold 2] Ep 92/100 | Train loss 0.0211 acc 99.44% f1M 0.9963 | Val loss 0.9211 acc 82.02% f1M 0.8180 | 7.9s\n",
            "[Fold 2] Ep 93/100 | Train loss 0.0239 acc 99.58% f1M 0.9974 | Val loss 0.9383 acc 82.02% f1M 0.8181 | 7.6s\n",
            "[Fold 2] Ep 94/100 | Train loss 0.0266 acc 99.30% f1M 0.9942 | Val loss 0.9538 acc 81.46% f1M 0.8176 | 8.2s\n",
            "[Fold 2] Ep 95/100 | Train loss 0.0330 acc 99.30% f1M 0.9945 | Val loss 0.9558 acc 82.02% f1M 0.8179 | 7.3s\n",
            "[Fold 2] Ep 96/100 | Train loss 0.0209 acc 99.58% f1M 0.9972 | Val loss 0.9910 acc 82.02% f1M 0.8165 | 8.5s\n",
            "[Fold 2] Ep 97/100 | Train loss 0.0286 acc 99.44% f1M 0.9944 | Val loss 1.0015 acc 80.34% f1M 0.8106 | 6.9s\n",
            "[Fold 2] Ep 98/100 | Train loss 0.0136 acc 99.72% f1M 0.9974 | Val loss 1.0058 acc 82.02% f1M 0.8180 | 8.6s\n",
            "[Fold 2] Ep 99/100 | Train loss 0.0326 acc 99.16% f1M 0.9916 | Val loss 1.0435 acc 81.46% f1M 0.8117 | 7.7s\n",
            "[Fold 2] Ep 100/100 | Train loss 0.0318 acc 99.44% f1M 0.9970 | Val loss 1.0499 acc 81.46% f1M 0.8155 | 7.5s\n",
            "[Fold 2] Best Val â†’ acc 83.15% | f1M 0.8418 (epoch 51)\n",
            "\n",
            "===== K-Fold 3/5 ===== (train 713 | val 178)\n",
            "[Fold 3] Ep 01/100 | Train loss 3.2273 acc 18.65% f1M 0.1328 | Val loss 3.1272 acc 39.89% f1M 0.2998 | 8.8s\n",
            "[Fold 3] Ep 02/100 | Train loss 3.0409 acc 45.16% f1M 0.3687 | Val loss 2.9664 acc 46.63% f1M 0.3575 | 6.6s\n",
            "[Fold 3] Ep 03/100 | Train loss 2.8677 acc 53.58% f1M 0.4686 | Val loss 2.8128 acc 52.25% f1M 0.4078 | 8.5s\n",
            "[Fold 3] Ep 04/100 | Train loss 2.7028 acc 59.33% f1M 0.5132 | Val loss 2.6878 acc 53.37% f1M 0.4152 | 6.8s\n",
            "[Fold 3] Ep 05/100 | Train loss 2.5889 acc 59.19% f1M 0.5284 | Val loss 2.5713 acc 53.93% f1M 0.4333 | 8.0s\n",
            "[Fold 3] Ep 06/100 | Train loss 2.4511 acc 61.57% f1M 0.5481 | Val loss 2.4583 acc 55.62% f1M 0.4577 | 7.4s\n",
            "[Fold 3] Ep 07/100 | Train loss 2.1804 acc 58.91% f1M 0.5271 | Val loss 1.9823 acc 55.62% f1M 0.4558 | 7.4s\n",
            "[Fold 3] Ep 08/100 | Train loss 1.7118 acc 68.30% f1M 0.6595 | Val loss 1.5617 acc 66.29% f1M 0.6475 | 8.0s\n",
            "[Fold 3] Ep 09/100 | Train loss 1.3259 acc 73.77% f1M 0.7571 | Val loss 1.3333 acc 66.85% f1M 0.6437 | 6.9s\n",
            "[Fold 3] Ep 10/100 | Train loss 1.0659 acc 76.72% f1M 0.7608 | Val loss 1.1443 acc 68.54% f1M 0.6710 | 8.3s\n",
            "[Fold 3] Ep 11/100 | Train loss 0.8273 acc 80.50% f1M 0.8208 | Val loss 1.0204 acc 70.79% f1M 0.7326 | 6.9s\n",
            "[Fold 3] Ep 12/100 | Train loss 0.7540 acc 80.79% f1M 0.8312 | Val loss 0.9574 acc 73.03% f1M 0.7554 | 8.1s\n",
            "[Fold 3] Ep 13/100 | Train loss 0.6688 acc 81.91% f1M 0.8481 | Val loss 0.9347 acc 73.03% f1M 0.7447 | 6.9s\n",
            "[Fold 3] Ep 14/100 | Train loss 0.5362 acc 85.69% f1M 0.8695 | Val loss 0.8929 acc 75.84% f1M 0.7775 | 8.3s\n",
            "[Fold 3] Ep 15/100 | Train loss 0.5037 acc 85.97% f1M 0.8730 | Val loss 0.8681 acc 76.97% f1M 0.8064 | 7.4s\n",
            "[Fold 3] Ep 16/100 | Train loss 0.4462 acc 86.68% f1M 0.8912 | Val loss 0.9196 acc 71.91% f1M 0.7336 | 7.9s\n",
            "[Fold 3] Ep 17/100 | Train loss 0.3532 acc 90.60% f1M 0.9244 | Val loss 0.8922 acc 74.16% f1M 0.7538 | 7.5s\n",
            "[Fold 3] Ep 18/100 | Train loss 0.4914 acc 88.50% f1M 0.8963 | Val loss 0.9105 acc 74.16% f1M 0.7689 | 8.9s\n",
            "[Fold 3] Ep 19/100 | Train loss 0.3046 acc 90.60% f1M 0.9053 | Val loss 0.8876 acc 76.40% f1M 0.7764 | 8.7s\n",
            "[Fold 3] Ep 20/100 | Train loss 0.2472 acc 93.41% f1M 0.9445 | Val loss 0.9040 acc 74.72% f1M 0.7698 | 7.1s\n",
            "[Fold 3] Ep 21/100 | Train loss 0.2384 acc 93.97% f1M 0.9586 | Val loss 0.9337 acc 75.84% f1M 0.7781 | 8.6s\n",
            "[Fold 3] Ep 22/100 | Train loss 0.1731 acc 94.81% f1M 0.9582 | Val loss 0.9089 acc 77.53% f1M 0.8040 | 7.1s\n",
            "[Fold 3] Ep 23/100 | Train loss 0.2803 acc 92.71% f1M 0.9394 | Val loss 0.9201 acc 74.72% f1M 0.7867 | 8.4s\n",
            "[Fold 3] Ep 24/100 | Train loss 0.1716 acc 95.51% f1M 0.9648 | Val loss 0.9323 acc 76.97% f1M 0.7947 | 7.7s\n",
            "[Fold 3] Ep 25/100 | Train loss 0.1850 acc 95.65% f1M 0.9662 | Val loss 0.9406 acc 76.97% f1M 0.7959 | 7.6s\n",
            "[Fold 3] Ep 26/100 | Train loss 0.1288 acc 97.05% f1M 0.9751 | Val loss 0.9044 acc 76.97% f1M 0.7942 | 8.4s\n",
            "[Fold 3] Ep 27/100 | Train loss 0.2155 acc 95.79% f1M 0.9664 | Val loss 0.9190 acc 78.65% f1M 0.8085 | 6.9s\n",
            "[Fold 3] Ep 28/100 | Train loss 0.1015 acc 97.76% f1M 0.9844 | Val loss 0.9881 acc 76.40% f1M 0.7865 | 8.4s\n",
            "[Fold 3] Ep 29/100 | Train loss 0.1280 acc 96.07% f1M 0.9688 | Val loss 0.9781 acc 75.84% f1M 0.8046 | 7.2s\n",
            "[Fold 3] Ep 30/100 | Train loss 0.1163 acc 97.05% f1M 0.9779 | Val loss 0.9821 acc 75.84% f1M 0.8075 | 8.6s\n",
            "[Fold 3] Ep 31/100 | Train loss 0.1405 acc 96.49% f1M 0.9666 | Val loss 0.9786 acc 75.28% f1M 0.7728 | 7.4s\n",
            "[Fold 3] Ep 32/100 | Train loss 0.1392 acc 96.77% f1M 0.9709 | Val loss 1.0175 acc 75.84% f1M 0.7845 | 7.8s\n",
            "[Fold 3] Ep 33/100 | Train loss 0.1116 acc 97.34% f1M 0.9777 | Val loss 0.9791 acc 77.53% f1M 0.7972 | 8.2s\n",
            "[Fold 3] Ep 34/100 | Train loss 0.0970 acc 97.76% f1M 0.9828 | Val loss 0.9911 acc 76.97% f1M 0.8033 | 7.1s\n",
            "[Fold 3] Ep 35/100 | Train loss 0.0865 acc 97.76% f1M 0.9831 | Val loss 1.0022 acc 78.09% f1M 0.7959 | 8.4s\n",
            "[Fold 3] Ep 36/100 | Train loss 0.1221 acc 97.34% f1M 0.9778 | Val loss 0.9843 acc 78.65% f1M 0.7997 | 7.2s\n",
            "[Fold 3] Ep 37/100 | Train loss 0.0973 acc 98.32% f1M 0.9881 | Val loss 0.9641 acc 78.09% f1M 0.7937 | 8.6s\n",
            "[Fold 3] Ep 38/100 | Train loss 0.0900 acc 98.46% f1M 0.9899 | Val loss 1.0521 acc 75.28% f1M 0.7725 | 7.7s\n",
            "[Fold 3] Ep 39/100 | Train loss 0.0670 acc 98.18% f1M 0.9858 | Val loss 1.0090 acc 76.97% f1M 0.7792 | 7.9s\n",
            "[Fold 3] Ep 40/100 | Train loss 0.0626 acc 98.60% f1M 0.9871 | Val loss 0.9636 acc 79.78% f1M 0.8179 | 8.5s\n",
            "[Fold 3] Ep 41/100 | Train loss 0.0449 acc 99.44% f1M 0.9953 | Val loss 0.9857 acc 78.65% f1M 0.8132 | 7.2s\n",
            "[Fold 3] Ep 42/100 | Train loss 0.0593 acc 98.88% f1M 0.9877 | Val loss 0.9787 acc 79.78% f1M 0.8197 | 8.6s\n",
            "[Fold 3] Ep 43/100 | Train loss 0.1269 acc 98.32% f1M 0.9847 | Val loss 1.0497 acc 78.65% f1M 0.8025 | 7.2s\n",
            "[Fold 3] Ep 44/100 | Train loss 0.0942 acc 97.34% f1M 0.9753 | Val loss 1.0840 acc 78.09% f1M 0.7986 | 8.7s\n",
            "[Fold 3] Ep 45/100 | Train loss 0.0794 acc 97.62% f1M 0.9770 | Val loss 1.0333 acc 80.34% f1M 0.8115 | 7.5s\n",
            "[Fold 3] Ep 46/100 | Train loss 0.0823 acc 98.32% f1M 0.9861 | Val loss 1.0328 acc 78.09% f1M 0.7832 | 8.3s\n",
            "[Fold 3] Ep 47/100 | Train loss 0.0628 acc 97.62% f1M 0.9825 | Val loss 0.9943 acc 79.78% f1M 0.8077 | 7.7s\n",
            "[Fold 3] Ep 48/100 | Train loss 0.0540 acc 98.74% f1M 0.9898 | Val loss 1.0244 acc 79.21% f1M 0.8053 | 7.4s\n",
            "[Fold 3] Ep 49/100 | Train loss 0.0734 acc 97.62% f1M 0.9727 | Val loss 1.0112 acc 80.34% f1M 0.8052 | 8.3s\n",
            "[Fold 3] Ep 50/100 | Train loss 0.0557 acc 99.30% f1M 0.9961 | Val loss 1.0010 acc 79.78% f1M 0.8024 | 7.1s\n",
            "[Fold 3] Ep 51/100 | Train loss 0.0698 acc 97.76% f1M 0.9847 | Val loss 1.0646 acc 79.21% f1M 0.7954 | 8.4s\n",
            "[Fold 3] Ep 52/100 | Train loss 0.0491 acc 98.88% f1M 0.9923 | Val loss 1.0474 acc 79.21% f1M 0.7965 | 6.9s\n",
            "[Fold 3] Ep 53/100 | Train loss 0.0599 acc 98.18% f1M 0.9887 | Val loss 1.0523 acc 79.21% f1M 0.7982 | 8.5s\n",
            "[Fold 3] Ep 54/100 | Train loss 0.0454 acc 98.18% f1M 0.9836 | Val loss 1.0194 acc 80.90% f1M 0.8181 | 7.4s\n",
            "[Fold 3] Ep 55/100 | Train loss 0.0541 acc 98.32% f1M 0.9849 | Val loss 1.0202 acc 80.90% f1M 0.8077 | 7.8s\n",
            "[Fold 3] Ep 56/100 | Train loss 0.0351 acc 99.02% f1M 0.9949 | Val loss 1.0376 acc 78.65% f1M 0.7980 | 8.2s\n",
            "[Fold 3] Ep 57/100 | Train loss 0.0797 acc 98.04% f1M 0.9850 | Val loss 1.0645 acc 79.78% f1M 0.8130 | 6.9s\n",
            "[Fold 3] Ep 58/100 | Train loss 0.0428 acc 98.74% f1M 0.9897 | Val loss 1.0399 acc 76.97% f1M 0.7949 | 8.8s\n",
            "[Fold 3] Ep 59/100 | Train loss 0.1059 acc 98.18% f1M 0.9838 | Val loss 0.9998 acc 78.09% f1M 0.8054 | 7.1s\n",
            "[Fold 3] Ep 60/100 | Train loss 0.0453 acc 99.16% f1M 0.9891 | Val loss 1.0395 acc 78.65% f1M 0.8193 | 8.5s\n",
            "[Fold 3] Ep 61/100 | Train loss 0.0604 acc 99.16% f1M 0.9952 | Val loss 1.0672 acc 79.21% f1M 0.8187 | 7.1s\n",
            "[Fold 3] Ep 62/100 | Train loss 0.0450 acc 98.88% f1M 0.9914 | Val loss 1.0078 acc 80.90% f1M 0.8279 | 8.4s\n",
            "[Fold 3] Ep 63/100 | Train loss 0.0409 acc 98.88% f1M 0.9934 | Val loss 1.0340 acc 79.78% f1M 0.8153 | 9.6s\n",
            "[Fold 3] Ep 64/100 | Train loss 0.0411 acc 99.16% f1M 0.9936 | Val loss 1.0540 acc 80.34% f1M 0.8122 | 7.1s\n",
            "[Fold 3] Ep 65/100 | Train loss 0.0304 acc 99.30% f1M 0.9969 | Val loss 1.1101 acc 79.78% f1M 0.8086 | 8.6s\n",
            "[Fold 3] Ep 66/100 | Train loss 0.0701 acc 98.60% f1M 0.9894 | Val loss 1.1515 acc 80.90% f1M 0.8273 | 6.9s\n",
            "[Fold 3] Ep 67/100 | Train loss 0.0323 acc 99.30% f1M 0.9955 | Val loss 1.0633 acc 79.21% f1M 0.8128 | 8.5s\n",
            "[Fold 3] Ep 68/100 | Train loss 0.0639 acc 98.60% f1M 0.9903 | Val loss 1.0358 acc 79.78% f1M 0.8203 | 7.5s\n",
            "[Fold 3] Ep 69/100 | Train loss 0.0569 acc 99.16% f1M 0.9941 | Val loss 1.0831 acc 77.53% f1M 0.7956 | 8.3s\n",
            "[Fold 3] Ep 70/100 | Train loss 0.0472 acc 98.18% f1M 0.9794 | Val loss 1.0740 acc 78.09% f1M 0.8070 | 7.8s\n",
            "[Fold 3] Ep 71/100 | Train loss 0.0685 acc 98.74% f1M 0.9928 | Val loss 1.1144 acc 78.09% f1M 0.8017 | 7.2s\n",
            "[Fold 3] Ep 72/100 | Train loss 0.0398 acc 98.74% f1M 0.9912 | Val loss 1.1719 acc 77.53% f1M 0.8022 | 8.4s\n",
            "[Fold 3] Ep 73/100 | Train loss 0.0352 acc 99.02% f1M 0.9900 | Val loss 1.1121 acc 77.53% f1M 0.8008 | 7.0s\n",
            "[Fold 3] Ep 74/100 | Train loss 0.0416 acc 98.74% f1M 0.9927 | Val loss 1.0362 acc 78.65% f1M 0.8158 | 8.4s\n",
            "[Fold 3] Ep 75/100 | Train loss 0.0272 acc 99.30% f1M 0.9948 | Val loss 1.0385 acc 78.65% f1M 0.8081 | 7.0s\n",
            "[Fold 3] Ep 76/100 | Train loss 0.0333 acc 99.16% f1M 0.9949 | Val loss 1.0352 acc 80.90% f1M 0.8293 | 8.6s\n",
            "[Fold 3] Ep 77/100 | Train loss 0.0232 acc 99.58% f1M 0.9960 | Val loss 1.1025 acc 79.21% f1M 0.8059 | 7.5s\n",
            "[Fold 3] Ep 78/100 | Train loss 0.0262 acc 99.58% f1M 0.9971 | Val loss 1.1241 acc 77.53% f1M 0.7895 | 7.7s\n",
            "[Fold 3] Ep 79/100 | Train loss 0.0239 acc 99.44% f1M 0.9967 | Val loss 1.1681 acc 76.97% f1M 0.7989 | 8.1s\n",
            "[Fold 3] Ep 80/100 | Train loss 0.0239 acc 99.30% f1M 0.9926 | Val loss 1.1611 acc 77.53% f1M 0.7982 | 7.0s\n",
            "[Fold 3] Ep 81/100 | Train loss 0.0580 acc 98.32% f1M 0.9851 | Val loss 1.1433 acc 78.09% f1M 0.7971 | 8.5s\n",
            "[Fold 3] Ep 82/100 | Train loss 0.0232 acc 99.72% f1M 0.9992 | Val loss 1.1252 acc 79.78% f1M 0.8182 | 6.8s\n",
            "[Fold 3] Ep 83/100 | Train loss 0.0351 acc 98.88% f1M 0.9923 | Val loss 1.1130 acc 79.78% f1M 0.8199 | 8.4s\n",
            "[Fold 3] Ep 84/100 | Train loss 0.0400 acc 99.30% f1M 0.9965 | Val loss 1.1059 acc 79.78% f1M 0.8180 | 7.1s\n",
            "[Fold 3] Ep 85/100 | Train loss 0.0246 acc 99.16% f1M 0.9922 | Val loss 1.0821 acc 77.53% f1M 0.8065 | 8.3s\n",
            "[Fold 3] Ep 86/100 | Train loss 0.0377 acc 99.30% f1M 0.9954 | Val loss 1.0581 acc 78.65% f1M 0.8098 | 7.3s\n",
            "[Fold 3] Ep 87/100 | Train loss 0.0378 acc 99.16% f1M 0.9925 | Val loss 1.0673 acc 76.40% f1M 0.7832 | 7.7s\n",
            "[Fold 3] Ep 88/100 | Train loss 0.0729 acc 98.46% f1M 0.9863 | Val loss 1.1386 acc 79.78% f1M 0.8185 | 7.6s\n",
            "[Fold 3] Ep 89/100 | Train loss 0.0803 acc 98.88% f1M 0.9900 | Val loss 1.1044 acc 79.78% f1M 0.8195 | 7.3s\n",
            "[Fold 3] Ep 90/100 | Train loss 0.0337 acc 98.88% f1M 0.9917 | Val loss 1.1187 acc 77.53% f1M 0.7872 | 8.3s\n",
            "[Fold 3] Ep 91/100 | Train loss 0.0463 acc 99.02% f1M 0.9923 | Val loss 1.0738 acc 78.09% f1M 0.8016 | 6.9s\n",
            "[Fold 3] Ep 92/100 | Train loss 0.0297 acc 99.16% f1M 0.9950 | Val loss 1.0625 acc 78.65% f1M 0.8149 | 8.7s\n",
            "[Fold 3] Ep 93/100 | Train loss 0.0648 acc 98.46% f1M 0.9887 | Val loss 1.0242 acc 79.78% f1M 0.8160 | 7.1s\n",
            "[Fold 3] Ep 94/100 | Train loss 0.0327 acc 98.88% f1M 0.9946 | Val loss 1.0376 acc 79.78% f1M 0.8178 | 8.3s\n",
            "[Fold 3] Ep 95/100 | Train loss 0.0243 acc 99.02% f1M 0.9926 | Val loss 1.0766 acc 77.53% f1M 0.8033 | 7.1s\n",
            "[Fold 3] Ep 96/100 | Train loss 0.0265 acc 99.30% f1M 0.9966 | Val loss 1.0914 acc 76.97% f1M 0.7890 | 8.4s\n",
            "[Fold 3] Ep 97/100 | Train loss 0.0371 acc 99.16% f1M 0.9945 | Val loss 1.0705 acc 78.09% f1M 0.7976 | 7.7s\n",
            "[Fold 3] Ep 98/100 | Train loss 0.0133 acc 99.72% f1M 0.9987 | Val loss 1.1272 acc 81.46% f1M 0.8336 | 7.4s\n",
            "[Fold 3] Ep 99/100 | Train loss 0.0492 acc 99.30% f1M 0.9960 | Val loss 1.1739 acc 80.34% f1M 0.8297 | 8.2s\n",
            "[Fold 3] Ep 100/100 | Train loss 0.0432 acc 99.58% f1M 0.9986 | Val loss 1.2447 acc 78.65% f1M 0.8083 | 7.1s\n",
            "[Fold 3] Best Val â†’ acc 81.46% | f1M 0.8336 (epoch 98)\n",
            "\n",
            "===== K-Fold 4/5 ===== (train 713 | val 178)\n",
            "[Fold 4] Ep 01/100 | Train loss 3.2325 acc 17.11% f1M 0.1429 | Val loss 3.1237 acc 47.75% f1M 0.3778 | 8.4s\n",
            "[Fold 4] Ep 02/100 | Train loss 3.0465 acc 45.72% f1M 0.3795 | Val loss 2.9607 acc 51.12% f1M 0.4235 | 7.6s\n",
            "[Fold 4] Ep 03/100 | Train loss 2.8833 acc 55.12% f1M 0.5136 | Val loss 2.8250 acc 55.06% f1M 0.4643 | 6.8s\n",
            "[Fold 4] Ep 04/100 | Train loss 2.7418 acc 57.92% f1M 0.5109 | Val loss 2.7000 acc 56.18% f1M 0.4673 | 8.1s\n",
            "[Fold 4] Ep 05/100 | Train loss 2.6005 acc 63.39% f1M 0.5809 | Val loss 2.5881 acc 55.62% f1M 0.4705 | 6.6s\n",
            "[Fold 4] Ep 06/100 | Train loss 2.4546 acc 65.64% f1M 0.6261 | Val loss 2.4772 acc 56.74% f1M 0.4857 | 8.1s\n",
            "[Fold 4] Ep 07/100 | Train loss 2.1850 acc 61.43% f1M 0.5404 | Val loss 1.9637 acc 58.43% f1M 0.5004 | 6.8s\n",
            "[Fold 4] Ep 08/100 | Train loss 1.7014 acc 69.57% f1M 0.6966 | Val loss 1.5360 acc 70.79% f1M 0.6941 | 8.6s\n",
            "[Fold 4] Ep 09/100 | Train loss 1.3438 acc 76.72% f1M 0.7787 | Val loss 1.2554 acc 72.47% f1M 0.7382 | 8.1s\n",
            "[Fold 4] Ep 10/100 | Train loss 1.0680 acc 77.70% f1M 0.7959 | Val loss 1.0691 acc 75.28% f1M 0.7936 | 8.0s\n",
            "[Fold 4] Ep 11/100 | Train loss 0.9116 acc 79.94% f1M 0.8097 | Val loss 0.9365 acc 75.84% f1M 0.7912 | 7.4s\n",
            "[Fold 4] Ep 12/100 | Train loss 0.7201 acc 83.59% f1M 0.8464 | Val loss 0.8571 acc 74.72% f1M 0.7859 | 7.4s\n",
            "[Fold 4] Ep 13/100 | Train loss 0.7411 acc 84.01% f1M 0.8690 | Val loss 0.7780 acc 75.28% f1M 0.7846 | 8.0s\n",
            "[Fold 4] Ep 14/100 | Train loss 0.6776 acc 83.73% f1M 0.8621 | Val loss 0.7683 acc 75.84% f1M 0.7857 | 6.8s\n",
            "[Fold 4] Ep 15/100 | Train loss 0.5267 acc 87.10% f1M 0.8873 | Val loss 0.7552 acc 76.40% f1M 0.7974 | 8.1s\n",
            "[Fold 4] Ep 16/100 | Train loss 0.4858 acc 89.90% f1M 0.9224 | Val loss 0.7140 acc 77.53% f1M 0.7965 | 6.8s\n",
            "[Fold 4] Ep 17/100 | Train loss 0.4210 acc 87.80% f1M 0.8975 | Val loss 0.7164 acc 76.97% f1M 0.7805 | 8.3s\n",
            "[Fold 4] Ep 18/100 | Train loss 0.4398 acc 88.08% f1M 0.8906 | Val loss 0.7224 acc 76.40% f1M 0.7929 | 7.0s\n",
            "[Fold 4] Ep 19/100 | Train loss 0.3220 acc 90.60% f1M 0.9221 | Val loss 0.7024 acc 78.09% f1M 0.7945 | 8.2s\n",
            "[Fold 4] Ep 20/100 | Train loss 0.2446 acc 93.41% f1M 0.9379 | Val loss 0.6984 acc 77.53% f1M 0.7841 | 7.3s\n",
            "[Fold 4] Ep 21/100 | Train loss 0.2643 acc 92.85% f1M 0.9340 | Val loss 0.7274 acc 79.21% f1M 0.8159 | 8.0s\n",
            "[Fold 4] Ep 22/100 | Train loss 0.2171 acc 93.69% f1M 0.9431 | Val loss 0.7053 acc 79.21% f1M 0.8024 | 7.8s\n",
            "[Fold 4] Ep 23/100 | Train loss 0.1664 acc 95.37% f1M 0.9661 | Val loss 0.7248 acc 78.65% f1M 0.8046 | 7.1s\n",
            "[Fold 4] Ep 24/100 | Train loss 0.1588 acc 95.51% f1M 0.9625 | Val loss 0.7318 acc 79.21% f1M 0.8139 | 8.3s\n",
            "[Fold 4] Ep 25/100 | Train loss 0.1680 acc 96.35% f1M 0.9720 | Val loss 0.7318 acc 79.78% f1M 0.8182 | 7.0s\n",
            "[Fold 4] Ep 26/100 | Train loss 0.1452 acc 96.49% f1M 0.9694 | Val loss 0.7735 acc 78.65% f1M 0.7999 | 8.4s\n",
            "[Fold 4] Ep 27/100 | Train loss 0.1062 acc 97.19% f1M 0.9765 | Val loss 0.7601 acc 78.65% f1M 0.7986 | 6.8s\n",
            "[Fold 4] Ep 28/100 | Train loss 0.1835 acc 96.63% f1M 0.9633 | Val loss 0.7771 acc 78.65% f1M 0.7937 | 8.5s\n",
            "[Fold 4] Ep 29/100 | Train loss 0.1624 acc 96.49% f1M 0.9700 | Val loss 0.7871 acc 78.09% f1M 0.8057 | 7.0s\n",
            "[Fold 4] Ep 30/100 | Train loss 0.1364 acc 97.19% f1M 0.9772 | Val loss 0.7545 acc 78.65% f1M 0.8113 | 8.3s\n",
            "[Fold 4] Ep 31/100 | Train loss 0.0871 acc 97.76% f1M 0.9823 | Val loss 0.7001 acc 79.21% f1M 0.8194 | 7.5s\n",
            "[Fold 4] Ep 32/100 | Train loss 0.0989 acc 97.90% f1M 0.9839 | Val loss 0.7339 acc 78.09% f1M 0.7922 | 7.5s\n",
            "[Fold 4] Ep 33/100 | Train loss 0.0753 acc 98.46% f1M 0.9853 | Val loss 0.7545 acc 78.09% f1M 0.8081 | 8.2s\n",
            "[Fold 4] Ep 34/100 | Train loss 0.0807 acc 98.04% f1M 0.9859 | Val loss 0.8015 acc 78.09% f1M 0.7946 | 7.1s\n",
            "[Fold 4] Ep 35/100 | Train loss 0.0845 acc 97.76% f1M 0.9809 | Val loss 0.8045 acc 78.65% f1M 0.8080 | 8.5s\n",
            "[Fold 4] Ep 36/100 | Train loss 0.0838 acc 97.34% f1M 0.9778 | Val loss 0.8093 acc 77.53% f1M 0.7963 | 6.9s\n",
            "[Fold 4] Ep 37/100 | Train loss 0.0767 acc 97.05% f1M 0.9728 | Val loss 0.8733 acc 78.09% f1M 0.8022 | 8.3s\n",
            "[Fold 4] Ep 38/100 | Train loss 0.0898 acc 98.04% f1M 0.9840 | Val loss 0.8464 acc 77.53% f1M 0.7976 | 7.0s\n",
            "[Fold 4] Ep 39/100 | Train loss 0.0792 acc 97.90% f1M 0.9820 | Val loss 0.8144 acc 78.09% f1M 0.8024 | 8.3s\n",
            "[Fold 4] Ep 40/100 | Train loss 0.0687 acc 98.46% f1M 0.9882 | Val loss 0.8543 acc 79.78% f1M 0.8140 | 7.6s\n",
            "[Fold 4] Ep 41/100 | Train loss 0.0703 acc 98.18% f1M 0.9814 | Val loss 0.8394 acc 79.21% f1M 0.8098 | 7.5s\n",
            "[Fold 4] Ep 42/100 | Train loss 0.0932 acc 97.48% f1M 0.9810 | Val loss 0.8542 acc 76.97% f1M 0.8020 | 8.0s\n",
            "[Fold 4] Ep 43/100 | Train loss 0.0688 acc 98.60% f1M 0.9863 | Val loss 0.8812 acc 78.65% f1M 0.8120 | 6.8s\n",
            "[Fold 4] Ep 44/100 | Train loss 0.0783 acc 97.48% f1M 0.9822 | Val loss 0.8735 acc 76.97% f1M 0.8239 | 8.5s\n",
            "[Fold 4] Ep 45/100 | Train loss 0.0616 acc 98.46% f1M 0.9861 | Val loss 0.8199 acc 78.09% f1M 0.8051 | 7.0s\n",
            "[Fold 4] Ep 46/100 | Train loss 0.0526 acc 98.60% f1M 0.9892 | Val loss 0.8415 acc 78.65% f1M 0.8105 | 8.4s\n",
            "[Fold 4] Ep 47/100 | Train loss 0.0467 acc 98.74% f1M 0.9885 | Val loss 0.8536 acc 77.53% f1M 0.7836 | 6.9s\n",
            "[Fold 4] Ep 48/100 | Train loss 0.0441 acc 99.16% f1M 0.9934 | Val loss 0.8065 acc 76.97% f1M 0.7854 | 8.4s\n",
            "[Fold 4] Ep 49/100 | Train loss 0.0401 acc 99.02% f1M 0.9948 | Val loss 0.8290 acc 79.21% f1M 0.8165 | 7.3s\n",
            "[Fold 4] Ep 50/100 | Train loss 0.0347 acc 99.30% f1M 0.9948 | Val loss 0.8653 acc 79.21% f1M 0.8140 | 7.7s\n",
            "[Fold 4] Ep 51/100 | Train loss 0.0659 acc 98.04% f1M 0.9832 | Val loss 0.9206 acc 77.53% f1M 0.8048 | 7.8s\n",
            "[Fold 4] Ep 52/100 | Train loss 0.0560 acc 98.32% f1M 0.9837 | Val loss 0.9500 acc 78.09% f1M 0.7907 | 7.1s\n",
            "[Fold 4] Ep 53/100 | Train loss 0.0448 acc 98.46% f1M 0.9885 | Val loss 0.9021 acc 78.65% f1M 0.8034 | 8.1s\n",
            "[Fold 4] Ep 54/100 | Train loss 0.0609 acc 98.46% f1M 0.9866 | Val loss 0.9072 acc 79.21% f1M 0.8142 | 6.9s\n",
            "[Fold 4] Ep 55/100 | Train loss 0.0812 acc 98.04% f1M 0.9851 | Val loss 0.9335 acc 79.21% f1M 0.8163 | 8.4s\n",
            "[Fold 4] Ep 56/100 | Train loss 0.0582 acc 98.04% f1M 0.9874 | Val loss 0.9430 acc 79.21% f1M 0.8207 | 8.5s\n",
            "[Fold 4] Ep 57/100 | Train loss 0.0629 acc 98.32% f1M 0.9901 | Val loss 0.9200 acc 80.34% f1M 0.8195 | 8.1s\n",
            "[Fold 4] Ep 58/100 | Train loss 0.0348 acc 98.88% f1M 0.9862 | Val loss 0.9187 acc 77.53% f1M 0.7976 | 7.7s\n",
            "[Fold 4] Ep 59/100 | Train loss 0.1055 acc 99.16% f1M 0.9938 | Val loss 0.9302 acc 78.09% f1M 0.8036 | 7.4s\n",
            "[Fold 4] Ep 60/100 | Train loss 0.0897 acc 98.46% f1M 0.9889 | Val loss 0.8682 acc 79.21% f1M 0.8107 | 8.2s\n",
            "[Fold 4] Ep 61/100 | Train loss 0.0553 acc 98.18% f1M 0.9873 | Val loss 0.9427 acc 76.40% f1M 0.7728 | 7.0s\n",
            "[Fold 4] Ep 62/100 | Train loss 0.0691 acc 97.76% f1M 0.9831 | Val loss 0.8485 acc 76.97% f1M 0.7901 | 8.5s\n",
            "[Fold 4] Ep 63/100 | Train loss 0.0672 acc 98.60% f1M 0.9919 | Val loss 0.7977 acc 78.65% f1M 0.8150 | 6.9s\n",
            "[Fold 4] Ep 64/100 | Train loss 0.0568 acc 99.02% f1M 0.9918 | Val loss 0.7576 acc 81.46% f1M 0.8253 | 8.3s\n",
            "[Fold 4] Ep 65/100 | Train loss 0.0655 acc 98.46% f1M 0.9875 | Val loss 0.7763 acc 80.34% f1M 0.8188 | 7.3s\n",
            "[Fold 4] Ep 66/100 | Train loss 0.0378 acc 99.58% f1M 0.9974 | Val loss 0.8155 acc 79.21% f1M 0.8052 | 7.8s\n",
            "[Fold 4] Ep 67/100 | Train loss 0.0502 acc 98.46% f1M 0.9867 | Val loss 0.8254 acc 78.65% f1M 0.8098 | 7.5s\n",
            "[Fold 4] Ep 68/100 | Train loss 0.0509 acc 99.02% f1M 0.9882 | Val loss 0.8407 acc 76.97% f1M 0.8083 | 7.4s\n",
            "[Fold 4] Ep 69/100 | Train loss 0.0441 acc 98.60% f1M 0.9877 | Val loss 0.8701 acc 78.65% f1M 0.8083 | 8.2s\n",
            "[Fold 4] Ep 70/100 | Train loss 0.0275 acc 99.02% f1M 0.9889 | Val loss 0.8702 acc 78.65% f1M 0.8146 | 6.8s\n",
            "[Fold 4] Ep 71/100 | Train loss 0.0356 acc 99.30% f1M 0.9957 | Val loss 0.8657 acc 80.34% f1M 0.8285 | 8.2s\n",
            "[Fold 4] Ep 72/100 | Train loss 0.0540 acc 99.16% f1M 0.9924 | Val loss 0.8553 acc 78.65% f1M 0.8114 | 7.1s\n",
            "[Fold 4] Ep 73/100 | Train loss 0.0337 acc 99.02% f1M 0.9947 | Val loss 0.8620 acc 78.65% f1M 0.8156 | 8.4s\n",
            "[Fold 4] Ep 74/100 | Train loss 0.0483 acc 98.32% f1M 0.9864 | Val loss 0.8827 acc 78.65% f1M 0.8177 | 7.0s\n",
            "[Fold 4] Ep 75/100 | Train loss 0.0432 acc 98.74% f1M 0.9900 | Val loss 0.8556 acc 79.78% f1M 0.8223 | 8.3s\n",
            "[Fold 4] Ep 76/100 | Train loss 0.1189 acc 99.02% f1M 0.9942 | Val loss 0.8399 acc 79.21% f1M 0.8205 | 7.5s\n",
            "[Fold 4] Ep 77/100 | Train loss 0.0550 acc 98.46% f1M 0.9876 | Val loss 0.8255 acc 77.53% f1M 0.8104 | 7.5s\n",
            "[Fold 4] Ep 78/100 | Train loss 0.0429 acc 98.46% f1M 0.9881 | Val loss 0.8137 acc 77.53% f1M 0.8157 | 8.0s\n",
            "[Fold 4] Ep 79/100 | Train loss 0.0250 acc 99.72% f1M 0.9982 | Val loss 0.7802 acc 78.09% f1M 0.8084 | 6.9s\n",
            "[Fold 4] Ep 80/100 | Train loss 0.0494 acc 98.74% f1M 0.9874 | Val loss 0.7942 acc 78.65% f1M 0.8195 | 8.4s\n",
            "[Fold 4] Ep 81/100 | Train loss 0.0257 acc 99.30% f1M 0.9956 | Val loss 0.7721 acc 78.09% f1M 0.8065 | 7.0s\n",
            "[Fold 4] Ep 82/100 | Train loss 0.0895 acc 98.88% f1M 0.9897 | Val loss 0.7914 acc 80.34% f1M 0.8406 | 8.3s\n",
            "[Fold 4] Ep 83/100 | Train loss 0.0530 acc 99.02% f1M 0.9921 | Val loss 0.8479 acc 77.53% f1M 0.8153 | 6.9s\n",
            "[Fold 4] Ep 84/100 | Train loss 0.0692 acc 97.62% f1M 0.9784 | Val loss 0.8175 acc 78.09% f1M 0.8155 | 8.3s\n",
            "[Fold 4] Ep 85/100 | Train loss 0.0747 acc 98.74% f1M 0.9886 | Val loss 0.7616 acc 78.65% f1M 0.8223 | 7.0s\n",
            "[Fold 4] Ep 86/100 | Train loss 0.0291 acc 99.16% f1M 0.9942 | Val loss 0.8596 acc 78.65% f1M 0.8150 | 8.1s\n",
            "[Fold 4] Ep 87/100 | Train loss 0.0394 acc 99.30% f1M 0.9946 | Val loss 0.8740 acc 78.09% f1M 0.8223 | 7.6s\n",
            "[Fold 4] Ep 88/100 | Train loss 0.0388 acc 98.88% f1M 0.9921 | Val loss 0.8624 acc 78.65% f1M 0.8181 | 7.5s\n",
            "[Fold 4] Ep 89/100 | Train loss 0.0454 acc 99.30% f1M 0.9937 | Val loss 0.8641 acc 77.53% f1M 0.8150 | 8.1s\n",
            "[Fold 4] Ep 90/100 | Train loss 0.0193 acc 99.72% f1M 0.9983 | Val loss 0.8472 acc 78.65% f1M 0.8041 | 6.8s\n",
            "[Fold 4] Ep 91/100 | Train loss 0.0454 acc 99.58% f1M 0.9973 | Val loss 0.8546 acc 78.09% f1M 0.7989 | 8.3s\n",
            "[Fold 4] Ep 92/100 | Train loss 0.0377 acc 99.30% f1M 0.9935 | Val loss 0.8906 acc 78.65% f1M 0.8147 | 7.0s\n",
            "[Fold 4] Ep 93/100 | Train loss 0.0221 acc 99.44% f1M 0.9965 | Val loss 0.9157 acc 78.65% f1M 0.8096 | 8.5s\n",
            "[Fold 4] Ep 94/100 | Train loss 0.0621 acc 98.74% f1M 0.9880 | Val loss 0.9387 acc 76.97% f1M 0.8033 | 6.9s\n",
            "[Fold 4] Ep 95/100 | Train loss 0.0256 acc 99.30% f1M 0.9934 | Val loss 0.9136 acc 79.21% f1M 0.8137 | 8.3s\n",
            "[Fold 4] Ep 96/100 | Train loss 0.0196 acc 99.58% f1M 0.9975 | Val loss 0.9386 acc 77.53% f1M 0.8037 | 7.2s\n",
            "[Fold 4] Ep 97/100 | Train loss 0.0233 acc 99.72% f1M 0.9984 | Val loss 0.9035 acc 78.65% f1M 0.8027 | 7.7s\n",
            "[Fold 4] Ep 98/100 | Train loss 0.0372 acc 98.74% f1M 0.9888 | Val loss 0.8974 acc 77.53% f1M 0.7977 | 7.8s\n",
            "[Fold 4] Ep 99/100 | Train loss 0.0237 acc 99.30% f1M 0.9959 | Val loss 0.9041 acc 78.65% f1M 0.8142 | 7.1s\n",
            "[Fold 4] Ep 100/100 | Train loss 0.0310 acc 99.58% f1M 0.9969 | Val loss 0.9292 acc 79.78% f1M 0.8228 | 8.2s\n",
            "[Fold 4] Best Val â†’ acc 80.34% | f1M 0.8406 (epoch 82)\n",
            "\n",
            "===== K-Fold 5/5 ===== (train 713 | val 178)\n",
            "[Fold 5] Ep 01/100 | Train loss 3.2254 acc 18.23% f1M 0.1294 | Val loss 3.1228 acc 43.82% f1M 0.3120 | 8.2s\n",
            "[Fold 5] Ep 02/100 | Train loss 3.0265 acc 45.44% f1M 0.3634 | Val loss 2.9518 acc 52.25% f1M 0.4223 | 7.6s\n",
            "[Fold 5] Ep 03/100 | Train loss 2.8850 acc 51.61% f1M 0.4519 | Val loss 2.8219 acc 54.49% f1M 0.4548 | 9.9s\n",
            "[Fold 5] Ep 04/100 | Train loss 2.7190 acc 55.96% f1M 0.4747 | Val loss 2.6833 acc 55.06% f1M 0.4714 | 6.9s\n",
            "[Fold 5] Ep 05/100 | Train loss 2.5952 acc 60.87% f1M 0.5473 | Val loss 2.5675 acc 54.49% f1M 0.4737 | 8.2s\n",
            "[Fold 5] Ep 06/100 | Train loss 2.4711 acc 63.25% f1M 0.5912 | Val loss 2.4571 acc 57.87% f1M 0.5160 | 6.9s\n",
            "[Fold 5] Ep 07/100 | Train loss 2.2004 acc 58.77% f1M 0.5309 | Val loss 1.9882 acc 52.81% f1M 0.4919 | 8.4s\n",
            "[Fold 5] Ep 08/100 | Train loss 1.6890 acc 70.69% f1M 0.6863 | Val loss 1.5829 acc 63.48% f1M 0.6079 | 6.8s\n",
            "[Fold 5] Ep 09/100 | Train loss 1.3809 acc 73.07% f1M 0.7244 | Val loss 1.2966 acc 69.66% f1M 0.6964 | 8.3s\n",
            "[Fold 5] Ep 10/100 | Train loss 1.1233 acc 75.88% f1M 0.7607 | Val loss 1.1172 acc 72.47% f1M 0.7587 | 7.3s\n",
            "[Fold 5] Ep 11/100 | Train loss 0.8689 acc 80.79% f1M 0.8292 | Val loss 1.0039 acc 72.47% f1M 0.7627 | 8.1s\n",
            "[Fold 5] Ep 12/100 | Train loss 0.8803 acc 80.65% f1M 0.8261 | Val loss 0.9397 acc 73.03% f1M 0.7631 | 8.0s\n",
            "[Fold 5] Ep 13/100 | Train loss 0.6313 acc 83.73% f1M 0.8583 | Val loss 0.8822 acc 77.53% f1M 0.8068 | 7.1s\n",
            "[Fold 5] Ep 14/100 | Train loss 0.5975 acc 85.55% f1M 0.8817 | Val loss 0.8829 acc 75.28% f1M 0.7900 | 8.4s\n",
            "[Fold 5] Ep 15/100 | Train loss 0.5346 acc 87.52% f1M 0.8814 | Val loss 0.9185 acc 76.40% f1M 0.7735 | 7.0s\n",
            "[Fold 5] Ep 16/100 | Train loss 0.4860 acc 85.69% f1M 0.8814 | Val loss 0.8823 acc 78.65% f1M 0.8125 | 8.5s\n",
            "[Fold 5] Ep 17/100 | Train loss 0.4504 acc 86.68% f1M 0.8784 | Val loss 0.8742 acc 78.65% f1M 0.8177 | 6.7s\n",
            "[Fold 5] Ep 18/100 | Train loss 0.4643 acc 89.20% f1M 0.9059 | Val loss 0.8914 acc 76.97% f1M 0.7936 | 8.4s\n",
            "[Fold 5] Ep 19/100 | Train loss 0.2952 acc 91.16% f1M 0.9307 | Val loss 0.9137 acc 78.65% f1M 0.8115 | 7.1s\n",
            "[Fold 5] Ep 20/100 | Train loss 0.2846 acc 92.01% f1M 0.9426 | Val loss 0.8846 acc 79.21% f1M 0.8097 | 8.2s\n",
            "[Fold 5] Ep 21/100 | Train loss 0.2639 acc 93.27% f1M 0.9377 | Val loss 0.9484 acc 79.78% f1M 0.8227 | 7.9s\n",
            "[Fold 5] Ep 22/100 | Train loss 0.2368 acc 94.39% f1M 0.9480 | Val loss 0.9714 acc 78.65% f1M 0.7971 | 7.6s\n",
            "[Fold 5] Ep 23/100 | Train loss 0.1756 acc 95.37% f1M 0.9574 | Val loss 0.9618 acc 76.97% f1M 0.7948 | 8.2s\n",
            "[Fold 5] Ep 24/100 | Train loss 0.2002 acc 95.23% f1M 0.9614 | Val loss 0.9641 acc 79.21% f1M 0.8133 | 7.3s\n",
            "[Fold 5] Ep 25/100 | Train loss 0.1227 acc 96.63% f1M 0.9716 | Val loss 0.9856 acc 76.97% f1M 0.7703 | 8.5s\n",
            "[Fold 5] Ep 26/100 | Train loss 0.1407 acc 95.93% f1M 0.9685 | Val loss 1.1182 acc 76.97% f1M 0.7868 | 7.1s\n",
            "[Fold 5] Ep 27/100 | Train loss 0.1115 acc 96.91% f1M 0.9745 | Val loss 1.0899 acc 78.65% f1M 0.8005 | 8.4s\n",
            "[Fold 5] Ep 28/100 | Train loss 0.1179 acc 96.63% f1M 0.9699 | Val loss 1.0887 acc 79.21% f1M 0.8089 | 7.9s\n",
            "[Fold 5] Ep 29/100 | Train loss 0.1118 acc 96.49% f1M 0.9639 | Val loss 1.1329 acc 76.40% f1M 0.7901 | 7.6s\n",
            "[Fold 5] Ep 30/100 | Train loss 0.1771 acc 97.05% f1M 0.9688 | Val loss 1.0978 acc 76.97% f1M 0.7974 | 8.5s\n",
            "[Fold 5] Ep 31/100 | Train loss 0.1161 acc 97.62% f1M 0.9753 | Val loss 1.1139 acc 77.53% f1M 0.7913 | 7.1s\n",
            "[Fold 5] Ep 32/100 | Train loss 0.1552 acc 96.77% f1M 0.9712 | Val loss 1.1435 acc 76.97% f1M 0.8179 | 8.5s\n",
            "[Fold 5] Ep 33/100 | Train loss 0.0614 acc 98.18% f1M 0.9861 | Val loss 1.1577 acc 78.09% f1M 0.8089 | 7.0s\n",
            "[Fold 5] Ep 34/100 | Train loss 0.0650 acc 98.88% f1M 0.9926 | Val loss 1.1360 acc 80.34% f1M 0.8150 | 8.8s\n",
            "[Fold 5] Ep 35/100 | Train loss 0.1317 acc 97.90% f1M 0.9811 | Val loss 1.0749 acc 79.21% f1M 0.8085 | 7.7s\n",
            "[Fold 5] Ep 36/100 | Train loss 0.1425 acc 97.76% f1M 0.9800 | Val loss 1.0694 acc 79.78% f1M 0.8183 | 7.5s\n",
            "[Fold 5] Ep 37/100 | Train loss 0.0878 acc 97.62% f1M 0.9815 | Val loss 1.1735 acc 76.40% f1M 0.7855 | 8.2s\n",
            "[Fold 5] Ep 38/100 | Train loss 0.1031 acc 97.05% f1M 0.9787 | Val loss 1.0856 acc 78.65% f1M 0.8072 | 7.2s\n",
            "[Fold 5] Ep 39/100 | Train loss 0.1335 acc 98.18% f1M 0.9839 | Val loss 1.0718 acc 76.97% f1M 0.7887 | 8.5s\n",
            "[Fold 5] Ep 40/100 | Train loss 0.0861 acc 97.76% f1M 0.9783 | Val loss 1.0904 acc 82.02% f1M 0.8423 | 7.0s\n",
            "[Fold 5] Ep 41/100 | Train loss 0.0803 acc 98.60% f1M 0.9914 | Val loss 1.0937 acc 79.78% f1M 0.8204 | 8.7s\n",
            "[Fold 5] Ep 42/100 | Train loss 0.0976 acc 98.32% f1M 0.9843 | Val loss 1.0801 acc 79.21% f1M 0.8188 | 7.6s\n",
            "[Fold 5] Ep 43/100 | Train loss 0.0861 acc 97.34% f1M 0.9790 | Val loss 1.0925 acc 78.65% f1M 0.8297 | 7.7s\n",
            "[Fold 5] Ep 44/100 | Train loss 0.1318 acc 98.32% f1M 0.9863 | Val loss 1.1246 acc 79.78% f1M 0.8336 | 8.4s\n",
            "[Fold 5] Ep 45/100 | Train loss 0.0735 acc 98.60% f1M 0.9871 | Val loss 1.1144 acc 79.78% f1M 0.8388 | 7.4s\n",
            "[Fold 5] Ep 46/100 | Train loss 0.0563 acc 98.46% f1M 0.9907 | Val loss 1.1189 acc 80.34% f1M 0.8445 | 8.7s\n",
            "[Fold 5] Ep 47/100 | Train loss 0.0719 acc 97.48% f1M 0.9825 | Val loss 1.1295 acc 79.21% f1M 0.8380 | 7.1s\n",
            "[Fold 5] Ep 48/100 | Train loss 0.0513 acc 99.02% f1M 0.9937 | Val loss 1.1324 acc 79.21% f1M 0.8301 | 8.5s\n",
            "[Fold 5] Ep 49/100 | Train loss 0.0491 acc 98.88% f1M 0.9863 | Val loss 1.1462 acc 80.90% f1M 0.8442 | 9.6s\n",
            "[Fold 5] Ep 50/100 | Train loss 0.0978 acc 98.74% f1M 0.9902 | Val loss 1.2136 acc 79.21% f1M 0.8133 | 7.3s\n",
            "[Fold 5] Ep 51/100 | Train loss 0.0698 acc 97.76% f1M 0.9829 | Val loss 1.1657 acc 79.21% f1M 0.8355 | 8.2s\n",
            "[Fold 5] Ep 52/100 | Train loss 0.1219 acc 98.46% f1M 0.9835 | Val loss 1.1175 acc 82.02% f1M 0.8535 | 7.2s\n",
            "[Fold 5] Ep 53/100 | Train loss 0.1098 acc 98.32% f1M 0.9833 | Val loss 1.1106 acc 80.34% f1M 0.8200 | 8.6s\n",
            "[Fold 5] Ep 54/100 | Train loss 0.1213 acc 98.04% f1M 0.9842 | Val loss 1.0427 acc 80.34% f1M 0.8427 | 7.1s\n",
            "[Fold 5] Ep 55/100 | Train loss 0.0777 acc 99.02% f1M 0.9937 | Val loss 1.0134 acc 80.90% f1M 0.8396 | 8.7s\n",
            "[Fold 5] Ep 56/100 | Train loss 0.0709 acc 97.48% f1M 0.9778 | Val loss 1.0321 acc 80.90% f1M 0.8525 | 7.8s\n",
            "[Fold 5] Ep 57/100 | Train loss 0.0693 acc 98.46% f1M 0.9894 | Val loss 0.9890 acc 80.34% f1M 0.8533 | 7.4s\n",
            "[Fold 5] Ep 58/100 | Train loss 0.0558 acc 98.46% f1M 0.9859 | Val loss 1.0801 acc 80.34% f1M 0.8473 | 8.3s\n",
            "[Fold 5] Ep 59/100 | Train loss 0.0726 acc 98.88% f1M 0.9897 | Val loss 1.0842 acc 79.78% f1M 0.8422 | 7.1s\n",
            "[Fold 5] Ep 60/100 | Train loss 0.0337 acc 99.44% f1M 0.9955 | Val loss 1.1443 acc 80.34% f1M 0.8398 | 8.4s\n",
            "[Fold 5] Ep 61/100 | Train loss 0.0444 acc 99.02% f1M 0.9926 | Val loss 1.1182 acc 78.09% f1M 0.8111 | 7.1s\n",
            "[Fold 5] Ep 62/100 | Train loss 0.0611 acc 98.60% f1M 0.9905 | Val loss 1.1699 acc 80.34% f1M 0.8375 | 8.6s\n",
            "[Fold 5] Ep 63/100 | Train loss 0.0506 acc 98.60% f1M 0.9910 | Val loss 1.1899 acc 78.65% f1M 0.8265 | 7.5s\n",
            "[Fold 5] Ep 64/100 | Train loss 0.0434 acc 98.74% f1M 0.9945 | Val loss 1.1468 acc 80.34% f1M 0.8383 | 7.5s\n",
            "[Fold 5] Ep 65/100 | Train loss 0.0477 acc 98.60% f1M 0.9926 | Val loss 1.1229 acc 79.78% f1M 0.8310 | 8.2s\n",
            "[Fold 5] Ep 66/100 | Train loss 0.0739 acc 98.74% f1M 0.9904 | Val loss 1.1233 acc 80.90% f1M 0.8514 | 7.1s\n",
            "[Fold 5] Ep 67/100 | Train loss 0.0514 acc 99.16% f1M 0.9920 | Val loss 1.2270 acc 79.78% f1M 0.8482 | 8.5s\n",
            "[Fold 5] Ep 68/100 | Train loss 0.0534 acc 99.02% f1M 0.9916 | Val loss 1.1913 acc 79.21% f1M 0.8444 | 7.2s\n",
            "[Fold 5] Ep 69/100 | Train loss 0.0314 acc 99.02% f1M 0.9905 | Val loss 1.2122 acc 79.21% f1M 0.8442 | 8.6s\n",
            "[Fold 5] Ep 70/100 | Train loss 0.0613 acc 98.88% f1M 0.9888 | Val loss 1.2173 acc 79.21% f1M 0.8423 | 7.4s\n",
            "[Fold 5] Ep 71/100 | Train loss 0.0500 acc 99.02% f1M 0.9890 | Val loss 1.1631 acc 79.21% f1M 0.8331 | 7.8s\n",
            "[Fold 5] Ep 72/100 | Train loss 0.0449 acc 99.44% f1M 0.9960 | Val loss 1.1425 acc 79.78% f1M 0.8445 | 7.9s\n",
            "[Fold 5] Ep 73/100 | Train loss 0.0484 acc 98.74% f1M 0.9913 | Val loss 1.1255 acc 80.90% f1M 0.8500 | 7.4s\n",
            "[Fold 5] Ep 74/100 | Train loss 0.0287 acc 99.30% f1M 0.9960 | Val loss 1.1877 acc 80.34% f1M 0.8415 | 8.5s\n",
            "[Fold 5] Ep 75/100 | Train loss 0.0288 acc 99.02% f1M 0.9929 | Val loss 1.1915 acc 80.90% f1M 0.8512 | 7.2s\n",
            "[Fold 5] Ep 76/100 | Train loss 0.0277 acc 99.44% f1M 0.9974 | Val loss 1.1862 acc 79.78% f1M 0.8485 | 8.6s\n",
            "[Fold 5] Ep 77/100 | Train loss 0.0336 acc 98.88% f1M 0.9925 | Val loss 1.2043 acc 80.34% f1M 0.8459 | 7.6s\n",
            "[Fold 5] Ep 78/100 | Train loss 0.0282 acc 99.16% f1M 0.9947 | Val loss 1.2140 acc 80.34% f1M 0.8438 | 8.2s\n",
            "[Fold 5] Ep 79/100 | Train loss 0.0271 acc 99.02% f1M 0.9936 | Val loss 1.2273 acc 79.78% f1M 0.8266 | 8.3s\n",
            "[Fold 5] Ep 80/100 | Train loss 0.0776 acc 99.16% f1M 0.9916 | Val loss 1.1980 acc 79.78% f1M 0.8307 | 7.0s\n",
            "[Fold 5] Ep 81/100 | Train loss 0.0500 acc 98.88% f1M 0.9884 | Val loss 1.2121 acc 79.78% f1M 0.8245 | 8.7s\n",
            "[Fold 5] Ep 82/100 | Train loss 0.0470 acc 99.02% f1M 0.9930 | Val loss 1.1877 acc 79.78% f1M 0.8238 | 7.2s\n",
            "[Fold 5] Ep 83/100 | Train loss 0.0201 acc 99.30% f1M 0.9967 | Val loss 1.1845 acc 79.21% f1M 0.8123 | 8.6s\n",
            "[Fold 5] Ep 84/100 | Train loss 0.0321 acc 99.02% f1M 0.9908 | Val loss 1.2111 acc 79.78% f1M 0.8317 | 7.5s\n",
            "[Fold 5] Ep 85/100 | Train loss 0.0460 acc 99.16% f1M 0.9950 | Val loss 1.2029 acc 79.21% f1M 0.8285 | 8.4s\n",
            "[Fold 5] Ep 86/100 | Train loss 0.0287 acc 99.16% f1M 0.9960 | Val loss 1.1595 acc 80.90% f1M 0.8252 | 8.3s\n",
            "[Fold 5] Ep 87/100 | Train loss 0.0665 acc 98.88% f1M 0.9918 | Val loss 1.1719 acc 80.34% f1M 0.8396 | 7.1s\n",
            "[Fold 5] Ep 88/100 | Train loss 0.0596 acc 97.90% f1M 0.9852 | Val loss 1.1541 acc 80.90% f1M 0.8518 | 8.5s\n",
            "[Fold 5] Ep 89/100 | Train loss 0.0546 acc 99.02% f1M 0.9946 | Val loss 1.1600 acc 81.46% f1M 0.8523 | 7.0s\n",
            "[Fold 5] Ep 90/100 | Train loss 0.0504 acc 99.30% f1M 0.9954 | Val loss 1.1694 acc 79.78% f1M 0.8287 | 8.6s\n",
            "[Fold 5] Ep 91/100 | Train loss 0.0281 acc 98.74% f1M 0.9930 | Val loss 1.2316 acc 79.78% f1M 0.8306 | 7.1s\n",
            "[Fold 5] Ep 92/100 | Train loss 0.0463 acc 98.74% f1M 0.9898 | Val loss 1.1501 acc 80.34% f1M 0.8173 | 8.6s\n",
            "[Fold 5] Ep 93/100 | Train loss 0.0289 acc 98.88% f1M 0.9920 | Val loss 1.1622 acc 80.34% f1M 0.8296 | 8.1s\n",
            "[Fold 5] Ep 94/100 | Train loss 0.0323 acc 99.16% f1M 0.9914 | Val loss 1.1737 acc 80.90% f1M 0.8210 | 7.2s\n",
            "[Fold 5] Ep 95/100 | Train loss 0.0207 acc 99.30% f1M 0.9956 | Val loss 1.1830 acc 80.34% f1M 0.8097 | 9.5s\n",
            "[Fold 5] Ep 96/100 | Train loss 0.0213 acc 99.16% f1M 0.9934 | Val loss 1.1912 acc 80.90% f1M 0.8186 | 7.5s\n",
            "[Fold 5] Ep 97/100 | Train loss 0.0356 acc 98.88% f1M 0.9887 | Val loss 1.2069 acc 80.34% f1M 0.8152 | 8.6s\n",
            "[Fold 5] Ep 98/100 | Train loss 0.0286 acc 99.58% f1M 0.9967 | Val loss 1.3049 acc 79.78% f1M 0.8268 | 7.2s\n",
            "[Fold 5] Ep 99/100 | Train loss 0.0431 acc 98.32% f1M 0.9862 | Val loss 1.3094 acc 78.09% f1M 0.8102 | 8.7s\n",
            "[Fold 5] Ep 100/100 | Train loss 0.0307 acc 99.02% f1M 0.9906 | Val loss 1.2343 acc 79.21% f1M 0.8074 | 8.2s\n",
            "[Fold 5] Best Val â†’ acc 82.02% | f1M 0.8535 (epoch 52)\n",
            "\n",
            "===== CV Summary (best epoch per fold) =====\n",
            "Val Accs: ['83.24%', '83.15%', '81.46%', '80.34%', '82.02%'] | MeanÂ±Std = 82.04% Â± 1.09%\n",
            "Val F1M : ['0.8667', '0.8418', '0.8336', '0.8406', '0.8535'] | MeanÂ±Std = 0.8473 Â± 0.0116\n",
            "[FULL] Ep 01/100 | loss 3.2006 acc 21.44% f1M 0.1545 | 31.0s\n",
            "[FULL] Ep 02/100 | loss 2.9890 acc 46.02% f1M 0.3717 | 8.9s\n",
            "[FULL] Ep 03/100 | loss 2.7985 acc 53.09% f1M 0.4589 | 7.5s\n",
            "[FULL] Ep 04/100 | loss 2.6156 acc 59.03% f1M 0.5437 | 8.7s\n",
            "[FULL] Ep 05/100 | loss 2.4623 acc 57.80% f1M 0.5184 | 8.4s\n",
            "[FULL] Ep 06/100 | loss 2.3047 acc 62.74% f1M 0.5649 | 7.4s\n",
            "[FULL] Ep 07/100 | loss 1.9876 acc 62.51% f1M 0.5947 | 13.5s\n",
            "[FULL] Ep 08/100 | loss 1.4506 acc 72.62% f1M 0.7322 | 9.2s\n",
            "[FULL] Ep 09/100 | loss 1.1089 acc 76.88% f1M 0.7691 | 7.6s\n",
            "[FULL] Ep 10/100 | loss 0.8497 acc 78.79% f1M 0.8125 | 8.6s\n",
            "[FULL] Ep 11/100 | loss 0.7140 acc 81.37% f1M 0.8343 | 9.1s\n",
            "[FULL] Ep 12/100 | loss 0.6047 acc 83.73% f1M 0.8601 | 7.5s\n",
            "[FULL] Ep 13/100 | loss 0.5185 acc 86.31% f1M 0.8771 | 9.0s\n",
            "[FULL] Ep 14/100 | loss 0.4448 acc 87.09% f1M 0.8845 | 8.2s\n",
            "[FULL] Ep 15/100 | loss 0.4069 acc 88.10% f1M 0.8931 | 7.6s\n",
            "[FULL] Ep 16/100 | loss 0.3736 acc 88.44% f1M 0.8979 | 8.8s\n",
            "[FULL] Ep 17/100 | loss 0.3294 acc 90.24% f1M 0.9206 | 7.5s\n",
            "[FULL] Ep 18/100 | loss 0.3237 acc 89.67% f1M 0.9125 | 14.1s\n",
            "[FULL] Ep 19/100 | loss 0.2521 acc 92.70% f1M 0.9340 | 8.9s\n",
            "[FULL] Ep 20/100 | loss 0.2066 acc 94.61% f1M 0.9571 | 9.1s\n",
            "[FULL] Ep 21/100 | loss 0.1796 acc 94.16% f1M 0.9506 | 7.8s\n",
            "[FULL] Ep 22/100 | loss 0.1500 acc 95.51% f1M 0.9629 | 9.2s\n",
            "[FULL] Ep 23/100 | loss 0.1635 acc 95.29% f1M 0.9624 | 8.9s\n",
            "[FULL] Ep 24/100 | loss 0.1356 acc 95.62% f1M 0.9663 | 7.6s\n",
            "[FULL] Ep 25/100 | loss 0.0992 acc 97.08% f1M 0.9729 | 8.9s\n",
            "[FULL] Ep 26/100 | loss 0.1172 acc 96.75% f1M 0.9712 | 7.8s\n",
            "[FULL] Ep 27/100 | loss 0.0991 acc 97.42% f1M 0.9808 | 8.7s\n",
            "[FULL] Ep 28/100 | loss 0.0870 acc 97.08% f1M 0.9754 | 9.1s\n",
            "[FULL] Ep 29/100 | loss 0.0887 acc 97.64% f1M 0.9790 | 7.7s\n",
            "[FULL] Ep 30/100 | loss 0.0881 acc 97.53% f1M 0.9823 | 8.9s\n",
            "[FULL] Ep 31/100 | loss 0.0971 acc 96.63% f1M 0.9769 | 8.5s\n",
            "[FULL] Ep 32/100 | loss 0.0887 acc 97.53% f1M 0.9826 | 7.7s\n",
            "[FULL] Ep 33/100 | loss 0.0813 acc 96.75% f1M 0.9755 | 8.9s\n",
            "[FULL] Ep 34/100 | loss 0.0679 acc 98.20% f1M 0.9872 | 10.1s\n",
            "[FULL] Ep 35/100 | loss 0.0684 acc 97.98% f1M 0.9798 | 8.1s\n",
            "[FULL] Ep 36/100 | loss 0.0494 acc 98.32% f1M 0.9849 | 8.9s\n",
            "[FULL] Ep 37/100 | loss 0.0526 acc 98.77% f1M 0.9894 | 7.6s\n",
            "[FULL] Ep 38/100 | loss 0.0659 acc 98.09% f1M 0.9807 | 8.6s\n",
            "[FULL] Ep 39/100 | loss 0.0591 acc 97.87% f1M 0.9802 | 8.9s\n",
            "[FULL] Ep 40/100 | loss 0.0713 acc 97.42% f1M 0.9798 | 7.5s\n",
            "[FULL] Ep 41/100 | loss 0.0481 acc 98.99% f1M 0.9910 | 8.8s\n",
            "[FULL] Ep 42/100 | loss 0.0508 acc 98.43% f1M 0.9876 | 7.8s\n",
            "[FULL] Ep 43/100 | loss 0.0481 acc 98.77% f1M 0.9902 | 8.5s\n",
            "[FULL] Ep 44/100 | loss 0.0409 acc 98.88% f1M 0.9887 | 9.1s\n",
            "[FULL] Ep 45/100 | loss 0.0432 acc 98.99% f1M 0.9919 | 7.7s\n",
            "[FULL] Ep 46/100 | loss 0.0358 acc 98.99% f1M 0.9939 | 9.0s\n",
            "[FULL] Ep 47/100 | loss 0.0334 acc 99.33% f1M 0.9969 | 8.7s\n",
            "[FULL] Ep 48/100 | loss 0.0603 acc 98.20% f1M 0.9839 | 7.9s\n",
            "[FULL] Ep 49/100 | loss 0.0390 acc 98.88% f1M 0.9936 | 8.9s\n",
            "[FULL] Ep 50/100 | loss 0.0434 acc 98.99% f1M 0.9918 | 7.6s\n",
            "[FULL] Ep 51/100 | loss 0.0323 acc 99.10% f1M 0.9904 | 8.8s\n",
            "[FULL] Ep 52/100 | loss 0.0342 acc 98.99% f1M 0.9913 | 9.1s\n",
            "[FULL] Ep 53/100 | loss 0.0459 acc 98.54% f1M 0.9883 | 7.5s\n",
            "[FULL] Ep 54/100 | loss 0.0285 acc 99.10% f1M 0.9944 | 8.8s\n",
            "[FULL] Ep 55/100 | loss 0.0505 acc 98.32% f1M 0.9870 | 8.0s\n",
            "[FULL] Ep 56/100 | loss 0.0419 acc 98.43% f1M 0.9870 | 8.4s\n",
            "[FULL] Ep 57/100 | loss 0.0335 acc 99.21% f1M 0.9942 | 8.9s\n",
            "[FULL] Ep 58/100 | loss 0.0472 acc 98.43% f1M 0.9884 | 7.6s\n",
            "[FULL] Ep 59/100 | loss 0.0257 acc 99.44% f1M 0.9951 | 8.9s\n",
            "[FULL] Ep 60/100 | loss 0.0302 acc 99.21% f1M 0.9953 | 8.2s\n",
            "[FULL] Ep 61/100 | loss 0.0292 acc 99.33% f1M 0.9954 | 7.9s\n",
            "[FULL] Ep 62/100 | loss 0.0222 acc 99.10% f1M 0.9952 | 8.9s\n",
            "[FULL] Ep 63/100 | loss 0.0240 acc 98.99% f1M 0.9913 | 7.8s\n",
            "[FULL] Ep 64/100 | loss 0.0295 acc 98.99% f1M 0.9933 | 8.9s\n",
            "[FULL] Ep 65/100 | loss 0.0220 acc 99.44% f1M 0.9967 | 8.7s\n",
            "[FULL] Ep 66/100 | loss 0.0244 acc 99.66% f1M 0.9980 | 7.9s\n",
            "[FULL] Ep 67/100 | loss 0.0258 acc 99.33% f1M 0.9947 | 8.9s\n",
            "[FULL] Ep 68/100 | loss 0.0333 acc 99.21% f1M 0.9949 | 7.6s\n",
            "[FULL] Ep 69/100 | loss 0.0272 acc 99.21% f1M 0.9945 | 8.4s\n",
            "[FULL] Ep 70/100 | loss 0.0233 acc 99.66% f1M 0.9962 | 8.9s\n",
            "[FULL] Ep 71/100 | loss 0.0273 acc 98.99% f1M 0.9951 | 7.4s\n",
            "[FULL] Ep 72/100 | loss 0.0220 acc 99.33% f1M 0.9955 | 8.9s\n",
            "[FULL] Ep 73/100 | loss 0.0282 acc 98.99% f1M 0.9904 | 7.5s\n",
            "[FULL] Ep 74/100 | loss 0.0205 acc 99.55% f1M 0.9973 | 8.6s\n",
            "[FULL] Ep 75/100 | loss 0.0166 acc 99.55% f1M 0.9955 | 9.3s\n",
            "[FULL] Ep 76/100 | loss 0.0225 acc 99.21% f1M 0.9953 | 7.7s\n",
            "[FULL] Ep 77/100 | loss 0.0253 acc 99.21% f1M 0.9942 | 9.4s\n",
            "[FULL] Ep 78/100 | loss 0.0268 acc 99.21% f1M 0.9951 | 9.6s\n",
            "[FULL] Ep 79/100 | loss 0.0181 acc 99.33% f1M 0.9959 | 7.6s\n",
            "[FULL] Ep 80/100 | loss 0.0183 acc 99.21% f1M 0.9949 | 9.1s\n",
            "[FULL] Ep 81/100 | loss 0.0194 acc 99.33% f1M 0.9955 | 8.1s\n",
            "[FULL] Ep 82/100 | loss 0.0275 acc 98.88% f1M 0.9937 | 8.1s\n",
            "[FULL] Ep 83/100 | loss 0.0245 acc 99.10% f1M 0.9949 | 8.9s\n",
            "[FULL] Ep 84/100 | loss 0.0227 acc 99.21% f1M 0.9949 | 7.8s\n",
            "[FULL] Ep 85/100 | loss 0.0159 acc 99.55% f1M 0.9979 | 9.2s\n",
            "[FULL] Ep 86/100 | loss 0.0192 acc 99.33% f1M 0.9964 | 9.0s\n",
            "[FULL] Ep 87/100 | loss 0.0204 acc 99.33% f1M 0.9966 | 7.7s\n",
            "[FULL] Ep 88/100 | loss 0.0159 acc 99.21% f1M 0.9940 | 9.1s\n",
            "[FULL] Ep 89/100 | loss 0.0258 acc 99.33% f1M 0.9957 | 7.9s\n",
            "[FULL] Ep 90/100 | loss 0.0197 acc 99.44% f1M 0.9953 | 8.5s\n",
            "[FULL] Ep 91/100 | loss 0.0162 acc 99.21% f1M 0.9957 | 9.0s\n",
            "[FULL] Ep 92/100 | loss 0.0209 acc 98.99% f1M 0.9926 | 7.6s\n",
            "[FULL] Ep 93/100 | loss 0.0230 acc 99.33% f1M 0.9941 | 8.8s\n",
            "[FULL] Ep 94/100 | loss 0.0184 acc 99.33% f1M 0.9968 | 7.9s\n",
            "[FULL] Ep 95/100 | loss 0.0141 acc 99.44% f1M 0.9959 | 8.2s\n",
            "[FULL] Ep 96/100 | loss 0.0164 acc 99.21% f1M 0.9941 | 9.1s\n",
            "[FULL] Ep 97/100 | loss 0.0198 acc 99.10% f1M 0.9944 | 7.5s\n",
            "[FULL] Ep 98/100 | loss 0.0218 acc 99.44% f1M 0.9960 | 9.0s\n",
            "[FULL] Ep 99/100 | loss 0.0156 acc 99.55% f1M 0.9969 | 7.9s\n",
            "[FULL] Ep 100/100 | loss 0.0204 acc 99.33% f1M 0.9942 | 8.1s\n",
            "\n",
            "TEST â†’ loss 0.9482 | acc 84.55% | f1M 0.8737\n",
            "\n",
            "Per-class report:\n",
            "                     precision    recall  f1-score   support\n",
            "\n",
            "       Actinophrys     1.0000    1.0000    1.0000         5\n",
            "            Amoeba     0.5556    0.5000    0.5263        10\n",
            "           Arcella     1.0000    0.7500    0.8571         4\n",
            "         Aspidisca     1.0000    1.0000    1.0000         4\n",
            "          Ceratium     1.0000    1.0000    1.0000         6\n",
            "          Codosiga     0.7500    0.7500    0.7500         4\n",
            "           Colpoda     1.0000    1.0000    1.0000         5\n",
            "         Epistylis     0.7500    0.6000    0.6667         5\n",
            "           Euglena     0.9583    0.8846    0.9200        26\n",
            "          Euglypha     0.6667    1.0000    0.8000         2\n",
            "         Gonyaulax     1.0000    1.0000    1.0000         5\n",
            "       Gymnodinium     1.0000    1.0000    1.0000         4\n",
            "             Hydra     0.8000    0.8889    0.8421         9\n",
            "Keratella_quadrala     1.0000    1.0000    1.0000         7\n",
            "         Noctiluca     1.0000    1.0000    1.0000         7\n",
            "        Paramecium     0.9286    0.8966    0.9123        29\n",
            "            Phacus     1.0000    1.0000    1.0000         3\n",
            "      Rod_bacteria     0.5714    0.5714    0.5714        14\n",
            "          Rotifera     1.0000    1.0000    1.0000         6\n",
            "       Siprostomum     0.8571    1.0000    0.9231         6\n",
            "Spherical_bacteria     0.7143    0.7143    0.7143        14\n",
            "   Spiral_bacteria     0.5833    0.5833    0.5833        12\n",
            "           Stentor     1.0000    1.0000    1.0000         7\n",
            "       Stylonychia     1.0000    1.0000    1.0000         4\n",
            "         Synchaeta     1.0000    1.0000    1.0000         8\n",
            "        Vorticella     0.7500    1.0000    0.8571         3\n",
            "             Yeast     0.6154    0.7273    0.6667        11\n",
            "\n",
            "          accuracy                         0.8455       220\n",
            "         macro avg     0.8704    0.8839    0.8737       220\n",
            "      weighted avg     0.8493    0.8455    0.8456       220\n",
            "\n"
          ]
        }
      ]
    }
  ]
}